{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8-SVM",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geocarvalho/python-ds/blob/master/MLmastering/master-ml-algorithms/8-SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6cPO4iKbCbh",
        "colab_type": "text"
      },
      "source": [
        "# Capítulo 26 - Máquinas de vetores de suporte (Support Vector Machines - SVM)\n",
        "\n",
        "## 26.1 Classificador de margem-máxima\n",
        "\n",
        "* O classifidor de margem-máxima é um classificador hipotético que explica muito bem como SVM funciona na prática. As variáveis numéricas de entrada ($x$) nos seus dados (colunas) formam um espaço n-dimensional. Por exemplo, se você tem duas variáveis de entrada, isso formaria um espaço bidimensional;\n",
        "\n",
        "* Um hiperplano é uma linha que divide as variáveis de entrada num espaço. Em SVM, um hiperplano é selecionado para melhor separar os pontos das variáveis de entrada num espaço pela sua classe, classe 0 ou classe 1. Em um espaço bidimensional é possível visualizar isso como uma linha e vamos assumir que todos os pontos de entrada podem ser completamente separáveis por uma linha;\n",
        "\n",
        "* Por exemplo $b_0 + (b_1 \\times x_1) + (b_2 \\times x_2) = 0$\n",
        "\n",
        "> Onde os coeficientes ($b_1$ e $b_2$) que determinam a inclinação da linha e a interceptação ($b_0$) são encontrados pelo algoritmo de apredizagem, sendo $x_1$ e $x_2$ as duas variáveis de entrada;\n",
        "\n",
        "> Classificações podem ser feitas usando essa linha, basta introduzir os valores de entrada na equação da linha que é possível calcular um novo ponto acima ou abaixo da linha.\n",
        "\n",
        "* Abaixo da linha, a equação retorna um valor maior que zero e o ponto pertence a primeira classe (classe 0). Acima da linha, a equação retorna um valor menor que zero e o ponto pertence a segunda classe (classe 1). Já um valor próximo da linha retorna um valor perto de zero e o ponto pode ser difícil de classificar. Se a magnitude do valor é grande, o modelo pode mais confiabilidade na predição.\n",
        "\n",
        "* A distância entre a linha e os dados mais próximos é referenciada como **margem**. A melhor linha ou linha ótima que pode separar duas classes é a linha com a **maior margem** (hiperplano de margem-máxima);\n",
        "\n",
        "* A margem é calculada como a distância perpendicular da linha para os pontos mais próximos. Apenas esses pontos são relevantes na definição da linha e a construção do classificador, esses pontos são chamados de **vetores de suporte**, eles suportam ou definem o hiperplano. O hiperplano é aprendido pelo dados de treinamento usando um processo de otimização que **maximiza** a margem.\n",
        "\n",
        "## 26.2 Classificador de margem suave\n",
        "\n",
        "* Ná prática, dados reais são bagunçados e não podem ser separados por um hiperplano. A restrição de maximizar a margem da linha que separa as classes deve ser relaxada, isso é normalmente chamado de **classificador de margem suave**;\n",
        "\n",
        "* Essa mudança permite que alguns pontos nos dados de treinamento violem a linha de separação. Um grupo adicional de coeficientes são introduzidos dando a margem espaço de manobra em cada dimensão. Esses coeficientes são chamados de variáveis de folga. Isso aumenta a complexidade do modelo havendo mais parâmetros para o modelo caber (*fit*) nos dados e fornecer essa complexidade;\n",
        "\n",
        "* Um parâmetro de afinação $C$ define a magnitude da flexibilidade permitida entre as dimensões. O parâmetro $C$ define a quantidade de violações na margem que são permitidas. Um $C = 0$ significa sem violação e voltamos a inflexibilidade do classificado de margem-máxima descrito anteriormente. Quanto maior o valor de $C$, mais violações no hiperplano são permitidas. Durante o aprendizado do hiperplano a partir dos dados, todas as instâncias de treinamento que estão entre a distância da margem irão afetar o posicionamento do hiperplano e são referidos como **vetores de suporte**. E como $C$ afeta o número de instâncias que permitem cairem dentro da margem, $C$ influencia o número de vetores de suporte usados pelo modelo.\n",
        "\n",
        "> Quanto **menor** o valor de $C$, **mais sensível** o algoritmo é aos dados de treinamento (**maior variância** e **menor viés**);\n",
        "\n",
        "> Quanto **mair** o valor de $C$, **menos sensível** o algoritmo é aos dados de treinamento (**baixa variância** e **maior viés**)\n",
        "\n",
        "## 26.3 Máquinas de suporte de vetores - SVMs (Kernels)\n",
        "\n",
        "* O algoritmo de SVM é implementado na prática usando um *kernel*. O aprendizado do hiperplano numa SVM linear é feito pelo aprendizado do problema usando algebra linear. Um ideia poderosa é que o SVM linear pode ser reformulado usnado o produto interior que qualquer duas observações, em vez das observações diretas;\n",
        "\n",
        "* O produto escalar entre dois vetores é a soma da multiplicação de cada par de valores de entrada. Por exemplo, o produto escalar dos vetores $[2, 3]$ e $[5, 6]$ é $2 \\times 5 + 3 \\times 6$ ou $28$. A equação para fazer a predição para um novo dado de entrada usando o produto escalar entre a entrada ($x$) e cada vetor de suporte ($x_i$) é calculada por:\n",
        "\n",
        "$f(x) = b_0 + \\sum^n_{i=1} (a_i \\times (x \\times x_i))$\n",
        "\n",
        "* Essa é uma equação que envolve calcular o produto escalar de um novo vetor de entrada ($x$) com todos os vetores de suporte nos dados de treinamento. O coeficiente $b_0$ e $a_i$ (para cada entrada) devem ser estimados dos dados de entrada pelo algoritmo de aprendizado.\n",
        "\n",
        "## 26.3.1 SVM *kernel* linear\n",
        "\n",
        "* O produto escalar é chamado de *kernel* e pode ser reescrito como: $K(x,x_i) = \\sum(x \\times x_i)$\n",
        "\n",
        "* O *kernel* define a similaridade ou a medida da distância entre um novo dado e os vetores de suporte. O produto escalar é a medida de similaridade usada no SVM linear ou *kernel* linear porque a distância é a combinação linear dos dados de entrada. Outros *kerneis* podem ser usados para transformar o espaço de entrada em dimensões maiores (chamado de **truque do kernel**) como o **kernel polinomial** e o **kernel radial**.\n",
        "> É desejável usar *kerneis* mais complexos quando se é necessário mais linhas para separar classes que são curvas ou mais complexas, isso pode gerar classificadores mais acurados.\n",
        "\n",
        "## 26.3.2 SVM *kernel* polinomial\n",
        "\n",
        "* Em vez do produto escalar, podemos usar o *kernel* polinomial:\n",
        "\n",
        "$K(x,x_i) = 1 + \\sum(x \\times x_i)^d$\n",
        "\n",
        "* Onde o grau do polinômio pode ser especificado manualmente para um algortimo de aprendizagem. Quando $d = 1$ é o mesmo que um *kernel* linear, o *kernel* polinomial permite linhas curvaos no espaço de entrada.\n",
        "\n",
        "## 26.3.3 SVM *kernel* radial (RBF)\n",
        "\n",
        "* Finalmente, podemos ter o kernel radial mais complexo:\n",
        "\n",
        "$K(x,x_i) = e^{-gamma \\times \\sum ((x-x_i)^2)}$\n",
        "\n",
        "* Onde $gamma$ é o parâmetro que deve ser especificado ao algoritmo de aprendizado. Um bom valor padrão de $gamma$ é 0.1, onde $gamma$ é frequentemente $0 < gamma < 1$;\n",
        "\n",
        "* O *kernel* radial é bem local e pode criar regiões complexas no espaço da *feature*, como aproximações de poligonos num espaço bidimensional.\n",
        "\n",
        "## 26.4 Como aprender um modelo SVM\n",
        "\n",
        "* O modelo SVM precisa ser resolvido unado um processo de otimização. Pode ser usado um processo de otimização numérica para pesquisar pelos coeficientes do hiperplano, isso é ineficiente e não é método mais utilizado nas implementações de SVM como o LIBSVM;\n",
        "\n",
        "* Se for implementar SVM como exercício, pode-se usar uma variação do gradiente descedente chamado **sub-gradiente descedente**;\n",
        "\n",
        "* Existem processos de otimizações especiais que reformulam o problema de otimização para ser um problma de **progamação quadrática**. O método mais popular de SVM é o de **otimização sequencial mínimo** (*Sequencial Minimal Optimization - SMO) que é muito eficient, ele quebra o problema em sub-problemas que podem ser resolvidos analiticamente (por calculo) em vez de numericamente (pesquisando ou otimizando).\n",
        "\n",
        "## 26.5 Preparando os dados para o modelo de SVM\n",
        "\n",
        "* Algumas sugestões:\n",
        "> 1. **Entradas numéricas**: SVM assume que os dados de entrada são numéricos, se você tem dados categoricos é preciso converter em uma variável para cada categoria (*binary dummy variables*);\n",
        "> 2. **Classificação binária**: SVM básica é descrita para uma problema de classificação binária (duas classes). Porém, extensões podem ser desenvolvidas para problemas de regressão e classificação de multiclasses.\n",
        "\n",
        "---\n",
        "\n",
        "# Capítulo 27 - tutorial de SVM\n",
        "\n",
        "* SVM é um algoritmo de aprendizado de máquina flexivel não-paramétrico;\n",
        "\n",
        "* Vamos ver o algoritmo de SVM com o kernel usando sub-gradiente descendente;\n",
        "\n",
        "* Os dados de exemplo foi criado para que as classes fossem linearmente separáveis. Isso significa que uma linha reta pode ser desenhada para separar as classes. Isso foi intensional para que fosse explorado como implementar uma SVM com um kernel linear."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHIDypmmaf9q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "43fcce83-0f1a-4204-9c9b-efcdde6acebe"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {'X1': [2.327868056, 3.032830419, 4.485465382, 3.684815246, 2.283558563, 7.807521179, 6.132998136, 7.514829366, 5.502385039, 7.432932365], \n",
        "        'X2': [2.458016525, 3.170770366, 3.696728111, 3.846846973, 1.853215997, 3.290132136, 2.140563087, 2.107056961, 1.404002608, 4.236232628], \n",
        "        'Y': [-1, -1, -1, -1, -1, 1, 1, 1, 1, 1]}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.327868</td>\n",
              "      <td>2.458017</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.032830</td>\n",
              "      <td>3.170770</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.485465</td>\n",
              "      <td>3.696728</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.684815</td>\n",
              "      <td>3.846847</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.283559</td>\n",
              "      <td>1.853216</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         X1        X2  Y\n",
              "0  2.327868  2.458017 -1\n",
              "1  3.032830  3.170770 -1\n",
              "2  4.485465  3.696728 -1\n",
              "3  3.684815  3.846847 -1\n",
              "4  2.283559  1.853216 -1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRVJTf12agK3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "f200fd9f-c671-4f2f-9234-d9aa8713275d"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.scatter(df[df['Y']==-1]['X1'], df[df['Y']==-1]['X2'], c='b', label='-1')\n",
        "plt.scatter(df[df['Y']==1]['X1'], df[df['Y']==1]['X2'], c='r', label='1')\n",
        "plt.xlim(0, 9)\n",
        "plt.ylim(0, 4.5)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qCj3KrPak89",
        "colab_type": "text"
      },
      "source": [
        "## 27.2 Treinando SVM com gradiente descendente\n",
        "\n",
        "### 27.2.1 Modelo de SVM linear\n",
        "\n",
        "* O modelo de SVM linear é uma linha onde o objetivo do algoritmo é encontrar os melhores valores de coeficientes que separam as classes;\n",
        "\n",
        "$b_0 + (b_1 \\times x_1) + (b_2 \\times x_2) = 0$\n",
        "\n",
        "> Onde $b_0$, $b_1$ e $b_2$ são os coeficientes, $x_1$ e $x_2$ as variáveis de entrada. Essa será a forma da equação que será usada com uma pequena mudança, retiramos o termo de viés ($b_0 = 0$), também chamado *bias* ou *offset* ou *intercept*.  \n",
        "\n",
        "$(b_1 \\times x_1) + (b_2 \\times x_2) = 0$\n",
        "\n",
        "* Isso significa que a linha irá passar na origem ($x_1 = 0$ e $x_2 = 0$). Isso é para deixar o tutorial fácil de ser seguido e porque o exemplo é simples e não precisa disso, podendo ser adicionado se quiser.\n",
        "\n",
        "### 27.2.2 Método de otimização do SVM\n",
        "\n",
        "* O algoritmo de otimização para encontrar o coeficiente pode ser fixado como um problema quadrático. Isso é um tipo de otimização restrita onde soluções rápidas podem ser emplementadas (não usaremos esse modelo no tutorial). Outra forma que pode ser usada para descobrir os valores de coeficiente para o SVM linear é o **sub-gradiente descedente**;\n",
        "\n",
        "* Nesse método um padrão de treinamento aleatório é selecionado cada interação e usado para atualizar os coeficientes. Depois de um grande número de iterações (dezenas ou centenas de dezenas) o algoritmo vai resolver com um grupo estável de coeficientes. A atualização da equação de atualização dos coeficientes funciona assim:\n",
        "\n",
        "1. Primeiro um valor de saída é calculado por: $output = y \\times ((b_1 \\times x_1) + (b_2 \\times x_2))$\n",
        "> Dois procedimentos diferentes de atualização são usados dependendo do valor de saída. \n",
        "\n",
        "2. Se o **valor for > 1** isso sugere que o padrão de treinamento **não foi um vetor de suporte** (isso significa que a instância não foi diretamente envolvida no cálculo do valor de saída) que no caso os pesos são levemente diminuidos por:\n",
        "$b = (1 - \\frac{1}{t}) \\times b$\n",
        "> Onde $b$ é o peso que está sendo atualizado (como $b_1$ ou $b_2$), $t$ é a interação corrente (ex.: 1 para a primeira atualização, 2 para a segunda e assim por seguinte).\n",
        "\n",
        "2. Se o **valor de saída for < 1** então é assumido que a instância de treino é um vetor de suporte e deve ser atualizado para explicar melhor os dados:\n",
        "$b = (1 - \\frac{1}{t}) \\times b + \\frac{1}{lambda \\times t} \\times (y \\times x)$\n",
        "> Onde $b$ é o peso que está sendo atualizado, $t$ é a interação corrente e $lambda$ é o parâmetro para o algoritmo de aprendizado. O $lambda$ é o parâmetro de aprendizado e é frequentemente escolhido com um valor bem baixo como 0.0001 or menor. \n",
        "\n",
        "3. O procedimento é repetido até a taxa de erro cair até um valor desejável ou para um grande número fixo de iterações. \n",
        "> Valores baixos de taxa de aprendizado frequentemente necessitam  de grande tempo de treinamento, o número de iterações é a desvantagem desse algoritmo.\n",
        "\n",
        "## 27.3 Modelo de SVM aprendendo com os dados de treino\n",
        "\n",
        "* Aqui vamos trabalhar com algumas atualizações nos coeficientes para demonstrar o algoritmo de SVM;\n",
        "\n",
        "* Usaremos um $lambda$ alto ($lambda = 0.45$), o que é estranho já que se usa valores muito pequenos, para demonstrar grandes mudanças na atualização.\n",
        "\n",
        "* Inicializamos os coeficientes zerados;\n",
        "\n",
        "* Preciamos acompanhar a iteração ($t = 1$). Vamos treinar o modelo usando a ordem dos padrões do treino. Idealmente, a ordem dos padrões deveria ser aleatória para evitart que o algoritm pare;\n",
        "\n",
        "* O primeiro padrão que usaremos para atualizar é o índice 0 do `df`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYM2lfswaijG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "606811d1-7c6c-4aa9-bbbf-a12c961956e3"
      },
      "source": [
        "lamb = 0.45\n",
        "b1 = 0\n",
        "b2 = 0\n",
        "t = 1\n",
        "df.iloc[[0]]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.327868</td>\n",
              "      <td>2.458017</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         X1        X2  Y\n",
              "0  2.327868  2.458017 -1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fXx9z_rapro",
        "colab_type": "text"
      },
      "source": [
        "* Podemos calcular o valor de saída para essa iteração:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qY7oYhmVanx4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a26da5f-72fe-4499-cb00-131adf62f02a"
      },
      "source": [
        "indice = df.iloc[[0]].values\n",
        "y = indice[0][2]\n",
        "x1 = indice[0][0]\n",
        "x2 = indice[0][1]\n",
        "output = y * ((b1 * x1) + (b2 * x2))\n",
        "output"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJR2fqDIasXA",
        "colab_type": "text"
      },
      "source": [
        "* A saída é < 1, então vamos utilizar a atualização mais complexa que assume que o padrão de treinamento é um vetor de suporte:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YKJz-N0arRX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1c455dcd-522a-4e30-97d4-799ce0611f85"
      },
      "source": [
        "b = output\n",
        "b1 = (1 - 1/t) * b + (1/lamb*t) * (y * x1)\n",
        "b2 = (1 - 1/t) * b + (1/lamb*t) * (y * x2)\n",
        "b1, b2"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.173040124444444, -5.462258944444445)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGq2ZRm3aw5n",
        "colab_type": "text"
      },
      "source": [
        "* Atualizamos os coeficientes que usaremos na próxima iteração ($t = 2$) do algoritmo com a segunda instância dos dados de treinamento (indice 1);\n",
        "\n",
        "* Vamos repetir o processo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYozBPa6avL8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5cccfbba-144e-4840-c7ed-a5253a5ed041"
      },
      "source": [
        "t = 2\n",
        "indice = df.iloc[[1]].values\n",
        "y = indice[0][2]\n",
        "x1 = indice[0][0]\n",
        "x2 = indice[0][1]\n",
        "output = y * ((b1 * x1) + (b2 * x2))\n",
        "output"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33.00852224058554"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OlcEWTda0QV",
        "colab_type": "text"
      },
      "source": [
        "* A saída é > 1, sugerindo que a instância do treino não é um vetor de suporte. Atualizamos os coeficientes do modo mais simples:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9t2_rZAaybk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ed72ce4-f64e-4d13-b62a-f480708b0474"
      },
      "source": [
        "b = output\n",
        "b1 = (1 - 1/t) * b1\n",
        "b2 = (1 - 1/t) * b2\n",
        "b1, b2"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.586520062222222, -2.7311294722222224)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CtlrhVea3di",
        "colab_type": "text"
      },
      "source": [
        "### 27.3.3 Mais iterações\n",
        "\n",
        "* Repitir o processo para o resto dos dados. Uma passagem pelos dados é chamado *epoch*. Agora repetiremos o processo por 15 *epochs* para um total de 160 iterações (16 *epochs* x 10 atualizações por *epoch*);\n",
        "\n",
        "* É possível ficar ciente da perda ou da acurácia do modelo para cada *epoch*. Isso é uma ótima forma de entender quando o algoritmo está convergindo ou quando há um bug na implementação. Se criarmos o gráfico para a acurácia do modelo no fim de cada *epoch*, poderiamos ver algo como:\n",
        "\n",
        "![acc](acc.png)\n",
        "\n",
        "* Você verá que depois de 16 *epochs* que teremos uma acurácia de 100% nos dados de treino. Os valores de coeficientes deveriam ser:\n",
        "\n",
        "$b_1 = 0.552391765$ e $b_2 = -0.724533592$\n",
        "\n",
        "* A forma do hiperplano aprendido é: $0 + (0.552391765 \\times x_1) + (-0.724533592 \\times x_2) = 0$\n",
        "\n",
        "## 27.4 Fazendo predições com o modelo de SVM\n",
        "\n",
        "* Agora que temos os coeficientes da linha, podemos fazer predições. Faremos predições para dados de treino, mas poderia ser adaptado para dados de teste (dados novos):\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wSfNU1qa1mu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "5c62ab8d-6156-45a1-df2f-ed08cdc070a0"
      },
      "source": [
        "def pred(x1, x2):\n",
        "  b1 = 0.52391765\n",
        "  b2 = -0.724533592\n",
        "  output = (b1 * x1) + (b2 * x2)\n",
        "  return output\n",
        "\n",
        "df['output'] = df.apply(lambda x: pred(x['X1'], x['X2']), axis=1)\n",
        "df['crisp'] = df['output'].apply(lambda x: -1 if x < 0 else 1)\n",
        "df"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>Y</th>\n",
              "      <th>output</th>\n",
              "      <th>crisp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.327868</td>\n",
              "      <td>2.458017</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.561304</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.032830</td>\n",
              "      <td>3.170770</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.708376</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.485465</td>\n",
              "      <td>3.696728</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.328389</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.684815</td>\n",
              "      <td>3.846847</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.856630</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.283559</td>\n",
              "      <td>1.853216</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.146321</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>7.807521</td>\n",
              "      <td>3.290132</td>\n",
              "      <td>1</td>\n",
              "      <td>1.706687</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6.132998</td>\n",
              "      <td>2.140563</td>\n",
              "      <td>1</td>\n",
              "      <td>1.662276</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7.514829</td>\n",
              "      <td>2.107057</td>\n",
              "      <td>1</td>\n",
              "      <td>2.410518</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>5.502385</td>\n",
              "      <td>1.404003</td>\n",
              "      <td>1</td>\n",
              "      <td>1.865550</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>7.432932</td>\n",
              "      <td>4.236233</td>\n",
              "      <td>1</td>\n",
              "      <td>0.824952</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         X1        X2  Y    output  crisp\n",
              "0  2.327868  2.458017 -1 -0.561304     -1\n",
              "1  3.032830  3.170770 -1 -0.708376     -1\n",
              "2  4.485465  3.696728 -1 -0.328389     -1\n",
              "3  3.684815  3.846847 -1 -0.856630     -1\n",
              "4  2.283559  1.853216 -1 -0.146321     -1\n",
              "5  7.807521  3.290132  1  1.706687      1\n",
              "6  6.132998  2.140563  1  1.662276      1\n",
              "7  7.514829  2.107057  1  2.410518      1\n",
              "8  5.502385  1.404003  1  1.865550      1\n",
              "9  7.432932  4.236233  1  0.824952      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPM_lXH-a7hy",
        "colab_type": "text"
      },
      "source": [
        "* Comparando a coluna `crisp` com a coluna `Y` podemos ver que o modelo teve 100% de acurácia nos dados de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CHFVz_sa50M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}