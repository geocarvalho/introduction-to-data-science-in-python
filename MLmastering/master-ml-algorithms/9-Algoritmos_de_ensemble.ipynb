{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "9-Algoritmos_de_ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geocarvalho/python-ds/blob/master/MLmastering/master-ml-algorithms/9-Algoritmos_de_ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPqjSWM8tb5t",
        "colab_type": "text"
      },
      "source": [
        "# Parte 5 - Algoritmos de *ensemble*\n",
        "\n",
        "## Capítulo 28 - *Bagging* e *Random forest*\n",
        "\n",
        "### 28.1 - Método de *bootstrap*\n",
        "\n",
        "* É um método para para estimar a quantidade de uma amostra, é fácil de entender se a quantidade é uma estatísca descritiva como média ou desvio padrão;\n",
        "\n",
        "* Vejamos um caso de amostra com 100 valores ($x$) e gostariamos de ter uma estimativa da média da amostra:\n",
        "\n",
        "$mean(x) = \\frac{1}{n} \\times \\sum^n_{i=1} x_i$\n",
        "\n",
        "* Sabemos que a amostra é pequena e que a média tem erros inclusos. Podemos melhoras a estimativa da média usando o procedimento de **bootstrap**:\n",
        "\n",
        "> 1. Criamos muitas (ex.: 1000) subamostras aleatórias dos dados com repetição;\n",
        "> 2. Calculamos a média de cada subamostra;\n",
        "> 3. Calculamos a variação de todas as médias coletadas e usamos isso como nossa média estimada para o dado.\n",
        "\n",
        "* Por exemplo, vamos dizer que usamos 3 reamostragens e tivemos os valores de média 2.3, 4.5 e 3.3. Tirando a média teriamos uma média estimada dos dados de 3.367. Esse processo pode ser usado para estimar outras quantidades como desvio padrão ou até mesmo coeficiente de aprendizado.\n",
        "\n",
        "### 28.2 Agregação de *bootstrap* (*Bagging*)\n",
        "\n",
        "* É um método simples e poderoso de *ensemble* (técnica que combina predições de multiplos modelos para criar uma predição mais acurada do que os modelos individualmente);\n",
        "\n",
        "* É um procedimento que pode ser usado para reduzir variância para os algoritmos que tem alta variância (árvores de decisão como classificação, ávores de regressão);\n",
        "\n",
        "* Árvores de decisão são sensíveis aos dados que são treinada, se os dados de treinamento mudam o resultado do modelo pode ser diferente e assim suas predições. *Bagging* é a aplicação de *bootstrap* em algoritmos de aprendizado de maquina com alta variação;\n",
        "\n",
        "* Vejamos um caso de 1000 instâncias e estamos usando o algoritmo CART:\n",
        "\n",
        "> 1. Criar muitas (ex.: 100) subamostras aleatórias dos dados com reposição;\n",
        "> 2. Treinar o CART para cada amostra;\n",
        "> 3. Dado um novo dado, calcular  a média das predições de cada modelo.\n",
        "\n",
        "* Por exemplo, se temos 5 arvores com *bagging* que fizeram as seguintes predições: `azul, azul, vermelho, azul, vermelho`, escolheriamos a classe mais frequente e predizendo `azul`;\n",
        "\n",
        "* Quando usamos *bagging* em árvores de decisão não estamos preocupados com árvores individuais terem *overfitting* nos dados de treinamento. Por essa razão e por eficiência, as árvores individuais crescem profundamente (ex.: poucas amostras de treino para cada folha-nó da árvore) e a ávores não é aparada. Essas árvores terão alta variância e baixo viés. Todas essas características são importantes dos submodelos quando combinamos as predições usando *bagging*;\n",
        "\n",
        "* O único parâmetro quando usamos *bagging* em árvores de decisão é o número de árvores a serem criadas, isso pode ser escolhido via *cross-validation* até que a acurácia não mude. Criar grandes quantidades de árvores pode gastar um bom tempo, mas não irá causar *overfitting*;\n",
        "\n",
        "* Pode ser usado para regressão ou classificação.\n",
        "\n",
        "### 28.3 *Random forest*\n",
        "\n",
        "* *Random forests* é algoritmo melhorado em cima do *bagging* de árvores de decisão. O problema das árvores de decisão como o CART é que são gananciosos. Eles escolhem qual variável para separar usando um algoritmo ganancioso que minimiza o erro;\n",
        "\n",
        "* Mesmo com o *bagging* as ávores de decisão podem ter estruturas similares causando predições com alta correlação. Combinações de predições de múltiplos modelos em *ensemble* tem resultados melhores se as predições dos submodelos não possuem ou são fracamente correlacionadas.\n",
        "\n",
        "* *Random forest* muda o algoritmo para que as sub-árvores aprendam para que seus resultados de predição tenham baixa correlação. Em CART, quando há a seleção do ponto de divisão o algoritmo olha para todas as variáveis para selecionar o ponto ótimo de divisão. Já no *random forest* o algoritmo é limitado a uma amostragem aleatória para pesquisa (isso é dado por um parâmetro, podendo tentar diferentes valores por *cross-validation*);\n",
        "> Para classificação um bom valor segue $m = \\sqrt{p}$ e para regressão $m = \\frac{p}{3}$; Onde $m$ é o número aleatório de *features* que podem ser pesquisadas como ponto de divisão e $p$ é número total de *features*.\n",
        "\n",
        "Para um dado com 25 variáveis de entrada para classificação: $m = \\sqrt{25} = 5$\n",
        "\n",
        "### 28.4 Desempenho estimado\n",
        "\n",
        "* Para cada amostra de *bootstrap* tirada dos dados de treinamento, haverá amostras não incluídas. Essas amostras são chamadas amostras *out-of-bag* ou OOB;\n",
        "\n",
        "* O desempenho de cada modelo nas suas amostras não incluídas quando tirado a média pode indicar um acurácia estimada dos modelos usados no *bagging*. Esse desempenho estimado se chama OOB estimado, é uma estimação confiável dos erros e está bem correlacionado com a estimação de erro por *cross-validation*.\n",
        "\n",
        "### 28.5 Importância da variável\n",
        "\n",
        "* Como as árvores de decisão são construidas, podemos calcular o quanto a função de erro diminui para uma variável em cada ponto de divisão. Em problemas de regressão isso se parace com o caída na soma do quadrado dos erros e em classificação se parece com a taxa de Gini;\n",
        "\n",
        "* Com essa diminuição no erro pode-se calcular a média para todas as árvores de decisão e ter uma estimativa da importância de cada variável de entrada. Quanto maior a diminuição quando a variável é escolhida, maior sua importância;\n",
        "\n",
        "* Essa estimativa pode ajudar a identificar grupos de variáveis que são importantes ou não para o problema, sugeriando uma seleção de *features*.\n",
        "\n",
        "## Capítulo 29 - Tutorial de árvores de decisão com *bagging*\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsQ_izu-sDft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}