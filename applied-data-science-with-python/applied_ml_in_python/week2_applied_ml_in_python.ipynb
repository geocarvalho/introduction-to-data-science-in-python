{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week2_applied-ml-in-python.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMZhW/Wv+vmoay/erFlR/FF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geocarvalho/python-ds/blob/master/applied-data-science-with-python/applied_ml_in_python/week2_applied_ml_in_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zxvbtyh_qQGf",
        "colab_type": "text"
      },
      "source": [
        "# Applied ML in Python\n",
        "\n",
        "## Week 2 - Module 2 (Supervised Learning, Part I)\n",
        "\n",
        "### Introduction to supervised machine learning\n",
        "\n",
        "#### 1. Review\n",
        "\n",
        "* Feature representation;\n",
        "* Data instances/samples/examples (X);\n",
        "* Target value (y);\n",
        "* Training (75%) and test (25%) sets;\n",
        "* Model/Estimator\n",
        " * Model fitting produces a \"trained model\";\n",
        " * Training is the process of estimating model parameters.\n",
        "* Evaluation method (accuracy, f1-score, etc ).\n",
        "\n",
        "#### 2. Classification and regression\n",
        "\n",
        "* Both classification and regression take a set of training instances and learn a mapping to a **target value**;\n",
        "\n",
        "* For classification, the target value is a discrete class value.\n",
        " * Binary: target value is 0 (negative class) or 1 (positive class);\n",
        " > e.g. detecting a fraudulent credit card transaction.\n",
        " * Multi-class: target value is one of a set of discrete values;\n",
        " > e.g. labelling the type of fruit from physical attributes.\n",
        " * Multi-label: there are multiple values (labels).\n",
        " > e.g. labelling the topics discussed on a web page.\n",
        "\n",
        "* For regression, the target value is **continuous** (floating point/real-valued).\n",
        "  > e.g. predicting the selling price of a house from its attributes.\n",
        "\n",
        "* Looking at the target value's type will guide you on what supervised learning method to use.\n",
        "\n",
        "#### 3. The relationship between model complexity and training/test performance\n",
        "\n",
        "* In ML we use the term features to refer to the input (independent variables) and target value (label) to refer to the output (depedent variables).\n",
        "\n",
        "---\n",
        "\n",
        "### Overfitting and underfitting\n",
        "\n",
        "#### Generalization\n",
        "\n",
        "* Algorithm's ability to give accurate predictions for a new, previously unseen data\n",
        "\n",
        "  - Future unseen data (test set) will have the same properties as the current training sets;\n",
        "  - Thus, models that are accurate on the training set are expected to be accurate on the test set;\n",
        "  - But that may not happen if the trained models is tuned too specifically to the training set (overfitting).\n",
        "\n",
        "* Models that are too complex for the amount of training data available are said to **overfit** and are not likely to generalize well to new examples;\n",
        "\n",
        "* Models that are to simple, that don't even do well on the training data are said to **underfit** and also not likely to generalize well.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXdryIO2Fw5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://stackoverflow.com/questions/50665594/i-am-unable-to-install-adspy-shared-utilities-in-anaconda-using-the-command-cond\n",
        "# adspy_shared_utilities\n",
        "\n",
        "import numpy\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
        "from sklearn import neighbors\n",
        "import matplotlib.patches as mpatches\n",
        "import graphviz\n",
        "from sklearn.tree import export_graphviz\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "def load_crime_dataset():\n",
        "    # Communities and Crime dataset for regression\n",
        "    # https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized\n",
        "    uri = \"https://raw.githubusercontent.com/Starignus/AppliedML_Python_Coursera/master/CommViolPredUnnormalizedData.txt\"\n",
        "    crime = pd.read_table(uri, sep=',', na_values='?')\n",
        "    # remove features with poor coverage or lower relevance, and keep ViolentCrimesPerPop target column\n",
        "    columns_to_keep = [5, 6] + list(range(11,26)) + list(range(32, 103)) + [145]  \n",
        "    crime = crime.iloc[:,columns_to_keep].dropna()\n",
        "\n",
        "    X_crime = crime.iloc[:,range(0,88)]\n",
        "    y_crime = crime['ViolentCrimesPerPop']\n",
        "\n",
        "    return (X_crime, y_crime)\n",
        "\n",
        "def plot_decision_tree(clf, feature_names, class_names):\n",
        "    # This function requires the pydotplus module and assumes it's been installed.\n",
        "    # In some cases (typically under Windows) even after running conda install, there is a problem where the\n",
        "    # pydotplus module is not found when running from within the notebook environment.  The following code\n",
        "    # may help to guarantee the module is installed in the current notebook environment directory.\n",
        "    #\n",
        "    # import sys; sys.executable\n",
        "    # !{sys.executable} -m pip install pydotplus\n",
        "\n",
        "    export_graphviz(clf, out_file=\"adspy_temp.dot\", feature_names=feature_names, class_names=class_names, filled = True, impurity = False)\n",
        "    with open(\"adspy_temp.dot\") as f:\n",
        "        dot_graph = f.read()\n",
        "    # Alternate method using pydotplus, if installed.\n",
        "    # graph = pydotplus.graphviz.graph_from_dot_data(dot_graph)\n",
        "    # return graph.create_png()\n",
        "    return graphviz.Source(dot_graph)\n",
        "\n",
        "def plot_feature_importances(clf, feature_names):\n",
        "    c_features = len(feature_names)\n",
        "    plt.barh(range(c_features), clf.feature_importances_)\n",
        "    plt.xlabel(\"Feature importance\")\n",
        "    plt.ylabel(\"Feature name\")\n",
        "    plt.yticks(numpy.arange(c_features), feature_names)\n",
        "\n",
        "def plot_labelled_scatter(X, y, class_labels):\n",
        "    num_labels = len(class_labels)\n",
        "\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "\n",
        "    marker_array = ['o', '^', '*']\n",
        "    color_array = ['#FFFF00', '#00AAFF', '#000000', '#FF00AA']\n",
        "    cmap_bold = ListedColormap(color_array)\n",
        "    bnorm = BoundaryNorm(numpy.arange(0, num_labels + 1, 1), ncolors=num_labels)\n",
        "    plt.figure()\n",
        "\n",
        "    plt.scatter(X[:, 0], X[:, 1], s=65, c=y, cmap=cmap_bold, norm = bnorm, alpha = 0.40, edgecolor='black', lw = 1)\n",
        "\n",
        "    plt.xlim(x_min, x_max)\n",
        "    plt.ylim(y_min, y_max)\n",
        "\n",
        "    h = []\n",
        "    for c in range(0, num_labels):\n",
        "        h.append(mpatches.Patch(color=color_array[c], label=class_labels[c]))\n",
        "    plt.legend(handles=h)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_class_regions_for_classifier_subplot(clf, X, y, X_test, y_test, title, subplot, target_names = None, plot_decision_regions = True):\n",
        "\n",
        "    numClasses = numpy.amax(y) + 1\n",
        "    color_list_light = ['#FFFFAA', '#EFEFEF', '#AAFFAA', '#AAAAFF']\n",
        "    color_list_bold = ['#EEEE00', '#000000', '#00CC00', '#0000CC']\n",
        "    cmap_light = ListedColormap(color_list_light[0:numClasses])\n",
        "    cmap_bold  = ListedColormap(color_list_bold[0:numClasses])\n",
        "\n",
        "    h = 0.03\n",
        "    k = 0.5\n",
        "    x_plot_adjust = 0.1\n",
        "    y_plot_adjust = 0.1\n",
        "    plot_symbol_size = 50\n",
        "\n",
        "    x_min = X[:, 0].min()\n",
        "    x_max = X[:, 0].max()\n",
        "    y_min = X[:, 1].min()\n",
        "    y_max = X[:, 1].max()\n",
        "    x2, y2 = numpy.meshgrid(numpy.arange(x_min-k, x_max+k, h), numpy.arange(y_min-k, y_max+k, h))\n",
        "\n",
        "    P = clf.predict(numpy.c_[x2.ravel(), y2.ravel()])\n",
        "    P = P.reshape(x2.shape)\n",
        "\n",
        "    if plot_decision_regions:\n",
        "        subplot.contourf(x2, y2, P, cmap=cmap_light, alpha = 0.8)\n",
        "\n",
        "    subplot.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, s=plot_symbol_size, edgecolor = 'black')\n",
        "    subplot.set_xlim(x_min - x_plot_adjust, x_max + x_plot_adjust)\n",
        "    subplot.set_ylim(y_min - y_plot_adjust, y_max + y_plot_adjust)\n",
        "\n",
        "    if (X_test is not None):\n",
        "        subplot.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap_bold, s=plot_symbol_size, marker='^', edgecolor = 'black')\n",
        "        train_score = clf.score(X, y)\n",
        "        test_score  = clf.score(X_test, y_test)\n",
        "        title = title + \"\\nTrain score = {:.2f}, Test score = {:.2f}\".format(train_score, test_score)\n",
        "\n",
        "    subplot.set_title(title)\n",
        "\n",
        "    if (target_names is not None):\n",
        "        legend_handles = []\n",
        "        for i in range(0, len(target_names)):\n",
        "            patch = mpatches.Patch(color=color_list_bold[i], label=target_names[i])\n",
        "            legend_handles.append(patch)\n",
        "        subplot.legend(loc=0, handles=legend_handles)\n",
        "\n",
        "\n",
        "def plot_class_regions_for_classifier(clf, X, y, X_test=None, y_test=None, title=None, target_names = None, plot_decision_regions = True):\n",
        "\n",
        "    numClasses = numpy.amax(y) + 1\n",
        "    color_list_light = ['#FFFFAA', '#EFEFEF', '#AAFFAA', '#AAAAFF']\n",
        "    color_list_bold = ['#EEEE00', '#000000', '#00CC00', '#0000CC']\n",
        "    cmap_light = ListedColormap(color_list_light[0:numClasses])\n",
        "    cmap_bold  = ListedColormap(color_list_bold[0:numClasses])\n",
        "\n",
        "    h = 0.03\n",
        "    k = 0.5\n",
        "    x_plot_adjust = 0.1\n",
        "    y_plot_adjust = 0.1\n",
        "    plot_symbol_size = 50\n",
        "\n",
        "    x_min = X[:, 0].min()\n",
        "    x_max = X[:, 0].max()\n",
        "    y_min = X[:, 1].min()\n",
        "    y_max = X[:, 1].max()\n",
        "    x2, y2 = numpy.meshgrid(numpy.arange(x_min-k, x_max+k, h), numpy.arange(y_min-k, y_max+k, h))\n",
        "\n",
        "    P = clf.predict(numpy.c_[x2.ravel(), y2.ravel()])\n",
        "    P = P.reshape(x2.shape)\n",
        "    plt.figure()\n",
        "    if plot_decision_regions:\n",
        "        plt.contourf(x2, y2, P, cmap=cmap_light, alpha = 0.8)\n",
        "\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, s=plot_symbol_size, edgecolor = 'black')\n",
        "    plt.xlim(x_min - x_plot_adjust, x_max + x_plot_adjust)\n",
        "    plt.ylim(y_min - y_plot_adjust, y_max + y_plot_adjust)\n",
        "\n",
        "    if (X_test is not None):\n",
        "        plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap_bold, s=plot_symbol_size, marker='^', edgecolor = 'black')\n",
        "        train_score = clf.score(X, y)\n",
        "        test_score  = clf.score(X_test, y_test)\n",
        "        title = title + \"\\nTrain score = {:.2f}, Test score = {:.2f}\".format(train_score, test_score)\n",
        "\n",
        "    if (target_names is not None):\n",
        "        legend_handles = []\n",
        "        for i in range(0, len(target_names)):\n",
        "            patch = mpatches.Patch(color=color_list_bold[i], label=target_names[i])\n",
        "            legend_handles.append(patch)\n",
        "        plt.legend(loc=0, handles=legend_handles)\n",
        "\n",
        "    if (title is not None):\n",
        "        plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def plot_fruit_knn(X, y, n_neighbors, weights):\n",
        "    X_mat = X[['height', 'width']].as_matrix()\n",
        "    y_mat = y.as_matrix()\n",
        "\n",
        "    # Create color maps\n",
        "    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF','#AFAFAF'])\n",
        "    cmap_bold  = ListedColormap(['#FF0000', '#00FF00', '#0000FF','#AFAFAF'])\n",
        "\n",
        "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
        "    clf.fit(X_mat, y_mat)\n",
        "\n",
        "    # Plot the decision boundary by assigning a color in the color map\n",
        "    # to each mesh point.\n",
        "\n",
        "    mesh_step_size = .01  # step size in the mesh\n",
        "    plot_symbol_size = 50\n",
        "\n",
        "    x_min, x_max = X_mat[:, 0].min() - 1, X_mat[:, 0].max() + 1\n",
        "    y_min, y_max = X_mat[:, 1].min() - 1, X_mat[:, 1].max() + 1\n",
        "    xx, yy = numpy.meshgrid(numpy.arange(x_min, x_max, mesh_step_size),\n",
        "                         numpy.arange(y_min, y_max, mesh_step_size))\n",
        "    Z = clf.predict(numpy.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "    # Put the result into a color plot\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.figure()\n",
        "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
        "\n",
        "    # Plot training points\n",
        "    plt.scatter(X_mat[:, 0], X_mat[:, 1], s=plot_symbol_size, c=y, cmap=cmap_bold, edgecolor = 'black')\n",
        "    plt.xlim(xx.min(), xx.max())\n",
        "    plt.ylim(yy.min(), yy.max())\n",
        "\n",
        "    patch0 = mpatches.Patch(color='#FF0000', label='apple')\n",
        "    patch1 = mpatches.Patch(color='#00FF00', label='mandarin')\n",
        "    patch2 = mpatches.Patch(color='#0000FF', label='orange')\n",
        "    patch3 = mpatches.Patch(color='#AFAFAF', label='lemon')\n",
        "    plt.legend(handles=[patch0, patch1, patch2, patch3])\n",
        "\n",
        "\n",
        "    plt.xlabel('height (cm)')\n",
        "    plt.ylabel('width (cm)')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plot_two_class_knn(X, y, n_neighbors, weights, X_test, y_test):\n",
        "    X_mat = X\n",
        "    y_mat = y\n",
        "\n",
        "    # Create color maps\n",
        "    cmap_light = ListedColormap(['#FFFFAA', '#AAFFAA', '#AAAAFF','#EFEFEF'])\n",
        "    cmap_bold  = ListedColormap(['#FFFF00', '#00FF00', '#0000FF','#000000'])\n",
        "\n",
        "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
        "    clf.fit(X_mat, y_mat)\n",
        "\n",
        "    # Plot the decision boundary by assigning a color in the color map\n",
        "    # to each mesh point.\n",
        "\n",
        "    mesh_step_size = .01  # step size in the mesh\n",
        "    plot_symbol_size = 50\n",
        "\n",
        "    x_min, x_max = X_mat[:, 0].min() - 1, X_mat[:, 0].max() + 1\n",
        "    y_min, y_max = X_mat[:, 1].min() - 1, X_mat[:, 1].max() + 1\n",
        "    xx, yy = numpy.meshgrid(numpy.arange(x_min, x_max, mesh_step_size),\n",
        "                         numpy.arange(y_min, y_max, mesh_step_size))\n",
        "    Z = clf.predict(numpy.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "    # Put the result into a color plot\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.figure()\n",
        "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
        "\n",
        "    # Plot training points\n",
        "    plt.scatter(X_mat[:, 0], X_mat[:, 1], s=plot_symbol_size, c=y, cmap=cmap_bold, edgecolor = 'black')\n",
        "    plt.xlim(xx.min(), xx.max())\n",
        "    plt.ylim(yy.min(), yy.max())\n",
        "\n",
        "    title = \"Neighbors = {}\".format(n_neighbors)\n",
        "    if (X_test is not None):\n",
        "        train_score = clf.score(X_mat, y_mat)\n",
        "        test_score  = clf.score(X_test, y_test)\n",
        "        title = title + \"\\nTrain score = {:.2f}, Test score = {:.2f}\".format(train_score, test_score)\n",
        "\n",
        "    patch0 = mpatches.Patch(color='#FFFF00', label='class 0')\n",
        "    patch1 = mpatches.Patch(color='#000000', label='class 1')\n",
        "    plt.legend(handles=[patch0, patch1])\n",
        "\n",
        "    plt.xlabel('Feature 0')\n",
        "    plt.ylabel('Feature 1')\n",
        "    plt.title(title)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljZfz01u_bHT",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Preamble and Review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yuDckDa1GdL",
        "colab_type": "code",
        "outputId": "3680d2c3-185e-4c86-e7c8-894aa00aa9c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "uri = \"https://raw.githubusercontent.com/susanli2016/Machine-Learning-with-Python/master/fruit_data_with_colors.txt\"\n",
        "fruits = pd.read_table(uri)\n",
        "\n",
        "feature_names_fruits = ['height', 'width', 'mass', 'color_score']\n",
        "X_fruits = fruits[feature_names_fruits]\n",
        "y_fruits = fruits['fruit_label']\n",
        "target_names_fruits = ['apple', 'mandarin', 'orange', 'lemon']\n",
        "\n",
        "X_fruits_2d = fruits[['height', 'width']]\n",
        "y_fruits_2d = fruits['fruit_label']\n",
        "\n",
        "cmap_bold = ListedColormap(['#FFFF00', '#00FF00', '#0000FF', '#000000'])\n",
        "plt.figure()\n",
        "plt.title('Sample regression problem with one input variable')\n",
        "X_R1, y_R1 = make_regression(n_samples=100, n_features=1,\n",
        "                             n_informative=1, bias=150.0,\n",
        "                             noise=30, random_state=0)\n",
        "plt.scatter(X_R1, y_R1, marker='o', s=50)\n",
        "plt.show()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "optjsOct243G",
        "colab_type": "code",
        "outputId": "20cfcd7f-a901-496b-f5e1-8c17dd79fa02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "from sklearn.datasets import make_friedman1, make_classification\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Complex regressio problem with one input variable\")\n",
        "X_F1, y_F1 = make_friedman1(n_samples=100, n_features=7, random_state=0)\n",
        "plt.scatter(X_F1[:, 2], y_F1, marker='o', s=50)\n",
        "plt.show()\n",
        "\n",
        "# plt.figure()\n",
        "# plt.title(\"Sample binary classification problem with two informative features\")\n",
        "# X_C2, y_C2 = make_classification(n_samples=100, n_features=2,\n",
        "#                                  n_redundant=0, n_informative=2,\n",
        "#                                  class_sep=0.5, random_state=0)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5-KuWKS4Fva",
        "colab_type": "code",
        "outputId": "2ba93e20-b09f-4fb1-89b7-b2592520fa26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.datasets import load_boston # https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized\n",
        "\n",
        "\n",
        "X_D2, y_D2 = make_blobs(n_samples=100, n_features=2,\n",
        "                        centers=8, cluster_std=1.3,\n",
        "                        random_state=4)\n",
        "\n",
        "y_D2 = y_D2 % 2\n",
        "plt.figure()\n",
        "plt.title(\"Sample binary classification problem with non-linearly separable classes\")\n",
        "plt.scatter(X_D2[:,0], X_D2[:,1], c=y_D2,\n",
        "            marker='o', s=50, cmap=cmap_bold)\n",
        "plt.show()\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "(X_cancer, y_cancer) = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "(X_crime, y_crime) = load_boston(return_X_y=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFuDUdumA1nr",
        "colab_type": "text"
      },
      "source": [
        "### The k-NN classifier algorithm\n",
        "\n",
        "1. Find the most similar instances (X_NN) to X_test that are in X_train;\n",
        "\n",
        "2. Get the labels y_NN for instances in X_NN;\n",
        "\n",
        "3. Predict the label for X_test by combining the labels y_NN.\n",
        "> e.g. simple majority vote.\n",
        "\n",
        "* With increasing k, single training data points no longer have as dramatic influence the prediction;\n",
        "\n",
        "> It means lower underfitting when we increase the k value (lower model complexity);\n",
        "\n",
        "> The decision boundary has much less variance.\n",
        "\n",
        "* If we increase k to be the total number of points in the training set, the result is a single decision region where all predictions would be the most frequent class.\n",
        "\n",
        "#### k-NN classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmenDEPyF30z",
        "colab_type": "code",
        "outputId": "91b797dd-e280-46d9-a083-21c3957ff515",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2, random_state=0)\n",
        "\n",
        "plot_two_class_knn(X_train, y_train, 1, 'uniform', X_test, y_test)\n",
        "plot_two_class_knn(X_train, y_train, 3, 'uniform', X_test, y_test)\n",
        "plot_two_class_knn(X_train, y_train, 11, 'uniform', X_test, y_test)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-14f58552891f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_C2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_C2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplot_two_class_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'uniform'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplot_two_class_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'uniform'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplot_two_class_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'uniform'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_C2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvRMinEEGjuX",
        "colab_type": "text"
      },
      "source": [
        "#### k-NN regression\n",
        "\n",
        "* Similar to the classification, it uses the average of the datas near our value.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7xonzkHHpbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_R1, y_R1, random_state=0)\n",
        "knnreg = KNeighborsRegressor(n_neighbors=5).fit(X_train, y_train)\n",
        "\n",
        "print(knnreg.predict(X_test))\n",
        "print(\"R-squared test score: {:.3f}\".format(\n",
        "    knnreg.score(X_test, y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRMsY1emK1QM",
        "colab_type": "text"
      },
      "source": [
        "#### The r-squared ($R^2$) regression score\n",
        "\n",
        "* Measures how well a prediction model for regression fits the given data;\n",
        "\n",
        "* The score is between 0 and 1:\n",
        " - A value of 0 corresponds to a constant model that predicts the mean value of all training target values;\n",
        " - A value of 1 corresponds to perfect predictions.\n",
        "\n",
        "* Also known as **coefficient of determination\". \n",
        "\n",
        "#### Regression model complexity as a function of K"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY4aoqrRLjMK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, subaxes = plt.subplots(5, 1, figsize=(5,20))\n",
        "X_predict_input = np.linspace(-3, 3, 500).reshape(-1, 1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1,\n",
        "                                                    random_state=0)\n",
        "\n",
        "for thisaxis, K in zip(subaxes, [1,3,7,15,55]):\n",
        "  knnreg = KNeighborsRegressor(n_neighbors=K).fit(X_train, y_train)\n",
        "  y_predict_output = knnreg.predict(X_predict_input)\n",
        "  train_score = knnreg.score(X_train, y_train)\n",
        "  test_score = knnreg.score(X_test, y_test)\n",
        "  thisaxis.plot(X_predict_input, y_predict_output)\n",
        "  thisaxis.plot(X_train, y_train, 'o', alpha=0.9, label=\"Train\")\n",
        "  thisaxis.plot(X_test, y_test, '^', alpha=0.9, label=\"Test\")\n",
        "  thisaxis.set_xlabel(\"Input feature\")\n",
        "  thisaxis.set_ylabel(\"Target value\")\n",
        "  thisaxis.set_title(\"KNN Regression (K={})\\n\\\n",
        "  Train $R^2 = {:.3f}$, Test $R^2 = {:.3f}$\".format(\n",
        "      K, train_score, test_score))\n",
        "  thisaxis.legend()\n",
        "  plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBgkPnhkNXfc",
        "colab_type": "text"
      },
      "source": [
        "* As before, small values of **k** give models with higher complexity and large values of **k** result in simpler models with lower complexity;\n",
        "\n",
        "* Until k=55, where the model starts to underfit, it's too simple to do well, even on the training data.\n",
        "\n",
        "* For k-NN, whe the training data has many instances, or each instance has lots of features, this can slow down the performance of the model (e.g. sparce datasets)\n",
        "\n",
        "#### k-NN classifier and k-NN regressor important parameters\n",
        "\n",
        "* Model complexity\n",
        " - `n_neighbors`: number of nearest neighbors (k) to consider.\n",
        " > default = 5.\n",
        "\n",
        "* Model fitting\n",
        " - `metric`: distance function between data points.\n",
        " > default = Minkowski distance with power parameter p=2 (Euclidean).\n",
        "\n",
        "---\n",
        "\n",
        "### Linear regression: least-squares\n",
        "\n",
        "#### Linear models\n",
        "\n",
        "* Is a *sum of weighted variables* that predicts a target output value given an input data instance;\n",
        "\n",
        "#### A linear regression model with one variable (feature)\n",
        "\n",
        "* input instance: $x = (x_0)$\n",
        "* predicted output: $\\hat{y} = \\hat{w}_0 x_0 + \\hat{b}$\n",
        "* parameters to estimate: \n",
        " * $\\hat{w}_0$ (slope)\n",
        " * $\\hat{b}$ (y-intercept)\n",
        "\n",
        "#### Least-squares linear regression (ordinary least-squares)\n",
        "\n",
        "* Finds $w$ and $b$ that minimizes the mean squared error of the model;\n",
        "> The sum of squared differences (RSS - residual sum of squares) between predicted target and actual target values.\n",
        "$RSS(w,b) = \\sum^N_{i=1} (y_i - (w \\times  x_i + b))^2$\n",
        "\n",
        "* No parameters to control model complexity.\n",
        "\n",
        "#### How are linear regression parameters $w$ and $b$ estimated?\n",
        "\n",
        "* They are estimated from training data;\n",
        "* There are many different ways to estimate $w$ and $b$:\n",
        " * Different methods correspond to diffrent *fit* criteria, goals and ways to control model complexity.\n",
        "\n",
        "* The learning algorithm finds the parameters that optimize an *objective function*, typically to minimize some kind of *loss function* of the predicted target values vs. actual target values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA_D1fruQbAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1,\n",
        "                                                   random_state = 0)\n",
        "linreg = LinearRegression().fit(X_train, y_train)\n",
        "\n",
        "print('linear model coeff (w): {}'\n",
        "     .format(linreg.coef_))\n",
        "print('linear model intercept (b): {:.3f}'\n",
        "     .format(linreg.intercept_))\n",
        "print('R-squared score (training): {:.3f}'\n",
        "     .format(linreg.score(X_train, y_train)))\n",
        "print('R-squared score (test): {:.3f}'\n",
        "     .format(linreg.score(X_test, y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HVFqKjJRUf6",
        "colab_type": "text"
      },
      "source": [
        "> Underscore denotes a quantity derived from training data, as opposed to a user setting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAsxxqjMR898",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(5,4))\n",
        "plt.scatter(X_R1, y_R1, marker= 'o', s=50, alpha=0.8)\n",
        "plt.plot(X_R1, linreg.coef_ * X_R1 + linreg.intercept_, 'r-')\n",
        "plt.title('Least-squares linear regression')\n",
        "plt.xlabel('Feature value (x)')\n",
        "plt.ylabel('Target value (y)')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdAUFi00SL-J",
        "colab_type": "text"
      },
      "source": [
        "* The k-NN regressor doesn't make a lot of assumptions for k=7 about the structure of the data, and gives potentially accurate but sometimes unstable predictions that are sensitive to small changes in the training data;\n",
        "> So it has a correspondingly higher training set, r-squared score (0.72) compared to least-squares linear regression (0.679).\n",
        "\n",
        "* On the other hand, linear models make strong assumptions about the structure of the data, in other words, that the target value can be predicted using a weighted sum of the input variables. It give stable but potentially inaccurate predictions. However in this case, it turns out that the linear model has a strong assumption that there's a linear relationship between the input and output variables happes to be a good fit for this dataset.\n",
        "> And so it's better at more accurately predicting the y value for new x values that weren't seen during training. And we can see that the linear model gets a slightly better test set score (0.492) versus k-NN (0.471).\n",
        "\n",
        "---\n",
        "\n",
        "### Linear regression: Ridge, lasso, and polynomial regression\n",
        "\n",
        "* Learns $w$, $b$ using the same least-squares criterion but adds a penalty for large variations in $w$ parameters;\n",
        "\n",
        "$RSS_{ridge}(w,b) = \\sum_{i=1}^N (y_i - (w \\times x_i + b))^2 + \\alpha \\sum_{j=1}^p w_{j}^2$\n",
        "\n",
        "* Once the parameters are learned, the ridge regression prediction formula is the same as ordinary least-squares;\n",
        "\n",
        "* The addition of a parameter penalty is called **regularization*. It prevents overfitting by restricting the model, typically to reduce its complexity;\n",
        "\n",
        "* Ridge regression uses **L2 regularization**: minimize sum of squares of $w$ entries;\n",
        "\n",
        "* The influence of the regularization term is controlled by the $\\alpha$ parameter;\n",
        "> the default value is 1.0, setting alpha to 0 is the case of ordinary least-squares linear regression.\n",
        "\n",
        "* Higher $\\alpha$ means more regularization and simpler models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uR4mVr1-lIBn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "e34c3913-4232-4346-8e16-ac990172c038"
      },
      "source": [
        "# Ridge regression\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "(X_crime, y_crime) = load_crime_dataset()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
        "                                                   random_state = 0)\n",
        "\n",
        "linridge = Ridge(alpha=20.0).fit(X_train, y_train)\n",
        "\n",
        "print('Crime dataset')\n",
        "print('ridge regression linear model intercept: {}'\n",
        "     .format(linridge.intercept_))\n",
        "print('ridge regression linear model coeff:\\n{}'\n",
        "     .format(linridge.coef_))\n",
        "print('R-squared score (training): {:.3f}'\n",
        "     .format(linridge.score(X_train, y_train)))\n",
        "print('R-squared score (test): {:.3f}'\n",
        "     .format(linridge.score(X_test, y_test)))\n",
        "print('Number of non-zero features: {}'\n",
        "     .format(np.sum(linridge.coef_ != 0)))\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Crime dataset\n",
            "ridge regression linear model intercept: -3352.423035846137\n",
            "ridge regression linear model coeff:\n",
            "[ 1.95e-03  2.19e+01  9.56e+00 -3.59e+01  6.36e+00 -1.97e+01 -2.81e-03\n",
            "  1.66e+00 -6.61e-03 -6.95e+00  1.72e+01 -5.63e+00  8.84e+00  6.79e-01\n",
            " -7.34e+00  6.70e-03  9.79e-04  5.01e-03 -4.90e+00 -1.79e+01  9.18e+00\n",
            " -1.24e+00  1.22e+00  1.03e+01 -3.78e+00 -3.73e+00  4.75e+00  8.43e+00\n",
            "  3.09e+01  1.19e+01 -2.05e+00 -3.82e+01  1.85e+01  1.53e+00 -2.20e+01\n",
            "  2.46e+00  3.29e-01  4.02e+00 -1.13e+01 -4.70e-03  4.27e+01 -1.23e-03\n",
            "  1.41e+00  9.35e-01 -3.00e+00  1.12e+00 -1.82e+01 -1.55e+01  2.42e+01\n",
            " -1.32e+01 -4.20e-01 -3.60e+01  1.30e+01 -2.81e+01  4.39e+01  3.87e+01\n",
            " -6.46e+01 -1.64e+01  2.90e+01  4.15e+00  5.34e+01  1.99e-02 -5.47e-01\n",
            "  1.24e+01  1.04e+01 -1.57e+00  3.16e+00  8.78e+00 -2.95e+01 -2.33e-04\n",
            "  3.14e-04 -4.14e-04 -1.79e-04 -5.74e-01 -5.18e-01 -4.21e-01  1.53e-01\n",
            "  1.33e+00  3.85e+00  3.03e+00 -3.78e+01  1.38e-01  3.08e-01  1.57e+01\n",
            "  3.31e-01  3.36e+00  1.61e-01 -2.68e+00]\n",
            "R-squared score (training): 0.671\n",
            "R-squared score (test): 0.494\n",
            "Number of non-zero features: 88\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3CsMbEemGBZ",
        "colab_type": "text"
      },
      "source": [
        "#### The need for feature normalization\n",
        "\n",
        "* Importante for some ML methods that all features are on the same scale;\n",
        "> e.g. faster convergence in learning, more uniform of 'fair' influence for all weights;\n",
        "\n",
        " > e.g. regularized regression, k-NN, SVM, NN, ..\n",
        "\n",
        "* Can also depend on the data. More on feature engineering later in the course. For now, we do `MinMax` scaling of the features:\n",
        " * For each feature $x_i$: compute the min value $x_i^{MIN}$ and the max value $x_i^{MAX}$ achieved across all instances in the training set;\n",
        " * For each feature: transform a given feature $x_i$ value to a scaled version $x_i'$ using the formula:\n",
        "$x_i' = (x_i - x_i^{MIN})/(x_i^{MAX} - x_i^{MIN})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgPg5umql1uW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "71456f5d-3b72-4461-a546-dfb254ee82ec"
      },
      "source": [
        "# Ridge regression with feature normalization\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
        "                                                   random_state = 0)\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "linridge = Ridge(alpha=20.0).fit(X_train_scaled, y_train)\n",
        "\n",
        "print('Crime dataset')\n",
        "print('ridge regression linear model intercept: {}'\n",
        "     .format(linridge.intercept_))\n",
        "print('ridge regression linear model coeff:\\n{}'\n",
        "     .format(linridge.coef_))\n",
        "print('R-squared score (training): {:.3f}'\n",
        "     .format(linridge.score(X_train_scaled, y_train)))\n",
        "print('R-squared score (test): {:.3f}'\n",
        "     .format(linridge.score(X_test_scaled, y_test)))\n",
        "print('Number of non-zero features: {}'\n",
        "     .format(np.sum(linridge.coef_ != 0)))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Crime dataset\n",
            "ridge regression linear model intercept: 933.3906385044153\n",
            "ridge regression linear model coeff:\n",
            "[  88.69   16.49  -50.3   -82.91  -65.9    -2.28   87.74  150.95   18.88\n",
            "  -31.06  -43.14 -189.44   -4.53  107.98  -76.53    2.86   34.95   90.14\n",
            "   52.46  -62.11  115.02    2.67    6.94   -5.67 -101.55  -36.91   -8.71\n",
            "   29.12  171.26   99.37   75.07  123.64   95.24 -330.61 -442.3  -284.5\n",
            " -258.37   17.66 -101.71  110.65  523.14   24.82    4.87  -30.47   -3.52\n",
            "   50.58   10.85   18.28   44.11   58.34   67.09  -57.94  116.14   53.81\n",
            "   49.02   -7.62   55.14  -52.09  123.39   77.13   45.5   184.91  -91.36\n",
            "    1.08  234.09   10.39   94.72  167.92  -25.14   -1.18   14.6    36.77\n",
            "   53.2   -78.86   -5.9    26.05  115.15   68.74   68.29   16.53  -97.91\n",
            "  205.2    75.97   61.38  -79.83   67.27   95.67  -11.88]\n",
            "R-squared score (training): 0.615\n",
            "R-squared score (test): 0.599\n",
            "Number of non-zero features: 88\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJvPtKcJoCdT",
        "colab_type": "text"
      },
      "source": [
        "#### Feature normalization: the test set must use identical scaling to the training set\n",
        "\n",
        "* Fit the scaler using the training set, then apply the same scaler to transform the test set;\n",
        "\n",
        "* Don't scaler the training and test sets using different scalers: this could lead to random skew in the data;\n",
        "\n",
        "* Don't fit the scaler using any part of the test data: refering the test data can lead to a form of *data leakage*;\n",
        "> The training phase has information that is leaked from the test set. The learning method can give unrealistically good estimates on the same test set.\n",
        "\n",
        "* The resulting model and the transformed features my be harder to interpret.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xBE6WYnpxWa",
        "colab_type": "text"
      },
      "source": [
        "* Generally, regularization works well when we have relatively small amounts of training data compared to the number of features in the model;\n",
        "\n",
        "* Regularization becomes less important as the amount of training data you have increases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cHcuGf6qMEI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "66dd9fa0-301f-4ba0-cdd2-498f6d97c3c0"
      },
      "source": [
        "# Ridge regression with regularization parameter: alpha\n",
        "print('Ridge regression: effect of alpha regularization parameter\\n')\n",
        "for this_alpha in [0, 1, 10, 20, 50, 100, 1000]:\n",
        "    linridge = Ridge(alpha = this_alpha).fit(X_train_scaled, y_train)\n",
        "    r2_train = linridge.score(X_train_scaled, y_train)\n",
        "    r2_test = linridge.score(X_test_scaled, y_test)\n",
        "    num_coeff_bigger = np.sum(abs(linridge.coef_) > 1.0)\n",
        "    print('Alpha = {:.2f}\\nnum abs(coeff) > 1.0: {}, \\\n",
        "r-squared training: {:.2f}, r-squared test: {:.2f}\\n'\n",
        "         .format(this_alpha, num_coeff_bigger, r2_train, r2_test))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ridge regression: effect of alpha regularization parameter\n",
            "\n",
            "Alpha = 0.00\n",
            "num abs(coeff) > 1.0: 88, r-squared training: 0.67, r-squared test: 0.50\n",
            "\n",
            "Alpha = 1.00\n",
            "num abs(coeff) > 1.0: 87, r-squared training: 0.66, r-squared test: 0.56\n",
            "\n",
            "Alpha = 10.00\n",
            "num abs(coeff) > 1.0: 87, r-squared training: 0.63, r-squared test: 0.59\n",
            "\n",
            "Alpha = 20.00\n",
            "num abs(coeff) > 1.0: 88, r-squared training: 0.61, r-squared test: 0.60\n",
            "\n",
            "Alpha = 50.00\n",
            "num abs(coeff) > 1.0: 86, r-squared training: 0.58, r-squared test: 0.58\n",
            "\n",
            "Alpha = 100.00\n",
            "num abs(coeff) > 1.0: 87, r-squared training: 0.55, r-squared test: 0.55\n",
            "\n",
            "Alpha = 1000.00\n",
            "num abs(coeff) > 1.0: 84, r-squared training: 0.31, r-squared test: 0.30\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hcvHt-BqgO_",
        "colab_type": "text"
      },
      "source": [
        "#### Lasso regression is another form of regularized linear regression that uses an **L1 regularization** penalty for training (instead of ridge's L2 penalty)\n",
        "\n",
        "* L1 penalty: Minimize the sum of the **absolute values** of the coefficients:\n",
        "$ RSS_{lasso}(w,b) = \\sum_{i=1}^N (y_i - (w \\times x_i + b))^2 + \\alpha \\sum_{j=1}^p |w_j|$\n",
        "\n",
        "* This has the effect of setting parameter weights in $w$ to **zero** for the least influential variables. This is called a **sparse** solution,a kind of feature selection;\n",
        "\n",
        "* The parameter $\\alpha$ controls the amount of L1 regularization (default=1.0);\n",
        "\n",
        "* The prediction formula is the same as ordinary least-squares;\n",
        "\n",
        "* When to use ridge vs lasso regression:\n",
        " * Ridge: many small/medium sized effects.\n",
        " * Lasso: only a few variables with medium/large effect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrhJqxEyr6pv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "outputId": "aa516703-b77c-4711-ac62-19df8f0d11b0"
      },
      "source": [
        "# Lasso regression\n",
        "from sklearn.linear_model import Lasso\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
        "                                                   random_state = 0)\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "linlasso = Lasso(alpha=2.0, max_iter = 10000).fit(X_train_scaled, y_train)\n",
        "\n",
        "print('Crime dataset')\n",
        "print('lasso regression linear model intercept: {}'\n",
        "     .format(linlasso.intercept_))\n",
        "print('lasso regression linear model coeff:\\n{}'\n",
        "     .format(linlasso.coef_))\n",
        "print('Non-zero features: {}'\n",
        "     .format(np.sum(linlasso.coef_ != 0)))\n",
        "print('R-squared score (training): {:.3f}'\n",
        "     .format(linlasso.score(X_train_scaled, y_train)))\n",
        "print('R-squared score (test): {:.3f}\\n'\n",
        "     .format(linlasso.score(X_test_scaled, y_test)))\n",
        "print('Features with non-zero weight (sorted by absolute magnitude):')\n",
        "\n",
        "for e in sorted (list(zip(list(X_crime), linlasso.coef_)),key = lambda e: -abs(e[1])):\n",
        "  if e[1] != 0:\n",
        "    print('\\t{}, {:.3f}'.format(e[0], e[1]))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Crime dataset\n",
            "lasso regression linear model intercept: 1186.612061998579\n",
            "lasso regression linear model coeff:\n",
            "[    0.       0.      -0.    -168.18    -0.      -0.       0.     119.69\n",
            "     0.      -0.       0.    -169.68    -0.       0.      -0.       0.\n",
            "     0.       0.      -0.      -0.       0.      -0.       0.       0.\n",
            "   -57.53    -0.      -0.       0.     259.33    -0.       0.       0.\n",
            "     0.      -0.   -1188.74    -0.      -0.      -0.    -231.42     0.\n",
            "  1488.37     0.      -0.      -0.      -0.       0.       0.       0.\n",
            "     0.       0.      -0.       0.      20.14     0.       0.       0.\n",
            "     0.       0.     339.04     0.       0.     459.54    -0.       0.\n",
            "   122.69    -0.      91.41     0.      -0.       0.       0.      73.14\n",
            "     0.      -0.       0.       0.      86.36     0.       0.       0.\n",
            "  -104.57   264.93     0.      23.45   -49.39     0.       5.2      0.  ]\n",
            "Non-zero features: 20\n",
            "R-squared score (training): 0.631\n",
            "R-squared score (test): 0.624\n",
            "\n",
            "Features with non-zero weight (sorted by absolute magnitude):\n",
            "\tPctKidsBornNeverMar, 1488.365\n",
            "\tPctKids2Par, -1188.740\n",
            "\tHousVacant, 459.538\n",
            "\tPctPersDenseHous, 339.045\n",
            "\tNumInShelters, 264.932\n",
            "\tMalePctDivorce, 259.329\n",
            "\tPctWorkMom, -231.423\n",
            "\tpctWInvInc, -169.676\n",
            "\tagePct12t29, -168.183\n",
            "\tPctVacantBoarded, 122.692\n",
            "\tpctUrban, 119.694\n",
            "\tMedOwnCostPctIncNoMtg, -104.571\n",
            "\tMedYrHousBuilt, 91.412\n",
            "\tRentQrange, 86.356\n",
            "\tOwnOccHiQuart, 73.144\n",
            "\tPctEmplManu, -57.530\n",
            "\tPctBornSameState, -49.394\n",
            "\tPctForeignBorn, 23.449\n",
            "\tPctLargHouseFam, 20.144\n",
            "\tPctSameCity85, 5.198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xio3CuAvT_j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "a0cc61a1-b18d-4ca6-8d89-97c91b2dce80"
      },
      "source": [
        "# Lasso regressuib with regularization parameter: alpha\n",
        "print('Lasso regression: effect of alpha regularization\\n\\\n",
        "parameter on number of features kept in final model\\n')\n",
        "\n",
        "for alpha in [0.5, 1, 2, 3, 5, 10, 20, 50]:\n",
        "    linlasso = Lasso(alpha, max_iter = 10000).fit(X_train_scaled, y_train)\n",
        "    r2_train = linlasso.score(X_train_scaled, y_train)\n",
        "    r2_test = linlasso.score(X_test_scaled, y_test)\n",
        "    \n",
        "    print('Alpha = {:.2f}\\nFeatures kept: {}, r-squared training: {:.2f}, \\\n",
        "r-squared test: {:.2f}\\n'\n",
        "         .format(alpha, np.sum(linlasso.coef_ != 0), r2_train, r2_test))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lasso regression: effect of alpha regularization\n",
            "parameter on number of features kept in final model\n",
            "\n",
            "Alpha = 0.50\n",
            "Features kept: 35, r-squared training: 0.65, r-squared test: 0.58\n",
            "\n",
            "Alpha = 1.00\n",
            "Features kept: 25, r-squared training: 0.64, r-squared test: 0.60\n",
            "\n",
            "Alpha = 2.00\n",
            "Features kept: 20, r-squared training: 0.63, r-squared test: 0.62\n",
            "\n",
            "Alpha = 3.00\n",
            "Features kept: 17, r-squared training: 0.62, r-squared test: 0.63\n",
            "\n",
            "Alpha = 5.00\n",
            "Features kept: 12, r-squared training: 0.60, r-squared test: 0.61\n",
            "\n",
            "Alpha = 10.00\n",
            "Features kept: 6, r-squared training: 0.57, r-squared test: 0.58\n",
            "\n",
            "Alpha = 20.00\n",
            "Features kept: 2, r-squared training: 0.51, r-squared test: 0.50\n",
            "\n",
            "Alpha = 50.00\n",
            "Features kept: 1, r-squared training: 0.31, r-squared test: 0.30\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvLacFgDr5xr",
        "colab_type": "text"
      },
      "source": [
        "* The alpha value will be different for different data sets, and depends of the feature preprocessing being used;\n",
        "\n",
        "#### Polynomial features with linear regression\n",
        "\n",
        "$x = (x_0, x_1)$\n",
        "\n",
        "$x' = (x_0, x_1, x_0^2, x_0x_1, x_1^2)$\n",
        "\n",
        "$\\hat{y} = \\hat{w_0}x_0 + \\hat{w_1}x_1 + \\hat{w_{00}}x_0^2 + \\hat{w_{01}}x_0x_1 + \\hat{w_{11}}x_1^2 + b$\n",
        "\n",
        "* Generate new features consisting of all polynomial combinations of the original two features ($x_0,x_1$);\n",
        "\n",
        "* The degree of the polynomial specifies how many variables participate at a time in each new feature (above example: degree 2);\n",
        "\n",
        "* This is still a weighted linear combination of features, so it's still a linear model, and can use the same least-squares estimation method for $w$ and $b$;\n",
        "\n",
        "* With polunomial feature transformation we can transform a problem into a higher dimensional regression space, allowing a much richer set of complex functions that we can use to fit to the data;\n",
        "\n",
        "* Why would we want to transform our data this way?\n",
        " * To capture interactions between the original features by adding them as features to the linear model;\n",
        " * To make a classification problem easier.\n",
        "\n",
        "* More generally, we can apply other non-linear transformations to create new features;\n",
        "> Technically, these are called non-linear basis functions.\n",
        "\n",
        "* Beware of polynomial feature expansion with high degree, as this can lead to complex models that overfit;\n",
        "> Thus. polynomial feature expansion os often combined with a regularized learning method like ridge regression.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zYuRg3vx48N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "outputId": "198f0a2f-17d0-4fa8-f4cd-19e7ddb5bcb1"
      },
      "source": [
        "# Polynomial regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_F1, y_F1,\n",
        "                                                   random_state = 0)\n",
        "linreg = LinearRegression().fit(X_train, y_train)\n",
        "\n",
        "print('linear model coeff (w): {}'\n",
        "     .format(linreg.coef_))\n",
        "print('linear model intercept (b): {:.3f}'\n",
        "     .format(linreg.intercept_))\n",
        "print('R-squared score (training): {:.3f}'\n",
        "     .format(linreg.score(X_train, y_train)))\n",
        "print('R-squared score (test): {:.3f}'\n",
        "     .format(linreg.score(X_test, y_test)))\n",
        "\n",
        "# 2 regression\n",
        "print('\\nNow we transform the original input data to add\\n\\\n",
        "polynomial features up to degree 2 (quadratic)\\n')\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_F1_poly = poly.fit_transform(X_F1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y_F1,\n",
        "                                                   random_state = 0)\n",
        "linreg = LinearRegression().fit(X_train, y_train)\n",
        "\n",
        "print('(poly deg 2) linear model coeff (w):\\n{}'\n",
        "     .format(linreg.coef_))\n",
        "print('(poly deg 2) linear model intercept (b): {:.3f}'\n",
        "     .format(linreg.intercept_))\n",
        "print('(poly deg 2) R-squared score (training): {:.3f}'\n",
        "     .format(linreg.score(X_train, y_train)))\n",
        "print('(poly deg 2) R-squared score (test): {:.3f}\\n'\n",
        "     .format(linreg.score(X_test, y_test)))\n",
        "\n",
        "# 3 regression\n",
        "print('\\nAddition of many polynomial features often leads to\\n\\\n",
        "overfitting, so we often use polynomial features in combination\\n\\\n",
        "with regression that has a regularization penalty, like ridge\\n\\\n",
        "regression.\\n')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y_F1,\n",
        "                                                   random_state = 0)\n",
        "linreg = Ridge().fit(X_train, y_train)\n",
        "\n",
        "print('(poly deg 2 + ridge) linear model coeff (w):\\n{}'\n",
        "     .format(linreg.coef_))\n",
        "print('(poly deg 2 + ridge) linear model intercept (b): {:.3f}'\n",
        "     .format(linreg.intercept_))\n",
        "print('(poly deg 2 + ridge) R-squared score (training): {:.3f}'\n",
        "     .format(linreg.score(X_train, y_train)))\n",
        "print('(poly deg 2 + ridge) R-squared score (test): {:.3f}'\n",
        "     .format(linreg.score(X_test, y_test)))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "linear model coeff (w): [ 4.42  6.    0.53 10.24  6.55 -2.02 -0.32]\n",
            "linear model intercept (b): 1.543\n",
            "R-squared score (training): 0.722\n",
            "R-squared score (test): 0.722\n",
            "\n",
            "Now we transform the original input data to add\n",
            "polynomial features up to degree 2 (quadratic)\n",
            "\n",
            "(poly deg 2) linear model coeff (w):\n",
            "[ 3.41e-12  1.66e+01  2.67e+01 -2.21e+01  1.24e+01  6.93e+00  1.05e+00\n",
            "  3.71e+00 -1.34e+01 -5.73e+00  1.62e+00  3.66e+00  5.05e+00 -1.46e+00\n",
            "  1.95e+00 -1.51e+01  4.87e+00 -2.97e+00 -7.78e+00  5.15e+00 -4.65e+00\n",
            "  1.84e+01 -2.22e+00  2.17e+00 -1.28e+00  1.88e+00  1.53e-01  5.62e-01\n",
            " -8.92e-01 -2.18e+00  1.38e+00 -4.90e+00 -2.24e+00  1.38e+00 -5.52e-01\n",
            " -1.09e+00]\n",
            "(poly deg 2) linear model intercept (b): -3.206\n",
            "(poly deg 2) R-squared score (training): 0.969\n",
            "(poly deg 2) R-squared score (test): 0.805\n",
            "\n",
            "\n",
            "Addition of many polynomial features often leads to\n",
            "overfitting, so we often use polynomial features in combination\n",
            "with regression that has a regularization penalty, like ridge\n",
            "regression.\n",
            "\n",
            "(poly deg 2 + ridge) linear model coeff (w):\n",
            "[ 0.    2.23  4.73 -3.15  3.86  1.61 -0.77 -0.15 -1.75  1.6   1.37  2.52\n",
            "  2.72  0.49 -1.94 -1.63  1.51  0.89  0.26  2.05 -1.93  3.62 -0.72  0.63\n",
            " -3.16  1.29  3.55  1.73  0.94 -0.51  1.7  -1.98  1.81 -0.22  2.88 -0.89]\n",
            "(poly deg 2 + ridge) linear model intercept (b): 5.418\n",
            "(poly deg 2 + ridge) R-squared score (training): 0.826\n",
            "(poly deg 2 + ridge) R-squared score (test): 0.825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztKIwlXjqrnd",
        "colab_type": "text"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RbmBQtjqpKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.datasets import make_classification, make_blobs\n",
        "# from matplotlib.colors import ListedColormap\n",
        "# from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# cmap_bold = ListedColormap(['#FFFF00', '#00FF00', '#0000FF','#000000'])\n",
        "\n",
        "\n",
        "# # synthetic dataset for simple regression\n",
        "# from sklearn.datasets import make_regression\n",
        "# plt.figure()\n",
        "# plt.title('Sample regression problem with one input variable')\n",
        "# X_R1, y_R1 = make_regression(n_samples = 100, n_features=1,\n",
        "#                             n_informative=1, bias = 150.0,\n",
        "#                             noise = 30, random_state=0)\n",
        "# plt.scatter(X_R1, y_R1, marker= 'o', s=50)\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# # synthetic dataset for more complex regression\n",
        "# from sklearn.datasets import make_friedman1\n",
        "# plt.figure()\n",
        "# plt.title('Complex regression problem with one input variable')\n",
        "# X_F1, y_F1 = make_friedman1(n_samples = 100,\n",
        "#                            n_features = 7, random_state=0)\n",
        "\n",
        "# plt.scatter(X_F1[:, 2], y_F1, marker= 'o', s=50)\n",
        "# plt.show()\n",
        "\n",
        "# # synthetic dataset for classification (binary) \n",
        "# plt.figure()\n",
        "# plt.title('Sample binary classification problem with two informative features')\n",
        "# X_C2, y_C2 = make_classification(n_samples = 100, n_features=2,\n",
        "#                                 n_redundant=0, n_informative=2,\n",
        "#                                 n_clusters_per_class=1, flip_y = 0.1,\n",
        "#                                 class_sep = 0.5, random_state=0)\n",
        "# plt.scatter(X_C2[:, 0], X_C2[:, 1], c=y_C2,\n",
        "#            marker= 'o', s=50, cmap=cmap_bold)\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# # more difficult synthetic dataset for classification (binary) \n",
        "# # with classes that are not linearly separable\n",
        "# X_D2, y_D2 = make_blobs(n_samples = 100, n_features = 2, centers = 8,\n",
        "#                        cluster_std = 1.3, random_state = 4)\n",
        "# y_D2 = y_D2\n",
        "# # from adspy_shared_utilities import load_crime_dataset % 2\n",
        "# plt.figure()\n",
        "# plt.title('Sample binary classification problem with non-linearly separable classes')\n",
        "# plt.scatter(X_D2[:,0], X_D2[:,1], c=y_D2,\n",
        "#            marker= 'o', s=50, cmap=cmap_bold)\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# # Breast cancer dataset for classification\n",
        "# cancer = load_breast_cancer()\n",
        "# (X_cancer, y_cancer) = load_breast_cancer(return_X_y = True)\n",
        "\n",
        "\n",
        "# # Communities and Crime dataset\n",
        "# (X_crime, y_crime) = load_crime_dataset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUYXWnMQsOyz",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Logistic regression\n",
        "\n",
        "\n",
        "#### Logistic regression for binary classification on fruits dataset using height, width features (positive class: apple, negative class: others)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jqkIheNsM7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "# from adspy_shared_utilities import (plot_class_regions_for_classifier_subplot)\n",
        "\n",
        "fig, subaxes = plt.subplots(1, 1, figsize=(7, 5))\n",
        "y_fruits_apple = y_fruits_2d == 1   # make into a binary problem: apples vs everything else\n",
        "X_train, X_test, y_train, y_test = (\n",
        "train_test_split(X_fruits_2d.to_numpy(),\n",
        "                y_fruits_apple.to_numpy(),\n",
        "                random_state = 0))\n",
        "\n",
        "clf = LogisticRegression(C=100).fit(X_train, y_train)\n",
        "# plot_class_regions_for_classifier_subplot(clf, X_train, y_train, None,\n",
        "#                                          None, 'Logistic regression \\\n",
        "# for binary classification\\nFruit dataset: Apple vs others',\n",
        "#                                          subaxes)\n",
        "\n",
        "h = 6\n",
        "w = 8\n",
        "print('A fruit with height {} and width {} is predicted to be: {}'\n",
        "     .format(h,w, ['not an apple', 'an apple'][clf.predict([[h,w]])[0]]))\n",
        "\n",
        "h = 10\n",
        "w = 7\n",
        "print('A fruit with height {} and width {} is predicted to be: {}'\n",
        "     .format(h,w, ['not an apple', 'an apple'][clf.predict([[h,w]])[0]]))\n",
        "subaxes.set_xlabel('height')\n",
        "subaxes.set_ylabel('width')\n",
        "\n",
        "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
        "     .format(clf.score(X_train, y_train)))\n",
        "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
        "     .format(clf.score(X_test, y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhnCxNgntIql",
        "colab_type": "text"
      },
      "source": [
        "#### Logistic regression on simple synthetic dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eapP4BIwsUPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "# from adspy_shared_utilities import (\n",
        "# plot_class_regions_for_classifier_subplot)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2,\n",
        "                                                   random_state = 0)\n",
        "\n",
        "fig, subaxes = plt.subplots(1, 1, figsize=(7, 5))\n",
        "clf = LogisticRegression().fit(X_train, y_train)\n",
        "title = 'Logistic regression, simple synthetic dataset C = {:.3f}'.format(1.0)\n",
        "# plot_class_regions_for_classifier_subplot(clf, X_train, y_train,\n",
        "#                                          None, None, title, subaxes)\n",
        "\n",
        "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
        "     .format(clf.score(X_train, y_train)))\n",
        "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
        "     .format(clf.score(X_test, y_test)))\n",
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEFVw3s8tWaJ",
        "colab_type": "text"
      },
      "source": [
        "#### Application to real dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFBtAxPztKEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state = 0)\n",
        "\n",
        "clf = LogisticRegression().fit(X_train, y_train)\n",
        "print('Breast cancer dataset')\n",
        "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
        "     .format(clf.score(X_train, y_train)))\n",
        "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
        "     .format(clf.score(X_test, y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lREh4I9AtagK",
        "colab_type": "text"
      },
      "source": [
        "### Support Vector Machines\n",
        "\n",
        "#### Linear Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYhtFMgGtXtK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "# from adspy_shared_utilities import plot_class_regions_for_classifier_subplot\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2, random_state = 0)\n",
        "\n",
        "fig, subaxes = plt.subplots(1, 1, figsize=(7, 5))\n",
        "this_C = 1.0\n",
        "clf = SVC(kernel = 'linear', C=this_C).fit(X_train, y_train)\n",
        "title = 'Linear SVC, C = {:.3f}'.format(this_C)\n",
        "# plot_class_regions_for_classifier_subplot(clf, X_train, y_train, None, None, title, subaxes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aoy0kIcetgTU",
        "colab_type": "text"
      },
      "source": [
        "#### Linear Support Vector Machine: C parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJLcR3SPteZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "# from adspy_shared_utilities import plot_class_regions_for_classifier\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2, random_state = 0)\n",
        "fig, subaxes = plt.subplots(1, 2, figsize=(8, 4))\n",
        "\n",
        "for this_C, subplot in zip([0.00001, 100], subaxes):\n",
        "    clf = LinearSVC(C=this_C).fit(X_train, y_train)\n",
        "    title = 'Linear SVC, C = {:.5f}'.format(this_C)\n",
        "    # plot_class_regions_for_classifier_subplot(clf, X_train, y_train,\n",
        "    #                                          None, None, title, subplot)\n",
        "plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8rhCSn9tmps",
        "colab_type": "text"
      },
      "source": [
        "#### Application to real dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-iGzFfJtjmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state = 0)\n",
        "\n",
        "clf = LinearSVC().fit(X_train, y_train)\n",
        "print('Breast cancer dataset')\n",
        "print('Accuracy of Linear SVC classifier on training set: {:.2f}'\n",
        "     .format(clf.score(X_train, y_train)))\n",
        "print('Accuracy of Linear SVC classifier on test set: {:.2f}'\n",
        "     .format(clf.score(X_test, y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkiMPspXtqR8",
        "colab_type": "text"
      },
      "source": [
        "### Multi-class classification with linear models\n",
        "\n",
        "#### LinearSVC with M classes generates M one vs rest classifiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW6FrIE7tn2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_fruits_2d, y_fruits_2d, random_state = 0)\n",
        "\n",
        "clf = LinearSVC(C=5, random_state = 67).fit(X_train, y_train)\n",
        "print('Coefficients:\\n', clf.coef_)\n",
        "print('Intercepts:\\n', clf.intercept_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYaBg3hdtuvG",
        "colab_type": "text"
      },
      "source": [
        "#### Multi-class results on the fruit dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-k4uJcmttGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "colors = ['r', 'g', 'b', 'y']\n",
        "cmap_fruits = ListedColormap(['#FF0000', '#00FF00', '#0000FF','#FFFF00'])\n",
        "\n",
        "plt.scatter(X_fruits_2d[['height']], X_fruits_2d[['width']],\n",
        "           c=y_fruits_2d, cmap=cmap_fruits, edgecolor = 'black', alpha=.7)\n",
        "\n",
        "x_0_range = np.linspace(-10, 15)\n",
        "\n",
        "for w, b, color in zip(clf.coef_, clf.intercept_, ['r', 'g', 'b', 'y']):\n",
        "    # Since class prediction with a linear model uses the formula y = w_0 x_0 + w_1 x_1 + b, \n",
        "    # and the decision boundary is defined as being all points with y = 0, to plot x_1 as a \n",
        "    # function of x_0 we just solve w_0 x_0 + w_1 x_1 + b = 0 for x_1:\n",
        "    plt.plot(x_0_range, -(x_0_range * w[0] + b) / w[1], c=color, alpha=.8)\n",
        "    \n",
        "plt.legend(target_names_fruits)\n",
        "plt.xlabel('height')\n",
        "plt.ylabel('width')\n",
        "plt.xlim(-2, 12)\n",
        "plt.ylim(-2, 15)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tl3ljmvtxzr",
        "colab_type": "text"
      },
      "source": [
        "## Kernelized Support Vector Machines\n",
        "\n",
        "### Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN_pCXGwtv-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "# from adspy_shared_utilities import plot_class_regions_for_classifier\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state = 0)\n",
        "\n",
        "# The default SVC kernel is radial basis function (RBF)\n",
        "# plot_class_regions_for_classifier(SVC().fit(X_train, y_train),\n",
        "#                                  X_train, y_train, None, None,\n",
        "#                                  'Support Vector Classifier: RBF kernel')\n",
        "\n",
        "# Compare decision boundries with polynomial kernel, degree = 3\n",
        "# plot_class_regions_for_classifier(SVC(kernel = 'poly', degree = 3)\n",
        "#                                  .fit(X_train, y_train), X_train,\n",
        "#                                  y_train, None, None,\n",
        "#                                  'Support Vector Classifier: Polynomial kernel, degree = 3')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5l3ovpRQt72q",
        "colab_type": "text"
      },
      "source": [
        "#### Support Vector Machine with RBF kernel: gamma parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lW1mOhjt8MN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from adspy_shared_utilities import plot_class_regions_for_classifier\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state = 0)\n",
        "fig, subaxes = plt.subplots(3, 1, figsize=(4, 11))\n",
        "\n",
        "for this_gamma, subplot in zip([0.01, 1.0, 10.0], subaxes):\n",
        "    clf = SVC(kernel = 'rbf', gamma=this_gamma).fit(X_train, y_train)\n",
        "    title = 'Support Vector Classifier: \\nRBF kernel, gamma = {:.2f}'.format(this_gamma)\n",
        "    # plot_class_regions_for_classifier_subplot(clf, X_train, y_train,\n",
        "    #                                          None, None, title, subplot)\n",
        "    # plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwnVfJvauGPh",
        "colab_type": "text"
      },
      "source": [
        "#### Support Vector Machine with RBF kernel: using both C and gamma parameter "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YlhMCvKt9Ui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "# from adspy_shared_utilities import plot_class_regions_for_classifier_subplot\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state = 0)\n",
        "fig, subaxes = plt.subplots(3, 4, figsize=(15, 10), dpi=50)\n",
        "\n",
        "for this_gamma, this_axis in zip([0.01, 1, 5], subaxes):\n",
        "    \n",
        "    for this_C, subplot in zip([0.1, 1, 15, 250], this_axis):\n",
        "        title = 'gamma = {:.2f}, C = {:.2f}'.format(this_gamma, this_C)\n",
        "        clf = SVC(kernel = 'rbf', gamma = this_gamma,\n",
        "                 C = this_C).fit(X_train, y_train)\n",
        "        # plot_class_regions_for_classifier_subplot(clf, X_train, y_train,\n",
        "        #                                          X_test, y_test, title,\n",
        "        #                                          subplot)\n",
        "        # plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFrVPEDZuLXE",
        "colab_type": "text"
      },
      "source": [
        "### Application of SVMs to a real dataset: unnormalized data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJK4rekyuJhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer,\n",
        "                                                   random_state = 0)\n",
        "\n",
        "clf = SVC(C=10).fit(X_train, y_train)\n",
        "print('Breast cancer dataset (unnormalized features)')\n",
        "print('Accuracy of RBF-kernel SVC on training set: {:.2f}'\n",
        "     .format(clf.score(X_train, y_train)))\n",
        "print('Accuracy of RBF-kernel SVC on test set: {:.2f}'\n",
        "     .format(clf.score(X_test, y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C_0sDtbuOGi",
        "colab_type": "text"
      },
      "source": [
        "### Application of SVMs to a real dataset: normalized data with feature preprocessing using minmax scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2YJpteVuMqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "clf = SVC(C=10).fit(X_train_scaled, y_train)\n",
        "print('Breast cancer dataset (normalized with MinMax scaling)')\n",
        "print('RBF-kernel SVC (with MinMax scaling) training set accuracy: {:.2f}'\n",
        "     .format(clf.score(X_train_scaled, y_train)))\n",
        "print('RBF-kernel SVC (with MinMax scaling) test set accuracy: {:.2f}'\n",
        "     .format(clf.score(X_test_scaled, y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfRJesH7uRBA",
        "colab_type": "text"
      },
      "source": [
        "## Cross-validation\n",
        "\n",
        "### Example based on k-NN classifier with fruit dataset (2 features)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWy8dWdyuPR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "clf = KNeighborsClassifier(n_neighbors = 5)\n",
        "X = X_fruits_2d.to_numpy()\n",
        "y = y_fruits_2d.to_numpy()\n",
        "cv_scores = cross_val_score(clf, X, y)\n",
        "\n",
        "print('Cross-validation scores (3-fold):', cv_scores)\n",
        "print('Mean cross-validation score (3-fold): {:.3f}'\n",
        "     .format(np.mean(cv_scores)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAzHXjsDuZYo",
        "colab_type": "text"
      },
      "source": [
        "### A note on performing cross-validation for more advanced scenarios.\n",
        "\n",
        "In some cases (e.g. when feature values have very different ranges), we've seen the need to scale or normalize the training and test sets before use with a classifier. The proper way to do cross-validation when you need to scale the data is *not* to scale the entire dataset with a single transform, since this will indirectly leak information into the training data about the whole dataset, including the test data (see the lecture on data leakage later in the course).  Instead, scaling/normalizing must be computed and applied for each cross-validation fold separately.  To do this, the easiest way in scikit-learn is to use *pipelines*.  While these are beyond the scope of this course, further information is available in the scikit-learn documentation here:\n",
        "\n",
        "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
        "\n",
        "or the Pipeline section in the recommended textbook: Introduction to Machine Learning with Python by Andreas C. Müller and Sarah Guido (O'Reilly Media).\n",
        "\n",
        "## Validation curve example\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqUMp8CEuTW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import validation_curve\n",
        "\n",
        "param_range = np.logspace(-3, 3, 4)\n",
        "train_scores, test_scores = validation_curve(SVC(), X, y,\n",
        "                                            param_name='gamma',\n",
        "                                            param_range=param_range, cv=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNdnRb9iuioS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3sLZb72uki4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(test_scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9GDHbhBulzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This code based on scikit-learn validation_plot example\n",
        "#  See:  http://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html\n",
        "plt.figure()\n",
        "\n",
        "train_scores_mean = np.mean(train_scores, axis=1)\n",
        "train_scores_std = np.std(train_scores, axis=1)\n",
        "test_scores_mean = np.mean(test_scores, axis=1)\n",
        "test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "plt.title('Validation Curve with SVM')\n",
        "plt.xlabel('$\\gamma$ (gamma)')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0.0, 1.1)\n",
        "lw = 2\n",
        "\n",
        "plt.semilogx(param_range, train_scores_mean, label='Training score',\n",
        "            color='darkorange', lw=lw)\n",
        "\n",
        "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
        "                train_scores_mean + train_scores_std, alpha=0.2,\n",
        "                color='darkorange', lw=lw)\n",
        "\n",
        "plt.semilogx(param_range, test_scores_mean, label='Cross-validation score',\n",
        "            color='navy', lw=lw)\n",
        "\n",
        "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
        "                test_scores_mean + test_scores_std, alpha=0.2,\n",
        "                color='navy', lw=lw)\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPJ_tID7uprp",
        "colab_type": "text"
      },
      "source": [
        "## Decision Trees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WweV7w8unIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# from adspy_shared_utilities import plot_decision_tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state = 3)\n",
        "clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
        "\n",
        "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
        "     .format(clf.score(X_train, y_train)))\n",
        "print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
        "     .format(clf.score(X_test, y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPivM0Ovux3i",
        "colab_type": "text"
      },
      "source": [
        "#### Setting max decision tree depth to help avoid overfitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_6IXir4uq88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf2 = DecisionTreeClassifier(max_depth = 3).fit(X_train, y_train)\n",
        "\n",
        "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
        "     .format(clf2.score(X_train, y_train)))\n",
        "print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
        "     .format(clf2.score(X_test, y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP6A7RR4u1lf",
        "colab_type": "text"
      },
      "source": [
        "#### Visualizing decision trees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl_Zjm9AuzDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot_decision_tree(clf, iris.feature_names, iris.target_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkbQj_xiu6eS",
        "colab_type": "text"
      },
      "source": [
        "#### Pre-pruned version (max_depth = 3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS1Ss07Su4UD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot_decision_tree(clf2, iris.feature_names, iris.target_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqsWmRzUu-pz",
        "colab_type": "text"
      },
      "source": [
        "#### Feature importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohMm63Bou8Ja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from adspy_shared_utilities import plot_feature_importances\n",
        "\n",
        "# plt.figure(figsize=(10,4), dpi=80)\n",
        "# plot_feature_importances(clf, iris.feature_names)\n",
        "# plt.show()\n",
        "\n",
        "# print('Feature importances: {}'.format(clf.feature_importances_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NboIXshZvCD2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# from adspy_shared_utilities import plot_class_regions_for_classifier_subplot\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state = 0)\n",
        "fig, subaxes = plt.subplots(6, 1, figsize=(6, 32))\n",
        "\n",
        "pair_list = [[0,1], [0,2], [0,3], [1,2], [1,3], [2,3]]\n",
        "tree_max_depth = 4\n",
        "\n",
        "for pair, axis in zip(pair_list, subaxes):\n",
        "    X = X_train[:, pair]\n",
        "    y = y_train\n",
        "    \n",
        "    clf = DecisionTreeClassifier(max_depth=tree_max_depth).fit(X, y)\n",
        "#     title = 'Decision Tree, max_depth = {:d}'.format(tree_max_depth)\n",
        "#     plot_class_regions_for_classifier_subplot(clf, X, y, None,\n",
        "#                                              None, title, axis,\n",
        "#                                              iris.target_names)\n",
        "    \n",
        "#     axis.set_xlabel(iris.feature_names[pair[0]])\n",
        "#     axis.set_ylabel(iris.feature_names[pair[1]])\n",
        "    \n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Og3B7lNvLY9",
        "colab_type": "text"
      },
      "source": [
        "#### Decision Trees on a real-world dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lU_iAEFvJLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# from adspy_shared_utilities import plot_decision_tree\n",
        "# from adspy_shared_utilities import plot_feature_importances\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state = 0)\n",
        "\n",
        "clf = DecisionTreeClassifier(max_depth = 4, min_samples_leaf = 8,\n",
        "                            random_state = 0).fit(X_train, y_train)\n",
        "\n",
        "# plot_decision_tree(clf, cancer.feature_names, cancer.target_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Clfoq7XlvOGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Breast cancer dataset: decision tree')\n",
        "print('Accuracy of DT classifier on training set: {:.2f}'\n",
        "     .format(clf.score(X_train, y_train)))\n",
        "print('Accuracy of DT classifier on test set: {:.2f}'\n",
        "     .format(clf.score(X_test, y_test)))\n",
        "\n",
        "plt.figure(figsize=(10,6),dpi=80)\n",
        "# plot_feature_importances(clf, cancer.feature_names)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3ygh_IMvQFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}