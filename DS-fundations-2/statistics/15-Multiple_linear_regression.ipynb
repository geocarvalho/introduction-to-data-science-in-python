{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple linear regression introduction\n",
    "We can use linear algebra to predict the **$Y$** value using all multi-column at the same time. The way we do this is by creating a matrix of inputs, and we create a vector of the response that we want to predict. The matrix with the multiple column is denoted **$Y$** (capital x bold) while the vector for the response is denoted **$y$** (y bold)\n",
    "* [Linear algebra from Khan academy](https://www.khanacademy.org/math/linear-algebra)\n",
    "* [an introduction to statistical learning](http://www-bcf.usc.edu/~gareth/ISL/) - chapter 3\n",
    "\n",
    "In this notebook (and following quizzes), you will be creating a few simple linear regression models, as well as a multiple linear regression model, to predict home value.\n",
    "\n",
    "Let's get started by importing the necessary libraries and reading in the data you will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>house_id</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>style</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1112</td>\n",
       "      <td>B</td>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>598291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491</td>\n",
       "      <td>B</td>\n",
       "      <td>3512</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1744259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5952</td>\n",
       "      <td>B</td>\n",
       "      <td>1134</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>571669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3525</td>\n",
       "      <td>A</td>\n",
       "      <td>1940</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>493675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5108</td>\n",
       "      <td>B</td>\n",
       "      <td>2208</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1101539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   house_id neighborhood  area  bedrooms  bathrooms      style    price\n",
       "0      1112            B  1188         3          2      ranch   598291\n",
       "1       491            B  3512         5          3  victorian  1744259\n",
       "2      5952            B  1134         3          2      ranch   571669\n",
       "3      3525            A  1940         4          2      ranch   493675\n",
       "4      5108            B  2208         6          4  victorian  1101539"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "from patsy import dmatrices\n",
    "import statsmodels.api as sm;\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('./house_prices.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4230.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 18 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:00:23</td>     <th>  Log-Likelihood:    </th> <td> -84517.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.690e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6024</td>      <th>  BIC:               </th> <td>1.691e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>area</th>      <td>  345.9110</td> <td>    7.227</td> <td>   47.863</td> <td> 0.000</td> <td>  331.743</td> <td>  360.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedrooms</th>  <td>-2925.8063</td> <td> 1.03e+04</td> <td>   -0.285</td> <td> 0.775</td> <td> -2.3e+04</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms</th> <td> 7345.3917</td> <td> 1.43e+04</td> <td>    0.515</td> <td> 0.607</td> <td>-2.06e+04</td> <td> 3.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td> 1.007e+04</td> <td> 1.04e+04</td> <td>    0.972</td> <td> 0.331</td> <td>-1.02e+04</td> <td> 3.04e+04</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>367.658</td> <th>  Durbin-Watson:     </th> <td>   2.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 350.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.536</td>  <th>  Prob(JB):          </th> <td>9.40e-77</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.503</td>  <th>  Cond. No.          </th> <td>1.16e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.16e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.678\n",
       "Model:                            OLS   Adj. R-squared:                  0.678\n",
       "Method:                 Least Squares   F-statistic:                     4230.\n",
       "Date:                Fri, 18 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        10:00:23   Log-Likelihood:                -84517.\n",
       "No. Observations:                6028   AIC:                         1.690e+05\n",
       "Df Residuals:                    6024   BIC:                         1.691e+05\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "area         345.9110      7.227     47.863      0.000     331.743     360.079\n",
       "bedrooms   -2925.8063   1.03e+04     -0.285      0.775    -2.3e+04    1.72e+04\n",
       "bathrooms   7345.3917   1.43e+04      0.515      0.607   -2.06e+04    3.53e+04\n",
       "intercept   1.007e+04   1.04e+04      0.972      0.331   -1.02e+04    3.04e+04\n",
       "==============================================================================\n",
       "Omnibus:                      367.658   Durbin-Watson:                   2.007\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              350.116\n",
       "Skew:                           0.536   Prob(JB):                     9.40e-77\n",
       "Kurtosis:                       2.503   Cond. No.                     1.16e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.16e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['intercept'] = 1\n",
    "mlm = sm.OLS(df['price'], df[['area', 'bedrooms', 'bathrooms', 'intercept']])\n",
    "results = mlm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.` Using statsmodels, fit three individual simple linear regression models to predict price.  You should have a model that uses **area**, another using **bedrooms**, and a final one using **bathrooms**.  You will also want to use an intercept in each of your three models.\n",
    "\n",
    "Use the results from each of your models to answer the first two quiz questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>1.269e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 18 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:00:23</td>     <th>  Log-Likelihood:    </th> <td> -84517.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.690e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6026</td>      <th>  BIC:               </th> <td>1.691e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td> 9587.8878</td> <td> 7637.479</td> <td>    1.255</td> <td> 0.209</td> <td>-5384.303</td> <td> 2.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>area</th>      <td>  348.4664</td> <td>    3.093</td> <td>  112.662</td> <td> 0.000</td> <td>  342.403</td> <td>  354.530</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>368.609</td> <th>  Durbin-Watson:     </th> <td>   2.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 349.279</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.534</td>  <th>  Prob(JB):          </th> <td>1.43e-76</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.499</td>  <th>  Cond. No.          </th> <td>4.93e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 4.93e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.678\n",
       "Model:                            OLS   Adj. R-squared:                  0.678\n",
       "Method:                 Least Squares   F-statistic:                 1.269e+04\n",
       "Date:                Fri, 18 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        10:00:23   Log-Likelihood:                -84517.\n",
       "No. Observations:                6028   AIC:                         1.690e+05\n",
       "Df Residuals:                    6026   BIC:                         1.691e+05\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept   9587.8878   7637.479      1.255      0.209   -5384.303    2.46e+04\n",
       "area         348.4664      3.093    112.662      0.000     342.403     354.530\n",
       "==============================================================================\n",
       "Omnibus:                      368.609   Durbin-Watson:                   2.007\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              349.279\n",
       "Skew:                           0.534   Prob(JB):                     1.43e-76\n",
       "Kurtosis:                       2.499   Cond. No.                     4.93e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 4.93e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['intercept'] = 1\n",
    "lm1 = sm.OLS(df['price'], df[['intercept', 'area']])\n",
    "result1 = lm1.fit()\n",
    "result1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.553</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.553</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   7446.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 18 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:00:24</td>     <th>  Log-Likelihood:    </th> <td> -85509.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.710e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6026</td>      <th>  BIC:               </th> <td>1.710e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>-9.485e+04</td> <td> 1.08e+04</td> <td>   -8.762</td> <td> 0.000</td> <td>-1.16e+05</td> <td>-7.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedrooms</th>  <td> 2.284e+05</td> <td> 2646.744</td> <td>   86.289</td> <td> 0.000</td> <td> 2.23e+05</td> <td> 2.34e+05</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>967.118</td> <th>  Durbin-Watson:     </th> <td>   2.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1599.431</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.074</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.325</td>  <th>  Cond. No.          </th> <td>    10.3</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.553\n",
       "Model:                            OLS   Adj. R-squared:                  0.553\n",
       "Method:                 Least Squares   F-statistic:                     7446.\n",
       "Date:                Fri, 18 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        10:00:24   Log-Likelihood:                -85509.\n",
       "No. Observations:                6028   AIC:                         1.710e+05\n",
       "Df Residuals:                    6026   BIC:                         1.710e+05\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept  -9.485e+04   1.08e+04     -8.762      0.000   -1.16e+05   -7.36e+04\n",
       "bedrooms    2.284e+05   2646.744     86.289      0.000    2.23e+05    2.34e+05\n",
       "==============================================================================\n",
       "Omnibus:                      967.118   Durbin-Watson:                   2.014\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1599.431\n",
       "Skew:                           1.074   Prob(JB):                         0.00\n",
       "Kurtosis:                       4.325   Cond. No.                         10.3\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm2 = sm.OLS(df['price'], df[['intercept', 'bedrooms']])\n",
    "result2 = lm2.fit()\n",
    "result2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.541</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.541</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   7116.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 18 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:00:24</td>     <th>  Log-Likelihood:    </th> <td> -85583.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.712e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6026</td>      <th>  BIC:               </th> <td>1.712e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td> 4.314e+04</td> <td> 9587.189</td> <td>    4.500</td> <td> 0.000</td> <td> 2.43e+04</td> <td> 6.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms</th> <td> 3.295e+05</td> <td> 3905.540</td> <td>   84.358</td> <td> 0.000</td> <td> 3.22e+05</td> <td> 3.37e+05</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>915.429</td> <th>  Durbin-Watson:     </th> <td>   2.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1537.531</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.010</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.428</td>  <th>  Cond. No.          </th> <td>    5.84</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.541\n",
       "Model:                            OLS   Adj. R-squared:                  0.541\n",
       "Method:                 Least Squares   F-statistic:                     7116.\n",
       "Date:                Fri, 18 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        10:00:24   Log-Likelihood:                -85583.\n",
       "No. Observations:                6028   AIC:                         1.712e+05\n",
       "Df Residuals:                    6026   BIC:                         1.712e+05\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept   4.314e+04   9587.189      4.500      0.000    2.43e+04    6.19e+04\n",
       "bathrooms   3.295e+05   3905.540     84.358      0.000    3.22e+05    3.37e+05\n",
       "==============================================================================\n",
       "Omnibus:                      915.429   Durbin-Watson:                   2.003\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1537.531\n",
       "Skew:                           1.010   Prob(JB):                         0.00\n",
       "Kurtosis:                       4.428   Cond. No.                         5.84\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm3 = sm.OLS(df['price'], df[['intercept', 'bathrooms']])\n",
    "result3 = lm3.fit()\n",
    "result3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Now that you have looked at the results from the simple linear regression models, let's try a multiple linear regression model using all three of these variables  at the same time.  You will still want an intercept in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4230.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 18 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:00:24</td>     <th>  Log-Likelihood:    </th> <td> -84517.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.690e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6024</td>      <th>  BIC:               </th> <td>1.691e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td> 1.007e+04</td> <td> 1.04e+04</td> <td>    0.972</td> <td> 0.331</td> <td>-1.02e+04</td> <td> 3.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>area</th>      <td>  345.9110</td> <td>    7.227</td> <td>   47.863</td> <td> 0.000</td> <td>  331.743</td> <td>  360.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedrooms</th>  <td>-2925.8063</td> <td> 1.03e+04</td> <td>   -0.285</td> <td> 0.775</td> <td> -2.3e+04</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms</th> <td> 7345.3917</td> <td> 1.43e+04</td> <td>    0.515</td> <td> 0.607</td> <td>-2.06e+04</td> <td> 3.53e+04</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>367.658</td> <th>  Durbin-Watson:     </th> <td>   2.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 350.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.536</td>  <th>  Prob(JB):          </th> <td>9.40e-77</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.503</td>  <th>  Cond. No.          </th> <td>1.16e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.16e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.678\n",
       "Model:                            OLS   Adj. R-squared:                  0.678\n",
       "Method:                 Least Squares   F-statistic:                     4230.\n",
       "Date:                Fri, 18 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        10:00:24   Log-Likelihood:                -84517.\n",
       "No. Observations:                6028   AIC:                         1.690e+05\n",
       "Df Residuals:                    6024   BIC:                         1.691e+05\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept   1.007e+04   1.04e+04      0.972      0.331   -1.02e+04    3.04e+04\n",
       "area         345.9110      7.227     47.863      0.000     331.743     360.079\n",
       "bedrooms   -2925.8063   1.03e+04     -0.285      0.775    -2.3e+04    1.72e+04\n",
       "bathrooms   7345.3917   1.43e+04      0.515      0.607   -2.06e+04    3.53e+04\n",
       "==============================================================================\n",
       "Omnibus:                      367.658   Durbin-Watson:                   2.007\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              350.116\n",
       "Skew:                           0.536   Prob(JB):                     9.40e-77\n",
       "Kurtosis:                       2.503   Cond. No.                     1.16e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.16e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = sm.OLS(df['price'], df[['intercept', 'area', 'bedrooms', 'bathrooms']])\n",
    "result = lm.fit()\n",
    "result.summary() # multicolinearidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Along with using the **area**, **bedrooms**, and **bathrooms** you might also want to use **style** to predict the price.  Try adding this to your multiple linear regression model.  What happens?  Use the final quiz below to provide your answer.\n",
    "\n",
    "* There is an error, because an object cannot be added to the multiple linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-af8693246f66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstyle_lm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOLS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intercept'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'area'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bedrooms'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bathrooms'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'style'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresult_style\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstyle_lm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresult_style\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/regression/linear_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    815\u001b[0m                  **kwargs):\n\u001b[1;32m    816\u001b[0m         super(OLS, self).__init__(endog, exog, missing=missing,\n\u001b[0;32m--> 817\u001b[0;31m                                   hasconst=hasconst, **kwargs)\n\u001b[0m\u001b[1;32m    818\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"weights\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/regression/linear_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, weights, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         super(WLS, self).__init__(endog, exog, missing=missing,\n\u001b[0;32m--> 663\u001b[0;31m                                   weights=weights, hasconst=hasconst, **kwargs)\n\u001b[0m\u001b[1;32m    664\u001b[0m         \u001b[0mnobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/regression/linear_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \"\"\"\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRegressionModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_attr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pinv_wexog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wendog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wexog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLikelihoodModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mhasconst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hasconst'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         self.data = self._handle_data(endog, exog, missing, hasconst,\n\u001b[0;32m---> 64\u001b[0;31m                                       **kwargs)\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_constant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m_handle_data\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhasconst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhasconst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;31m# kwargs arrays could have changed, easier to just attach here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/base/data.py\u001b[0m in \u001b[0;36mhandle_data\u001b[0;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m     \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_data_class_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m     return klass(endog, exog=exog, missing=missing, hasconst=hasconst,\n\u001b[0;32m--> 633\u001b[0;31m                  **kwargs)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/base/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_endog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_exog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_endog_exog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# this has side-effects, attaches k_constant and const_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/base/data.py\u001b[0m in \u001b[0;36m_convert_endog_exog\u001b[0;34m(self, endog, exog)\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0mexog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexog\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mexog\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mexog\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m             raise ValueError(\"Pandas data cast to numpy dtype of object. \"\n\u001b[0m\u001b[1;32m    475\u001b[0m                              \"Check input data with np.asarray(data).\")\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPandasData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_endog_exog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Pandas data cast to numpy dtype of object. Check input data with np.asarray(data)."
     ]
    }
   ],
   "source": [
    "style_lm = sm.OLS(df['price'], df[['intercept', 'area', 'bedrooms', 'bathrooms', 'style']])\n",
    "result_style = style_lm.fit()\n",
    "result_style.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y matrix\n",
    "X = df[['intercept', 'area', 'bedrooms', 'bathrooms']]\n",
    "y = df['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression closed form solution:\n",
    "\n",
    "$\\hat{\\beta}=(X`X)^-X`y$\n",
    "\n",
    "$X`$ means X transpose\n",
    "\n",
    "$()⁻$ means the inverstion of the results inside the parentheses\n",
    "\n",
    "In Numpy get the transpose, the inverses and the dot products to get the coeficients results for each column\n",
    "\n",
    "* [OLS in matrix form](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10072.10704672,   345.91101884, -2925.80632467,  7345.3917137 ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = np.linalg.inv(np.dot(X.transpose(),X)) #(X'X)^-\n",
    "beta = np.dot(beta, X.transpose()) #(X'X)^- * X'\n",
    "beta = np.dot(beta, y) # (X'X)^-X' * y\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy variables\n",
    "Instead of adding columns with categorical variables, create columns with the presence or not of a variable (been the column name the varible present or not) enconded by 1 (exist) and 0 (not exist). Because the last column can be inferred from the earlier two columns, as the one values are only in the rows that didn't have ones in one of these other columns, we actually end up choosing to drop this column (which one doesn't really matter too much, noted as reference column). Without the last column, the matrix is [full rank](https://www.cds.caltech.edu/~murray/amwiki/index.php/FAQ:_What_does_it_mean_for_a_non-square_matrix_to_be_full_rank%3F)\n",
    "![image-example](https://d17h27t6h515a5.cloudfront.net/topher/2017/December/5a297de8_screen-shot-2017-12-07-at-9.43.05-am/screen-shot-2017-12-07-at-9.43.05-am.png)\n",
    "\n",
    "* The number of variables dummy added to the matrix X and the level of each categorical variable minus one have to be equal;\n",
    "* The motivation to delete one dummy variable is to garanteee that all the column are linear independent, product of $X`X$ is invertible and the matrix X be full rank.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.339</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.339</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1548.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 18 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:01:00</td>     <th>  Log-Likelihood:    </th> <td> -86683.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.734e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6025</td>      <th>  BIC:               </th> <td>1.734e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td> 1.046e+06</td> <td> 7775.607</td> <td>  134.534</td> <td> 0.000</td> <td> 1.03e+06</td> <td> 1.06e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lodge</th>     <td>-7.411e+05</td> <td> 1.44e+04</td> <td>  -51.396</td> <td> 0.000</td> <td>-7.69e+05</td> <td>-7.13e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ranch</th>     <td> -4.71e+05</td> <td> 1.27e+04</td> <td>  -37.115</td> <td> 0.000</td> <td>-4.96e+05</td> <td>-4.46e+05</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1340.120</td> <th>  Durbin-Watson:     </th> <td>   2.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3232.810</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.230</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 5.611</td>  <th>  Cond. No.          </th> <td>    3.28</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.339\n",
       "Model:                            OLS   Adj. R-squared:                  0.339\n",
       "Method:                 Least Squares   F-statistic:                     1548.\n",
       "Date:                Fri, 18 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        10:01:00   Log-Likelihood:                -86683.\n",
       "No. Observations:                6028   AIC:                         1.734e+05\n",
       "Df Residuals:                    6025   BIC:                         1.734e+05\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept   1.046e+06   7775.607    134.534      0.000    1.03e+06    1.06e+06\n",
       "lodge      -7.411e+05   1.44e+04    -51.396      0.000   -7.69e+05   -7.13e+05\n",
       "ranch       -4.71e+05   1.27e+04    -37.115      0.000   -4.96e+05   -4.46e+05\n",
       "==============================================================================\n",
       "Omnibus:                     1340.120   Durbin-Watson:                   2.004\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             3232.810\n",
       "Skew:                           1.230   Prob(JB):                         0.00\n",
       "Kurtosis:                       5.611   Cond. No.                         3.28\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['A', 'B', 'C']] = pd.get_dummies(df['neighborhood']) \n",
    "df[['lodge', 'ranch', 'victorian']] = pd.get_dummies(df['style'])\n",
    "# You always drop one level from each category. This is called the baseline (the dropped column)\n",
    "df['intercept'] = 1\n",
    "lm = sm.OLS(df['price'], df[['intercept', 'lodge', 'ranch']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The intercept means that if our home is a **victorian** home, we predict its price to be 1,046,000 dollars and **lodge** is predicted to be 741,000 less than a **victorian**. A **ranch** is predicted to be 471,000 less than a **victorian**.\n",
    "\n",
    "\n",
    "`1.` Use the [pd.get_dummies](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) documentation to assist you with obtaining dummy variables for the **neighborhood** column.  Then use [join](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.join.html) to add the dummy variables to your dataframe, **df**, and store the joined results in **df_new**.\n",
    "\n",
    "Fit a linear model using **all three levels** of **neighborhood** to predict the price. Don't forget an intercept.\n",
    "\n",
    "Use your results to answer quiz 1 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "columns overlap but no suffix specified: Index(['A', 'B', 'C'], dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-0aa0c279160c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mneighborhood_dummies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'neighborhood'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneighborhood_dummies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[1;32m   6324\u001b[0m         \u001b[0;31m# For SparseDataFrame's benefit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6325\u001b[0m         return self._join_compat(other, on=on, how=how, lsuffix=lsuffix,\n\u001b[0;32m-> 6326\u001b[0;31m                                  rsuffix=rsuffix, sort=sort)\n\u001b[0m\u001b[1;32m   6327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6328\u001b[0m     def _join_compat(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_join_compat\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[1;32m   6339\u001b[0m             return merge(self, other, left_on=on, how=how,\n\u001b[1;32m   6340\u001b[0m                          \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6341\u001b[0;31m                          suffixes=(lsuffix, rsuffix), sort=sort)\n\u001b[0m\u001b[1;32m   6342\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6343\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     59\u001b[0m                          \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                          validate=validate)\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         llabels, rlabels = items_overlap_with_suffix(ldata.items, lsuf,\n\u001b[0;32m--> 573\u001b[0;31m                                                      rdata.items, rsuf)\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0mlindexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mleft_indexer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mitems_overlap_with_suffix\u001b[0;34m(left, lsuffix, right, rsuffix)\u001b[0m\n\u001b[1;32m   5242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlsuffix\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5243\u001b[0m             raise ValueError('columns overlap but no suffix specified: '\n\u001b[0;32m-> 5244\u001b[0;31m                              '{rename}'.format(rename=to_rename))\n\u001b[0m\u001b[1;32m   5245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5246\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mlrenamer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: columns overlap but no suffix specified: Index(['A', 'B', 'C'], dtype='object')"
     ]
    }
   ],
   "source": [
    "neighborhood_dummies = pd.get_dummies(df['neighborhood'])\n",
    "df_new = df.join(neighborhood_dummies)\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-979d17b2ed0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intercept'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOLS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intercept'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'A'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'B'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'C'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_new' is not defined"
     ]
    }
   ],
   "source": [
    "df_new['intercept'] = 1\n",
    "lm = sm.OLS(df_new['price'], df_new[['intercept', 'A', 'B', 'C']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.`  Now, fit an appropriate linear model for using **neighborhood** to predict the price of a home. Use **neighborhood A** as your baseline.  Use your resulting model to answer the questions in Quiz 2 and Quiz 3 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = sm.OLS(df_new['price'], df_new[['intercept', 'C', 'B']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Observe que cada um dos coeficientes é uma comparação da categoria com a referência. Portanto, um coeficiente positivo sugere que o bairro é mais caro, em média, do que a referência. Por outro lado, um coeficiente negativo sugere que o bairro é menos caro, em média, do que a referência.\n",
    "\n",
    "* Você pode olhar para os valores-p para comparar com o bairro A. Para comparar o bairro B ao bairro C, você pode comparar os intervalos de confiança. Como os intervalos de confiança para B e C não se sobrepõem, temos provas de que eles diferem também.\n",
    "\n",
    "`3.` Run the two cells below to look at the home prices for the A and C neighborhoods. Add neighborhood B. This creates a glimpse into the differences that you found in the previous linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_new.query(\"C == 1\")['price'], alpha = 0.3, label = 'C');\n",
    "plt.hist(df_new.query(\"A == 1\")['price'], alpha = 0.3, label = 'A');\n",
    "plt.hist(df_new.query(\"B == 1\")['price'], alpha = 0.3, label = 'B')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Now, add dummy variables for the **style** of house. Create a new linear model using these new dummies, as well as the previous **neighborhood** dummies.  Use **ranch** as the baseline for the **style**.  Additionally, add **bathrooms** and **bedrooms** to your linear model.  Don't forget an intercept.  Use the results of your linear model to answer the last two questions below. **Home prices are measured in dollars, and this dataset is not real.**\n",
    "\n",
    "To minimize scrolling, it might be useful to open another browser window to this concept to answer the quiz questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sty = pd.get_dummies(df_new['style'])\n",
    "new_new_df = df_new.join(sty)\n",
    "lm = sm.OLS(new_new_df['price'], new_new_df[['intercept', 'B', 'C', 'lodge', 'victorian', 'bedrooms', 'bathrooms']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 80.9%  (r-squared) da variabilidade no preço pode ser explicada pelo modelo linear construído usando o estilo de quartos, banheiros, vizinhança e da casa;\n",
    "* Para cada quarto adicional em uma casa, espera-se um aumento de preço de 173200, em que todas as outras variáveis são mantidas constantes;\n",
    "* Para cada banheiro adicional em uma casa, espera-se um aumento de preço de 99960, em que todas as outras variáveis são mantidas constantes;\n",
    "* Espera-se que uma casa vitoriana custará 70560 a mais do que uma fazenda, sendo todo o resto igual;\n",
    "* Espera-se que uma casa na vizinhança C custará 7168 menos que uma casa na vizinhança A, sendo todo o resto igual.\n",
    "\n",
    "---\n",
    "\n",
    "## Dummy variables recap\n",
    "\n",
    "The biggest reason for use encoding one, zero and negative one is that it changes the coefficients that we get back from the model as well as how interpret those coefficients.\n",
    "With one-zero encoding your interpreter coefficients as a comparion to a baseline category. However, if use one-zer-negative one encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./house_prices.csv')\n",
    "df2 = df.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The below function creates 1, 0, -1 coded dummy variables.\n",
    "\n",
    "def dummy_cat(df, col):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df - the dataframe where col is stored\n",
    "    col - the categorical column you want to dummy (as a string)\n",
    "    OUTPUT:\n",
    "    df - the dataframe with the added columns\n",
    "         for dummy variables using 1, 0, -1 coding\n",
    "    '''\n",
    "    for idx, val_0 in enumerate(df[col].unique()):\n",
    "        if idx + 1 < df[col].nunique():            \n",
    "            df[val_0] = df[col].apply(lambda x: 1 if x == val_0 else 0)\n",
    "        else:    \n",
    "            df[val_0] = df[col].apply(lambda x: -1 if x == val_0 else 0)\n",
    "            for idx, val_1 in enumerate(df[col].unique()):\n",
    "                if idx + 1 < df[col].nunique():\n",
    "                    df[val_1] = df[val_0] + df[val_1]\n",
    "                else:\n",
    "                    del df[val_1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = dummy_cat(df, 'style') # Use on style\n",
    "new_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['intercept'] = 1\n",
    "\n",
    "lm = sm.OLS(new_df['price'], new_df[['intercept', 'ranch', 'victorian']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_dummies = pd.get_dummies(df['style'])\n",
    "new_df2 = df2.join(style_dummies)\n",
    "new_df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df2['intercept'] = 1\n",
    "\n",
    "lm2 = sm.OLS(new_df2['price'], new_df2[['intercept', 'ranch', 'victorian']])\n",
    "results2 = lm2.fit()\n",
    "results2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alguns pontos a serem observados: Em primeiro lugar, a programação 1, 0 significa uma comparação com a categoria de base. Depois, a programação 1, 0,-1 significa uma comparação com a média geral. Por fim, a linguagem de aumento de uma unidade está associada às variáveis quantitativas, e não às variáveis categóricas.\n",
    "\n",
    "* 33.9% da variabilidade no preço pode ser explicada pelo estilo de casa.\n",
    "* 642100 é o preço médio de moradia previsto, sem levar em conta o estilo.\n",
    "* Em comparação a uma hospedaria, prevemos que uma casa vitoriana tenha uma alta de preço de 741100, mantendo todo o resto constante.\n",
    "* Em comparação a uma casa mediana, prevemos que o preço de uma casa vitoriana seja 404000 maior, mantendo todas as outras variáveis constantes.\n",
    "\n",
    "Para prever a categoria de referência na codificação 1, 0, você tem que utilizar o intercepto. Na codificação 1, 0, -1, você precisa multiplicar cada coeficiente categórico por -1 para chegar na categoria que falta. Com isto em mente, qual é o preço médio previsto para hospedarias utilizando o modelo de codificação 1, 0, -1?\n",
    "\n",
    "* Multiplicando -1 pela fazenda e pela casa vitoriana, obtemos o seguinte resultado: 642100 + 66950 - 404000 = 305050. Observe também que isso coincide com a mesma previsão (erro de 50 devido aos arredondamentos), que você vê no modelo de codificação 0,1\n",
    "\n",
    "---\n",
    "\n",
    "## Potential problems introduction\n",
    "\n",
    "There's a number of problems that may arise. First, what is your model for?\n",
    "* To understand if your X and Y variables are related?\n",
    "* To best predict the response variable?\n",
    "* Find which variables are really useful in predicting your response?\n",
    "\n",
    "Depending on which aspects you're most interesed in, this can help determine which problems you actually care about addressing.\n",
    "\n",
    "* A linear relationship may not exist between your response and predictor variables;\n",
    "* You might have correlated errors;\n",
    "* You might not have constant variance of your errors;\n",
    "* You might have outliers or leverage points that hurt your model;\n",
    "* You might have multicolliearity.\n",
    "\n",
    "Chapter 3 of \"[An introduction to statistical learning](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf)\" dives into each of the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Não-linearidade das relações entre preditor e resposta\n",
    "2. Correlação dos termos de erro\n",
    "3. Variância não-constante e erros normalmente distribuídos\n",
    "4. Outliers/pontos de alta alavancagem\n",
    "5. Colinearidade\n",
    "\n",
    "### Linearidade\n",
    "A suposição de linearidade é que o modelo linear representa a verdadeira relação existente entre a variável de resposta e a preditora. Se isso não for verdade, então as suas previsões não serão muito precisas. Além disso, as relações lineares associadas aos seus coeficientes também não são muito úteis.\n",
    "\n",
    "Para avaliar se uma relação linear é razoável, um gráfico dos resíduos $(y-\\hat{y})$ pelos valores preditos (\\hat{y}) geralmente é útil. Se existem padrões de curvatura neste gráfico, isso sugere que um modelo linear pode não se ajustar adequadamente aos dados, e alguma outra relação existe entre as variáveis preditoras e de resposta. Existem muitas maneiras de criar modelos não-lineares (até mesmo usando o formato do modelo linear), e você será apresentado a algumas delas.\n",
    "\n",
    "Na imagem na parte inferior desta página, os modelos são considerados viesados. O ideal seria que tivéssemos uma dispersão aleatória de pontos como na figura do gráfico de resíduos do canto superior esquerdo.\n",
    "\n",
    "#### Erros correlacionados\n",
    "Os erros correlacionados frequentemente ocorrem quando nossos dados são coletados ao longo do tempo (como na projeção de preços de ações ou taxas de juros futuras) ou quando os dados são relacionados espacialmente (como a previsão de regiões de inundações ou secas). Muitas vezes podemos melhorar nossas previsões usando informações dos últimos pontos de dados (para o tempo) ou os pontos nas proximidades (para o espaço).\n",
    "\n",
    "O principal problema em não levar em conta os erros correlacionados é que você poderia utilizar essa correlação para sua vantagem, prevendo de uma maneira melhor os eventos futuros ou eventos espacialmente próximos uns dos outros.\n",
    "\n",
    "Um dos jeitos mais comuns de identificar se os erros são correlacionados é verificar o domínio de onde os dados são coletados. Se você não tiver certeza, há um teste conhecido como teste [Durbin-Watson](https://en.wikipedia.org/wiki/Durbin%E2%80%93Watson_statistic) que é comumente usado para avaliar se a correlação dos erros é um problema. Depois disso, os modelos [ARIMA ou ARMA](http://www.statsref.com/HTML/index.html?arima.html) são comumente implementados para usar esta correlação em previsões melhores.\n",
    "\n",
    "#### Variância não-constante e erros normalmente distribuídos\n",
    "Variância não-constante ocorre quando a propagação dos valores previstos difere dependendo de qual é o valor que se está tentando prever. Isto não é um problema enorme em termos de uma boa previsão. No entanto, isso leva a intervalos de confiança e p-valores que são imprecisos. Os intervalos de confiança para os coeficientes serão amplos demais para áreas onde os valores reais estão mais perto dos valores previstos, mas serão muito estreitos para áreas onde os valores reais estão mais separados dos valores previstos.\n",
    "\n",
    "Comumente, um logaritmo (ou alguma outra transformação da variável de resposta) é feita para “se livrar” da variância não-constante. A fim de escolher a transformação, um [Box-Cox](http://www.statisticshowto.com/box-cox-transformation/) é geralmente usado.\n",
    "\n",
    "A variância não constante pode ser avaliada novamente, usando um gráfico dos resíduos pelos valores previstos. Na imagem na parte inferior da página, a variância não-constante é rotulada como **heteroscedástica**. Idealmente, queremos um modelo não viesado e com resíduos homocedásticos (consistente em toda o intervalo de valores).\n",
    "\n",
    "Embora o texto não aborde a normalidade dos resíduos, esta é uma suposição importante da regressão se você está interessado em criar intervalos de confiança adequados. Mais sobre este tema é fornecido [aqui](http://www.itl.nist.gov/div898/handbook/pri/section2/pri24.htm).\n",
    "\n",
    "#### Outliers/pontos de alavancagem\n",
    "Outliers e pontos de alavancagem são pontos que se encontram longe das tendências regulares de seus dados. Estes pontos podem ter uma grande influência na sua solução. Na prática, estes pontos podem até ser erros de digitação. Se você estiver agregando dados de várias fontes, é possível que alguns dos valores dos dados sejam transferidos ou agregados incorretamente.\n",
    "\n",
    "Em outros momentos os outliers são pontos de dados precisos e verdadeiros, não necessariamente um erro de medição ou de entrada de dados. Nesses casos, o ‘ajuste’ é mais subjetivo. Muitas vezes a estratégia para trabalhar com estes pontos depende do objetivo de sua análise. Modelos lineares usando o método de mínimos quadrados ordinários, em particular, não são muito robustos. Ou seja, outliers grandes podem alterar fortemente nossos resultados. Existem técnicas para combater isso - amplamente conhecidas como técnicas de **regularização**. Elas estão além do escopo desta aula, mas são discutidas rapidamente na versão gratuita do [Nanodegree de Machine Learning](https://classroom.udacity.com/courses/ud120).\n",
    "\n",
    "Um curso inteiro sobre regressão é fornecido pela Penn State University e eles tomam um tempo particularmente grande para discutir o tema dos pontos de alavancagem [aqui](https://newonlinecourses.science.psu.edu/stat501/node/336/).\n",
    "\n",
    "#### Colinearidade (multicolinearidade)\n",
    "ulticolinearidade ocorre quando temos variáveis preditoras que estão correlacionadas entre si. Uma das principais preocupações da multicolinearidade é que ela pode levar a coeficientes a serem invertidos da direção que esperamos na regressão linear simples.\n",
    "\n",
    "Uma das maneiras mais comuns para identificar multicolinearidade é com gráficos bivariados ou com **fatores de inflação de variância (ou VIFs)**.\n",
    "\n",
    "![ibagem](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/June/5b3254fc_estatistica-regressao-linear-multipla-pt/estatistica-regressao-linear-multipla-pt.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Multicollinearity and VIFs\n",
    "\n",
    "One of the main assumptions of multiple linear regression models, is that our predictor variables are uncorrelated with one another (our x-var should be correlated with the response, but not each other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>house_id</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>style</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1112</td>\n",
       "      <td>B</td>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>598291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491</td>\n",
       "      <td>B</td>\n",
       "      <td>3512</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1744259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5952</td>\n",
       "      <td>B</td>\n",
       "      <td>1134</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>571669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3525</td>\n",
       "      <td>A</td>\n",
       "      <td>1940</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>493675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5108</td>\n",
       "      <td>B</td>\n",
       "      <td>2208</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1101539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   house_id neighborhood  area  bedrooms  bathrooms      style    price\n",
       "0      1112            B  1188         3          2      ranch   598291\n",
       "1       491            B  3512         5          3  victorian  1744259\n",
       "2      5952            B  1134         3          2      ranch   571669\n",
       "3      3525            A  1940         4          2      ranch   493675\n",
       "4      5108            B  2208         6          4  victorian  1101539"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./house_prices.csv')\n",
    "df2 = df.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 540x540 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.pairplot(df[['area', 'bedrooms', 'bathrooms']]);\n",
    "# each of them has pretty strong positive relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4230.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 18 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:22:06</td>     <th>  Log-Likelihood:    </th> <td> -84517.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.690e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6024</td>      <th>  BIC:               </th> <td>1.691e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td> 1.007e+04</td> <td> 1.04e+04</td> <td>    0.972</td> <td> 0.331</td> <td>-1.02e+04</td> <td> 3.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>area</th>      <td>  345.9110</td> <td>    7.227</td> <td>   47.863</td> <td> 0.000</td> <td>  331.743</td> <td>  360.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedrooms</th>  <td>-2925.8063</td> <td> 1.03e+04</td> <td>   -0.285</td> <td> 0.775</td> <td> -2.3e+04</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms</th> <td> 7345.3917</td> <td> 1.43e+04</td> <td>    0.515</td> <td> 0.607</td> <td>-2.06e+04</td> <td> 3.53e+04</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>367.658</td> <th>  Durbin-Watson:     </th> <td>   2.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 350.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.536</td>  <th>  Prob(JB):          </th> <td>9.40e-77</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.503</td>  <th>  Cond. No.          </th> <td>1.16e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.16e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.678\n",
       "Model:                            OLS   Adj. R-squared:                  0.678\n",
       "Method:                 Least Squares   F-statistic:                     4230.\n",
       "Date:                Fri, 18 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        10:22:06   Log-Likelihood:                -84517.\n",
       "No. Observations:                6028   AIC:                         1.690e+05\n",
       "Df Residuals:                    6024   BIC:                         1.691e+05\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept   1.007e+04   1.04e+04      0.972      0.331   -1.02e+04    3.04e+04\n",
       "area         345.9110      7.227     47.863      0.000     331.743     360.079\n",
       "bedrooms   -2925.8063   1.03e+04     -0.285      0.775    -2.3e+04    1.72e+04\n",
       "bathrooms   7345.3917   1.43e+04      0.515      0.607   -2.06e+04    3.53e+04\n",
       "==============================================================================\n",
       "Omnibus:                      367.658   Durbin-Watson:                   2.007\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              350.116\n",
       "Skew:                           0.536   Prob(JB):                     9.40e-77\n",
       "Kurtosis:                       2.503   Cond. No.                     1.16e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.16e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['intercept'] = 1\n",
    "lm = sm.OLS(df['price'], df[['intercept', 'area', 'bedrooms', 'bathrooms']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The bedrooms has a negative coefficient associated with it.\n",
    "* Even though price and bedrooms have a positive relationship between one another, in the multiple linear regression model it showed up negative\n",
    "* The interpretation of this coefficient is now counter-intuitive to the relationship expected and what is actually true in the bivariate case.\n",
    "* This is one potential side effect of having multicollinearity in the model, is these flipped coefficients from what you expect to be true.\n",
    "* Another way to identify our predictors is correlated with one another, is **variance inflation factors** (VIFs)\n",
    "* It can be calculated for each x variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,  X = dmatrices('price ~ area + bedrooms + bathrooms', df, return_type='dataframe')\n",
    "vif = pd.DataFrame()\n",
    "vif['VIF Factor'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif['features'] = X.columns\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Remove at least one of these last two variables as both of their variants inflation factors are larger than 10.\n",
    "\n",
    "> Vimos que quando variáveis ‘x’ estão relacionadas entre si, podemos inverter as relações em nossos modelos de regressão linear múltipla frente ao que esperaríamos quando olhamos para as relações bivariadas da regressão linear.\n",
    "\n",
    "> Para saber mais sobre VIFs e multicolinearidade, aqui está a publicação referenciada do vídeo sobre [VIFs](https://etav.github.io/python/vif_factor_python.html).\n",
    "\n",
    "* The case that X variables were correlated with one another can lead to flipped regression coefficients from the expected relationships and inaccurate hypothesis testing results.\n",
    "* When X variables are related to one another these results can be very misleading.\n",
    "* We saw two ways to identify multicollinearity: scatterplot matrix or variance inflation factors (VIFs).\n",
    "* If you have larger than ten for a VIF then you have multicollinearity in your model. \n",
    "* VIF for a particular variable is computed as one over one minus R squared, where the R squared is computed as the R squared for that X variable being predicted by all the other X variables.\n",
    "\n",
    "$VIF_i=\\frac{1}{1-R_i^2}$\n",
    "\n",
    "$x_i=b_0+b_1x_1+b_2x_2+...+b_n+x_n$\n",
    "\n",
    " more R2 = less (1-R2) =  more $VIF_i$\n",
    "\n",
    "It's unusual to find only one large VIF in a model, because if two or more X variables are related to one anoter you would expect each of these variables to have a high VIF.\n",
    "The most common way to work with variables that are corre\n",
    "\n",
    "> 1. As relações esperadas entre suas variáveis ‘x’ e a de resposta podem não se sustentar quando a multicolinearidade está presente. Ou seja, você pode esperar uma relação positiva entre as variáveis explicativas e a resposta (com base nas relações bivariadas), mas no caso da regressão linear múltipla, acontece que a relação é negativa.\n",
    "> 2. Nossos resultados de teste de hipótese podem não ser confiáveis. Acontece que tendo variáveis explicativas correlacionadas significa que nossas estimativas de coeficiente são menos estáveis. Ou seja, os desvios padrão (muitas vezes chamados de erros padrão) associados com seus coeficientes de regressão são bastante grandes. Portanto, uma variável em particular pode ser útil para prever a resposta, mas por causa da relação que tem com outras variáveis ‘x’, você já não vai ver esta associação.\n",
    "\n",
    "## How do we know if our model fits well?\n",
    "\n",
    "It's possible to fit linear models that look like non-linear models, by adding higher order terms, like interactions $(x_1x_2)$, quadratics $(x²)$, cubics $(x^3)$, and even higher order values $(x⁴)$ to the model. \n",
    "It might allow us to better predict the response, and the interpretation more complex, the interpretations for lower order terms (like the slopes on square footage or on hte level of a categorical variable), are no longer easily interpreted when these terms show up in higher order terms.\n",
    "\n",
    "> **Como identificar termos de ordem superior?**\n",
    "> Termos de ordem superior em modelos lineares são criados quando multiplicamos duas ou mais variáveis ‘x’ entre si. Termos comuns de ordem superior incluem quadráticos $(x_1²)$ e cúbicos $(x_1³)$, onde uma variável ‘x’ é multiplicada por ela mesma, assim como as interações $(x_1x_2)$, onde duas ou mais variáveis ‘x’ são multiplicadas umas pelas outras.\n",
    "![ibagi](https://d17h27t6h515a5.cloudfront.net/topher/2017/December/5a29a5f5_screen-shot-2017-12-07-at-1.33.46-pm/screen-shot-2017-12-07-at-1.33.46-pm.png)\n",
    "> Em um modelo sem termos de ordem superior, você pode ter uma equação como:\n",
    "\n",
    "$\\hat{y}=b_0+b_1x_1+b_2x_2$\n",
    "> Então podemos decidir que talvez o modelo linear possa ser melhorado com termos de ordem superior. A equação pode mudar para:\n",
    "\n",
    "$\\hat{y}=b_0+b_1x_1+b_2x_1^2+b_3x_2+b_4x_1x_2$\n",
    "> Aqui, nós introduzimos uma função quadrática $(b_2x_1^2)$ e um termo de interação $(b_4x_1x_2)$ no modelo.\n",
    "\n",
    "> Em geral, esses termos podem ajudar a ajustar relações mais complexas aos seus dados. No entanto, eles também se beneficiam da facilidade de interpretar os coeficientes, como temos visto até agora. Você deve estar se perguntando: \"Como eu identifico se preciso de um desses termos de ordem superior?\"\n",
    "\n",
    "> Ao criar modelos **quadráticos**, **cúbicos**, ou ordens ainda mais altas de uma variável, estamos essencialmente vendo quantas curvas existem nas relações entre as variáveis explicativas e de resposta.\n",
    "\n",
    "> Se houver uma curva, como no gráfico abaixo, então você vai querer adicionar uma função quadrática. Claramente, podemos ver que uma linha não é o melhor ajuste para esta relação.\n",
    "![maisibagem](https://d17h27t6h515a5.cloudfront.net/topher/2017/January/58868097_quadraticlinearregression/quadraticlinearregression.png)\n",
    "\n",
    "> Então, se queremos adicionar uma relação cúbica, é porque vemos duas curvas na relação entre a variável explicativa e a de resposta. Um exemplo disso é mostrado no gráfico abaixo.\n",
    "\n",
    "> [so what?](https://tamino.wordpress.com/2011/03/31/so-what/)\n",
    "\n",
    "---\n",
    "\n",
    "## Interactions and higher order terms\n",
    "\n",
    "To fit models where the response is not linearly related to the explanatory variable we have to use **higher order terms**\n",
    "* Cubic, quadratics, interactions, where more than one variable is attached to a coeficient.\n",
    "\n",
    "To add to the model, we can simply multiply our columns by one another or squaring all the values in (quadratic relationship)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('house_prices.csv')\n",
    "df['bedrooms_squared'] = df['bedrooms']*df['bedrooms']\n",
    "df['intercept'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = sm.OLS(df['price'], df[['intercept', 'bedrooms', 'bedrooms_squared']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* bedrooms' coef isn't the predicted change in price for each addtional bedroom any longer, because changing the bedroom and this term also means changing it in the quadratic term.\n",
    "* The change in price is dependent on the starting and ending number of bedrooms.\n",
    "* The change in price for changing from three to four bedrooms isn't the same as changing from five to six bedrooms. So neither of these terms is easily interpreted.\n",
    "\n",
    "We could also add a cubic term to our dataset and add to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bedrooms_cubed'] = df['bedrooms_squared']*df['bedrooms']\n",
    "lm = sm.OLS(df['price'], df[['intercept', 'bedrooms', 'bedrooms_squared', 'bedrooms_cubed']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The r-squared increased by a little bit. But not a substantial amount for including this cube number of bedrooms;\n",
    "* Isn't appropriate to think that the cubed value of the bedrooms is associated with the price;\n",
    "* Each of the coefficients down there isn't interpreted in a nice way;\n",
    "\n",
    "**Interaction term:** Created when multiplying two or more x-variables by one another to add to the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a interaction between the area and the bedrooms number\n",
    "df['area_bed'] = df['area']*df['bedrooms']\n",
    "# Add the lower order terms to the model\n",
    "lm = sm.OLS(df['price'], df[[\n",
    "    'intercept', 'bedrooms', 'bedrooms_squared', 'bedrooms_cubed',\n",
    "    'area', 'area_bed']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There is a substantial improvement, how we know er should add one of these terms?\n",
    "\n",
    "---\n",
    "\n",
    "## Interpreting interactions\n",
    "* When adding higher order terms, also add the lower order terms;\n",
    "\n",
    "When adding interaction terms, you're considering it the way that a variable $X_1$ is related to your response $Y$, is dependent on another variable $X_2$\n",
    "\n",
    "The question of adding an interaction is a question about the slopes being close enough to equal. If it is, then we don't add an interaction. If it isn't, is an indication that we should add an interaction to the model.\n",
    "\n",
    "> **Termos de interação**\n",
    "\n",
    ">No vídeo anterior, você foi apresentado para a maneira como você pode interpretar as interações (ou como ser capaz de identificá-las).\n",
    "\n",
    "> Matematicamente, uma interação é criada pela multiplicação de duas variáveis, uma pela outra, e adicionando este termo ao nosso modelo de regressão linear.\n",
    "\n",
    "> O exemplo do vídeo anterior usou **área** ($x_1$) e a vizinhança (ou bairro) ($x_2$)de uma casa (sendo **A** ou **B**) para prever o **preço** ($y$) da casa. Na parte superior da tela no vídeo, você deve ter notado a equação para um modelo linear usando essas variáveis, como:\n",
    "\n",
    "$\\hat{y}=b_0+b_1x_1+b_2x_2$\n",
    "\n",
    "> Este exemplo não envolve um termo de interação, e este modelo é apropriado se a relação entre as variáveis parece como no gráfico abaixo.\n",
    "\n",
    "![ibagen](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/June/5b329a2d_reg.-linear-mult.-texto-interp.-pt/reg.-linear-mult.-texto-interp.-pt.png)\n",
    "\n",
    "> Onde $b_1$ é a maneira como estimamos a relação entre **área** e **preço**, que neste modelo que acreditamos ser a mesma, independentemente do bairro. Então $b_2$ é a diferença de preço dependendo do local no qual está localizada, que é a distância **vertical** entre as duas linhas:\n",
    "\n",
    "![ibagensona](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/June/5b32a41b_screen-shot-2017-12-07-at-3.56.39-pm-pt/screen-shot-2017-12-07-at-3.56.39-pm-pt.png)\n",
    "\n",
    "> Observe que, aqui, a maneira com que **área** está relacionada com **preço** é a mesma, independentemente da **vizinhança**.\n",
    "\n",
    ">  A diferença em preço para as diferentes **vizinhanças** é a mesma, independentemente da **área**. Quando estas declarações são verdadeiras, não precisamos de um termo de interação em nosso modelo. No entanto, precisamos de uma interação quando a **maneira com que a área se relaciona com o preço varia dependendo do local**.\n",
    "\n",
    "> Matematicamente, quando a maneira com que a área se relaciona com o preço depende do local, isto sugere que devemos acrescentar uma interação. Adicionando a interação, permitimos que as inclinações da linha para cada vizinhança sejam diferentes, conforme mostrado no gráfico abaixo. Aqui, adicionamos a interação, e você pode ver que isto permite uma diferença entre as duas inclinações.\n",
    "\n",
    "> Estas linhas podem até se cruzar ou se afastar rapidamente. Qualquer uma dessas situações sugeriria que uma interação está presente entre **área** e **vizinhança** na maneira com que elas se relacionam com o **preço**.\n",
    "\n",
    "We will be fitting a number of different models to this dataset throughout this notebook.  For each model, there is a quiz question that will allow you to match the interpretations of the model coefficients to the corresponding values.  If there is no 'nice' interpretation, this is also an option!\n",
    "\n",
    "### Model 1\n",
    "\n",
    "`1.` For the first model, fit a model to predict `price` using `neighborhood`, `style`, and the `area` of the home.  Use the output to match the correct values to the corresponding interpretation in quiz 1 below.  Don't forget an intercept!  You will also need to build your dummy variables, and don't forget to drop one of the columns when you are fitting your linear model. It may be easiest to connect your interpretations to the values in the first quiz by creating the baselines as neighborhood C and home style **lodge**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>house_id</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>style</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1112</td>\n",
       "      <td>B</td>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>598291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491</td>\n",
       "      <td>B</td>\n",
       "      <td>3512</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1744259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5952</td>\n",
       "      <td>B</td>\n",
       "      <td>1134</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>571669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3525</td>\n",
       "      <td>A</td>\n",
       "      <td>1940</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>493675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5108</td>\n",
       "      <td>B</td>\n",
       "      <td>2208</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1101539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   house_id neighborhood  area  bedrooms  bathrooms      style    price\n",
       "0      1112            B  1188         3          2      ranch   598291\n",
       "1       491            B  3512         5          3  victorian  1744259\n",
       "2      5952            B  1134         3          2      ranch   571669\n",
       "3      3525            A  1940         4          2      ranch   493675\n",
       "4      5108            B  2208         6          4  victorian  1101539"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./house_prices.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>house_id</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>style</th>\n",
       "      <th>price</th>\n",
       "      <th>lodge</th>\n",
       "      <th>ranch</th>\n",
       "      <th>victorian</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>intercept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1112</td>\n",
       "      <td>B</td>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>598291</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491</td>\n",
       "      <td>B</td>\n",
       "      <td>3512</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1744259</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5952</td>\n",
       "      <td>B</td>\n",
       "      <td>1134</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>571669</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3525</td>\n",
       "      <td>A</td>\n",
       "      <td>1940</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>493675</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5108</td>\n",
       "      <td>B</td>\n",
       "      <td>2208</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1101539</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   house_id neighborhood  area  bedrooms  bathrooms      style    price  \\\n",
       "0      1112            B  1188         3          2      ranch   598291   \n",
       "1       491            B  3512         5          3  victorian  1744259   \n",
       "2      5952            B  1134         3          2      ranch   571669   \n",
       "3      3525            A  1940         4          2      ranch   493675   \n",
       "4      5108            B  2208         6          4  victorian  1101539   \n",
       "\n",
       "   lodge  ranch  victorian  A  B  C  intercept  \n",
       "0      0      1          0  0  1  0          1  \n",
       "1      0      0          1  0  1  0          1  \n",
       "2      0      1          0  0  1  0          1  \n",
       "3      0      1          0  1  0  0          1  \n",
       "4      0      0          1  0  1  0          1  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.join(pd.get_dummies(df['style']))\n",
    "df = df.join(pd.get_dummies(df['neighborhood']))\n",
    "df['intercept'] = 1\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.919</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.919</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>1.372e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 18 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:44:27</td>     <th>  Log-Likelihood:    </th> <td> -80348.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.607e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6022</td>      <th>  BIC:               </th> <td>1.607e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>area</th>      <td>  348.7375</td> <td>    2.205</td> <td>  158.177</td> <td> 0.000</td> <td>  344.415</td> <td>  353.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>-1.983e+05</td> <td> 5540.744</td> <td>  -35.791</td> <td> 0.000</td> <td>-2.09e+05</td> <td>-1.87e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>A</th>         <td> -194.2464</td> <td> 4965.459</td> <td>   -0.039</td> <td> 0.969</td> <td>-9928.324</td> <td> 9539.832</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>B</th>         <td> 5.243e+05</td> <td> 4687.484</td> <td>  111.844</td> <td> 0.000</td> <td> 5.15e+05</td> <td> 5.33e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ranch</th>     <td>-1974.7032</td> <td> 5757.527</td> <td>   -0.343</td> <td> 0.732</td> <td>-1.33e+04</td> <td> 9312.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>victorian</th> <td>-6262.7365</td> <td> 6893.293</td> <td>   -0.909</td> <td> 0.364</td> <td>-1.98e+04</td> <td> 7250.586</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>114.369</td> <th>  Durbin-Watson:     </th> <td>   2.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 139.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.271</td>  <th>  Prob(JB):          </th> <td>6.29e-31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.509</td>  <th>  Cond. No.          </th> <td>1.12e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.12e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.919\n",
       "Model:                            OLS   Adj. R-squared:                  0.919\n",
       "Method:                 Least Squares   F-statistic:                 1.372e+04\n",
       "Date:                Fri, 18 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        10:44:27   Log-Likelihood:                -80348.\n",
       "No. Observations:                6028   AIC:                         1.607e+05\n",
       "Df Residuals:                    6022   BIC:                         1.607e+05\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "area         348.7375      2.205    158.177      0.000     344.415     353.060\n",
       "intercept  -1.983e+05   5540.744    -35.791      0.000   -2.09e+05   -1.87e+05\n",
       "A           -194.2464   4965.459     -0.039      0.969   -9928.324    9539.832\n",
       "B           5.243e+05   4687.484    111.844      0.000    5.15e+05    5.33e+05\n",
       "ranch      -1974.7032   5757.527     -0.343      0.732   -1.33e+04    9312.111\n",
       "victorian  -6262.7365   6893.293     -0.909      0.364   -1.98e+04    7250.586\n",
       "==============================================================================\n",
       "Omnibus:                      114.369   Durbin-Watson:                   2.002\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              139.082\n",
       "Skew:                           0.271   Prob(JB):                     6.29e-31\n",
       "Kurtosis:                       3.509   Cond. No.                     1.12e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.12e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml = sm.OLS(df['price'], df[['area', 'intercept', 'A', 'B', 'ranch', 'victorian']])\n",
    "results = ml.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A diferença prevista no preço de uma casa na vizinhança A em comparação a uma casa na vizinhança C, mantendo constantes as outras variáveis: -194.25\n",
    "* Ao aumentar uma unidade da área de uma casa, prevemos que o preço da casa aumente em 348.74 (mantendo constantes as outras variáveis)?\n",
    "* O preço previsto se a casa é uma hospedaria na vizinhança C com uma área de 0 (-198300).\n",
    "* A diferença de preço prevista entre uma casa vitoriana e uma hospedaria, mantendo constantes todas as variáveis, a hospedaria é mais cara por 6262.73.\n",
    "\n",
    "### Model 2\n",
    "\n",
    "`2.` Now let's try a second model for predicting price.  This time, use `area` and `area squared` to predict price.  Also use the `style` of the home, but not `neighborhood` this time. You will again need to use your dummy variables, and add an intercept to the model. Use the results of your model to answer quiz questions 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['area_squared'] = df['area']*df['area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['ranch' 'victorian'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1052dbe2093d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOLS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intercept'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'area'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'area_squared'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ranch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'victorian'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2678\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2679\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2680\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2721\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2722\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2723\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2724\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1325\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[0;32m-> 1327\u001b[0;31m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['ranch' 'victorian'] not in index\""
     ]
    }
   ],
   "source": [
    "lm = sm.OLS(df['price'], df[['intercept', 'area', 'area_squared', 'ranch', 'victorian']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ao aumentar cada unidade na área da construção e cada unidade de área quadrada da casa, não é possível prever se o preço aumenta ou diminui já que a variavel está associada a ordem superior;\n",
    "* Com base nos resultados, você acha que é útil adicionar um termo de ordem superior para a área na hora de prever o preço de uma casa? Não, o r-squared diminuiu.\n",
    "* A diferença prevista entre o preço da casa de fazenda e uma hospedaria, mantendo todas as outras variáveis constantes é de 9917.25\n",
    "* Com o termo de ordem superior, os coeficientes associados com área e área ao quadrado não são facilmente interpretáveis. No entanto, coeficientes que não estão associados com os termos de ordem superior são ainda interpretáveis da maneira que você fez anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.964</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.964</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>7.983e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 17 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>14:08:36</td>     <th>  Log-Likelihood:    </th> <td> -81330.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.627e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6026</td>      <th>  BIC:               </th> <td>1.627e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>area</th> <td>  287.5637</td> <td>    1.095</td> <td>  262.610</td> <td> 0.000</td> <td>  285.417</td> <td>  289.710</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>B</th>    <td> 4.535e+05</td> <td> 4261.287</td> <td>  106.427</td> <td> 0.000</td> <td> 4.45e+05</td> <td> 4.62e+05</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1996.505</td> <th>  Durbin-Watson:     </th> <td>   1.888</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>8039.495</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.598</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 7.669</td>  <th>  Cond. No.          </th> <td>4.66e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 4.66e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.964\n",
       "Model:                            OLS   Adj. R-squared:                  0.964\n",
       "Method:                 Least Squares   F-statistic:                 7.983e+04\n",
       "Date:                Thu, 17 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        14:08:36   Log-Likelihood:                -81330.\n",
       "No. Observations:                6028   AIC:                         1.627e+05\n",
       "Df Residuals:                    6026   BIC:                         1.627e+05\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "area         287.5637      1.095    262.610      0.000     285.417     289.710\n",
       "B           4.535e+05   4261.287    106.427      0.000    4.45e+05    4.62e+05\n",
       "==============================================================================\n",
       "Omnibus:                     1996.505   Durbin-Watson:                   1.888\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             8039.495\n",
       "Skew:                           1.598   Prob(JB):                         0.00\n",
       "Kurtosis:                       7.669   Cond. No.                     4.66e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 4.66e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml = sm.OLS(df['price'], df[['area', 'B']])\n",
    "results = ml.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* O melhor modelo deve incluir somente a área, e uma variável dummy para a vizinhança B vs. as outras vizinhanças.\n",
    "* A julgar pelos primeiros resultados dos dois modelos que você construiu, o melhor provavelmente envolveria apenas estas duas variáveis, pois seria simplificado, enquanto ainda preveria bem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
