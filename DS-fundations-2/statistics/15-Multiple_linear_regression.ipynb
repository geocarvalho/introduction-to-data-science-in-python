{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple linear regression introduction\n",
    "We can use linear algebra to predict the **$Y$** value using all multi-column at the same time. The way we do this is by creating a matrix of inputs, and we create a vector of the response that we want to predict. The matrix with the multiple column is denoted **$Y$** (capital x bold) while the vector for the response is denoted **$y$** (y bold)\n",
    "* [Linear algebra from Khan academy](https://www.khanacademy.org/math/linear-algebra)\n",
    "* [an introduction to statistical learning](http://www-bcf.usc.edu/~gareth/ISL/) - chapter 3\n",
    "\n",
    "In this notebook (and following quizzes), you will be creating a few simple linear regression models, as well as a multiple linear regression model, to predict home value.\n",
    "\n",
    "Let's get started by importing the necessary libraries and reading in the data you will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>house_id</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>style</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1112</td>\n",
       "      <td>B</td>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>598291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491</td>\n",
       "      <td>B</td>\n",
       "      <td>3512</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1744259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5952</td>\n",
       "      <td>B</td>\n",
       "      <td>1134</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>571669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3525</td>\n",
       "      <td>A</td>\n",
       "      <td>1940</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>493675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5108</td>\n",
       "      <td>B</td>\n",
       "      <td>2208</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1101539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   house_id neighborhood  area  bedrooms  bathrooms      style    price\n",
       "0      1112            B  1188         3          2      ranch   598291\n",
       "1       491            B  3512         5          3  victorian  1744259\n",
       "2      5952            B  1134         3          2      ranch   571669\n",
       "3      3525            A  1940         4          2      ranch   493675\n",
       "4      5108            B  2208         6          4  victorian  1101539"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "from patsy import dmatrices\n",
    "import statsmodels.api as sm;\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('./house_prices.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4230.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 16 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:03:08</td>     <th>  Log-Likelihood:    </th> <td> -84517.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.690e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6024</td>      <th>  BIC:               </th> <td>1.691e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>area</th>      <td>  345.9110</td> <td>    7.227</td> <td>   47.863</td> <td> 0.000</td> <td>  331.743</td> <td>  360.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedrooms</th>  <td>-2925.8063</td> <td> 1.03e+04</td> <td>   -0.285</td> <td> 0.775</td> <td> -2.3e+04</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms</th> <td> 7345.3917</td> <td> 1.43e+04</td> <td>    0.515</td> <td> 0.607</td> <td>-2.06e+04</td> <td> 3.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td> 1.007e+04</td> <td> 1.04e+04</td> <td>    0.972</td> <td> 0.331</td> <td>-1.02e+04</td> <td> 3.04e+04</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>367.658</td> <th>  Durbin-Watson:     </th> <td>   2.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 350.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.536</td>  <th>  Prob(JB):          </th> <td>9.40e-77</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.503</td>  <th>  Cond. No.          </th> <td>1.16e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.16e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.678\n",
       "Model:                            OLS   Adj. R-squared:                  0.678\n",
       "Method:                 Least Squares   F-statistic:                     4230.\n",
       "Date:                Wed, 16 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        23:03:08   Log-Likelihood:                -84517.\n",
       "No. Observations:                6028   AIC:                         1.690e+05\n",
       "Df Residuals:                    6024   BIC:                         1.691e+05\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "area         345.9110      7.227     47.863      0.000     331.743     360.079\n",
       "bedrooms   -2925.8063   1.03e+04     -0.285      0.775    -2.3e+04    1.72e+04\n",
       "bathrooms   7345.3917   1.43e+04      0.515      0.607   -2.06e+04    3.53e+04\n",
       "intercept   1.007e+04   1.04e+04      0.972      0.331   -1.02e+04    3.04e+04\n",
       "==============================================================================\n",
       "Omnibus:                      367.658   Durbin-Watson:                   2.007\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              350.116\n",
       "Skew:                           0.536   Prob(JB):                     9.40e-77\n",
       "Kurtosis:                       2.503   Cond. No.                     1.16e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.16e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['intercept'] = 1\n",
    "mlm = sm.OLS(df['price'], df[['area', 'bedrooms', 'bathrooms', 'intercept']])\n",
    "results = mlm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.` Using statsmodels, fit three individual simple linear regression models to predict price.  You should have a model that uses **area**, another using **bedrooms**, and a final one using **bathrooms**.  You will also want to use an intercept in each of your three models.\n",
    "\n",
    "Use the results from each of your models to answer the first two quiz questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>1.269e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 16 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:03:09</td>     <th>  Log-Likelihood:    </th> <td> -84517.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.690e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6026</td>      <th>  BIC:               </th> <td>1.691e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td> 9587.8878</td> <td> 7637.479</td> <td>    1.255</td> <td> 0.209</td> <td>-5384.303</td> <td> 2.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>area</th>      <td>  348.4664</td> <td>    3.093</td> <td>  112.662</td> <td> 0.000</td> <td>  342.403</td> <td>  354.530</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>368.609</td> <th>  Durbin-Watson:     </th> <td>   2.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 349.279</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.534</td>  <th>  Prob(JB):          </th> <td>1.43e-76</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.499</td>  <th>  Cond. No.          </th> <td>4.93e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 4.93e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.678\n",
       "Model:                            OLS   Adj. R-squared:                  0.678\n",
       "Method:                 Least Squares   F-statistic:                 1.269e+04\n",
       "Date:                Wed, 16 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        23:03:09   Log-Likelihood:                -84517.\n",
       "No. Observations:                6028   AIC:                         1.690e+05\n",
       "Df Residuals:                    6026   BIC:                         1.691e+05\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept   9587.8878   7637.479      1.255      0.209   -5384.303    2.46e+04\n",
       "area         348.4664      3.093    112.662      0.000     342.403     354.530\n",
       "==============================================================================\n",
       "Omnibus:                      368.609   Durbin-Watson:                   2.007\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              349.279\n",
       "Skew:                           0.534   Prob(JB):                     1.43e-76\n",
       "Kurtosis:                       2.499   Cond. No.                     4.93e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 4.93e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['intercept'] = 1\n",
    "lm1 = sm.OLS(df['price'], df[['intercept', 'area']])\n",
    "result1 = lm1.fit()\n",
    "result1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.553</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.553</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   7446.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 16 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:03:10</td>     <th>  Log-Likelihood:    </th> <td> -85509.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.710e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6026</td>      <th>  BIC:               </th> <td>1.710e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>-9.485e+04</td> <td> 1.08e+04</td> <td>   -8.762</td> <td> 0.000</td> <td>-1.16e+05</td> <td>-7.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedrooms</th>  <td> 2.284e+05</td> <td> 2646.744</td> <td>   86.289</td> <td> 0.000</td> <td> 2.23e+05</td> <td> 2.34e+05</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>967.118</td> <th>  Durbin-Watson:     </th> <td>   2.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1599.431</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.074</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.325</td>  <th>  Cond. No.          </th> <td>    10.3</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.553\n",
       "Model:                            OLS   Adj. R-squared:                  0.553\n",
       "Method:                 Least Squares   F-statistic:                     7446.\n",
       "Date:                Wed, 16 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        23:03:10   Log-Likelihood:                -85509.\n",
       "No. Observations:                6028   AIC:                         1.710e+05\n",
       "Df Residuals:                    6026   BIC:                         1.710e+05\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept  -9.485e+04   1.08e+04     -8.762      0.000   -1.16e+05   -7.36e+04\n",
       "bedrooms    2.284e+05   2646.744     86.289      0.000    2.23e+05    2.34e+05\n",
       "==============================================================================\n",
       "Omnibus:                      967.118   Durbin-Watson:                   2.014\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1599.431\n",
       "Skew:                           1.074   Prob(JB):                         0.00\n",
       "Kurtosis:                       4.325   Cond. No.                         10.3\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm2 = sm.OLS(df['price'], df[['intercept', 'bedrooms']])\n",
    "result2 = lm2.fit()\n",
    "result2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.541</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.541</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   7116.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 16 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:03:10</td>     <th>  Log-Likelihood:    </th> <td> -85583.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.712e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6026</td>      <th>  BIC:               </th> <td>1.712e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td> 4.314e+04</td> <td> 9587.189</td> <td>    4.500</td> <td> 0.000</td> <td> 2.43e+04</td> <td> 6.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms</th> <td> 3.295e+05</td> <td> 3905.540</td> <td>   84.358</td> <td> 0.000</td> <td> 3.22e+05</td> <td> 3.37e+05</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>915.429</td> <th>  Durbin-Watson:     </th> <td>   2.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1537.531</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.010</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.428</td>  <th>  Cond. No.          </th> <td>    5.84</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.541\n",
       "Model:                            OLS   Adj. R-squared:                  0.541\n",
       "Method:                 Least Squares   F-statistic:                     7116.\n",
       "Date:                Wed, 16 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        23:03:10   Log-Likelihood:                -85583.\n",
       "No. Observations:                6028   AIC:                         1.712e+05\n",
       "Df Residuals:                    6026   BIC:                         1.712e+05\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept   4.314e+04   9587.189      4.500      0.000    2.43e+04    6.19e+04\n",
       "bathrooms   3.295e+05   3905.540     84.358      0.000    3.22e+05    3.37e+05\n",
       "==============================================================================\n",
       "Omnibus:                      915.429   Durbin-Watson:                   2.003\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1537.531\n",
       "Skew:                           1.010   Prob(JB):                         0.00\n",
       "Kurtosis:                       4.428   Cond. No.                         5.84\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm3 = sm.OLS(df['price'], df[['intercept', 'bathrooms']])\n",
    "result3 = lm3.fit()\n",
    "result3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Now that you have looked at the results from the simple linear regression models, let's try a multiple linear regression model using all three of these variables  at the same time.  You will still want an intercept in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4230.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 16 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:03:10</td>     <th>  Log-Likelihood:    </th> <td> -84517.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.690e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6024</td>      <th>  BIC:               </th> <td>1.691e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td> 1.007e+04</td> <td> 1.04e+04</td> <td>    0.972</td> <td> 0.331</td> <td>-1.02e+04</td> <td> 3.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>area</th>      <td>  345.9110</td> <td>    7.227</td> <td>   47.863</td> <td> 0.000</td> <td>  331.743</td> <td>  360.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedrooms</th>  <td>-2925.8063</td> <td> 1.03e+04</td> <td>   -0.285</td> <td> 0.775</td> <td> -2.3e+04</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms</th> <td> 7345.3917</td> <td> 1.43e+04</td> <td>    0.515</td> <td> 0.607</td> <td>-2.06e+04</td> <td> 3.53e+04</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>367.658</td> <th>  Durbin-Watson:     </th> <td>   2.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 350.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.536</td>  <th>  Prob(JB):          </th> <td>9.40e-77</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.503</td>  <th>  Cond. No.          </th> <td>1.16e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.16e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.678\n",
       "Model:                            OLS   Adj. R-squared:                  0.678\n",
       "Method:                 Least Squares   F-statistic:                     4230.\n",
       "Date:                Wed, 16 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        23:03:10   Log-Likelihood:                -84517.\n",
       "No. Observations:                6028   AIC:                         1.690e+05\n",
       "Df Residuals:                    6024   BIC:                         1.691e+05\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept   1.007e+04   1.04e+04      0.972      0.331   -1.02e+04    3.04e+04\n",
       "area         345.9110      7.227     47.863      0.000     331.743     360.079\n",
       "bedrooms   -2925.8063   1.03e+04     -0.285      0.775    -2.3e+04    1.72e+04\n",
       "bathrooms   7345.3917   1.43e+04      0.515      0.607   -2.06e+04    3.53e+04\n",
       "==============================================================================\n",
       "Omnibus:                      367.658   Durbin-Watson:                   2.007\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              350.116\n",
       "Skew:                           0.536   Prob(JB):                     9.40e-77\n",
       "Kurtosis:                       2.503   Cond. No.                     1.16e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.16e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = sm.OLS(df['price'], df[['intercept', 'area', 'bedrooms', 'bathrooms']])\n",
    "result = lm.fit()\n",
    "result.summary() # multicolinearidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Along with using the **area**, **bedrooms**, and **bathrooms** you might also want to use **style** to predict the price.  Try adding this to your multiple linear regression model.  What happens?  Use the final quiz below to provide your answer.\n",
    "\n",
    "* There is an error, because an object cannot be added to the multiple linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-af8693246f66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstyle_lm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOLS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intercept'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'area'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bedrooms'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bathrooms'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'style'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresult_style\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstyle_lm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresult_style\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/george/miniconda2/lib/python2.7/site-packages/statsmodels/regression/linear_model.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    815\u001b[0m                  **kwargs):\n\u001b[1;32m    816\u001b[0m         super(OLS, self).__init__(endog, exog, missing=missing,\n\u001b[0;32m--> 817\u001b[0;31m                                   hasconst=hasconst, **kwargs)\n\u001b[0m\u001b[1;32m    818\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"weights\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/george/miniconda2/lib/python2.7/site-packages/statsmodels/regression/linear_model.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, weights, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         super(WLS, self).__init__(endog, exog, missing=missing,\n\u001b[0;32m--> 663\u001b[0;31m                                   weights=weights, hasconst=hasconst, **kwargs)\n\u001b[0m\u001b[1;32m    664\u001b[0m         \u001b[0mnobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/george/miniconda2/lib/python2.7/site-packages/statsmodels/regression/linear_model.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \"\"\"\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRegressionModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_attr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pinv_wexog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wendog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wexog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/george/miniconda2/lib/python2.7/site-packages/statsmodels/base/model.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLikelihoodModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/george/miniconda2/lib/python2.7/site-packages/statsmodels/base/model.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mhasconst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hasconst'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         self.data = self._handle_data(endog, exog, missing, hasconst,\n\u001b[0;32m---> 64\u001b[0;31m                                       **kwargs)\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_constant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/george/miniconda2/lib/python2.7/site-packages/statsmodels/base/model.pyc\u001b[0m in \u001b[0;36m_handle_data\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhasconst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhasconst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;31m# kwargs arrays could have changed, easier to just attach here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/george/miniconda2/lib/python2.7/site-packages/statsmodels/base/data.pyc\u001b[0m in \u001b[0;36mhandle_data\u001b[0;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m     \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_data_class_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m     return klass(endog, exog=exog, missing=missing, hasconst=hasconst,\n\u001b[0;32m--> 633\u001b[0;31m                  **kwargs)\n\u001b[0m",
      "\u001b[0;32m/home/george/miniconda2/lib/python2.7/site-packages/statsmodels/base/data.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_endog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_exog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_endog_exog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# this has side-effects, attaches k_constant and const_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/george/miniconda2/lib/python2.7/site-packages/statsmodels/base/data.pyc\u001b[0m in \u001b[0;36m_convert_endog_exog\u001b[0;34m(self, endog, exog)\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0mexog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexog\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mexog\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mexog\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m             raise ValueError(\"Pandas data cast to numpy dtype of object. \"\n\u001b[0m\u001b[1;32m    475\u001b[0m                              \"Check input data with np.asarray(data).\")\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPandasData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_endog_exog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Pandas data cast to numpy dtype of object. Check input data with np.asarray(data)."
     ]
    }
   ],
   "source": [
    "style_lm = sm.OLS(df['price'], df[['intercept', 'area', 'bedrooms', 'bathrooms', 'style']])\n",
    "result_style = style_lm.fit()\n",
    "result_style.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y matrix\n",
    "X = df[['intercept', 'area', 'bedrooms', 'bathrooms']]\n",
    "y = df['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression closed form solution:\n",
    "\n",
    "$\\hat{\\beta}=(X`X)^-X`y$\n",
    "\n",
    "$X`$ means X transpose\n",
    "\n",
    "$()⁻$ means the inverstion of the results inside the parentheses\n",
    "\n",
    "In Numpy get the transpose, the inverses and the dot products to get the coeficients results for each column\n",
    "\n",
    "* [OLS in matrix form](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-293da30d4ed0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(X'X)^-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(X'X)^- * X'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (X'X)^-X' * y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "beta = np.linalg.inv(np.dot(X.transpose(),X)) #(X'X)^-\n",
    "beta = np.dot(beta, X.transpose()) #(X'X)^- * X'\n",
    "beta = np.dot(beta, y) # (X'X)^-X' * y\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy variables\n",
    "Instead of adding columns with categorical variables, create columns with the presence or not of a variable (been the column name the varible present or not) enconded by 1 (exist) and 0 (not exist). Because the last column can be inferred from the earlier two columns, as the one values are only in the rows that didn't have ones in one of these other columns, we actually end up choosing to drop this column (which one doesn't really matter too much, noted as reference column). Without the last column, the matrix is [full rank](https://www.cds.caltech.edu/~murray/amwiki/index.php/FAQ:_What_does_it_mean_for_a_non-square_matrix_to_be_full_rank%3F)\n",
    "![image-example](https://d17h27t6h515a5.cloudfront.net/topher/2017/December/5a297de8_screen-shot-2017-12-07-at-9.43.05-am/screen-shot-2017-12-07-at-9.43.05-am.png)\n",
    "\n",
    "* The number of variables dummy added to the matrix X and the level of each categorical variable minus one have to be equal;\n",
    "* The motivation to delete one dummy variable is to garanteee that all the column are linear independent, product of $X`X$ is invertible and the matrix X be full rank.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.339</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.339</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1548.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 16 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:03:11</td>     <th>  Log-Likelihood:    </th> <td> -86683.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.734e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6025</td>      <th>  BIC:               </th> <td>1.734e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td> 1.046e+06</td> <td> 7775.607</td> <td>  134.534</td> <td> 0.000</td> <td> 1.03e+06</td> <td> 1.06e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lodge</th>     <td>-7.411e+05</td> <td> 1.44e+04</td> <td>  -51.396</td> <td> 0.000</td> <td>-7.69e+05</td> <td>-7.13e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ranch</th>     <td> -4.71e+05</td> <td> 1.27e+04</td> <td>  -37.115</td> <td> 0.000</td> <td>-4.96e+05</td> <td>-4.46e+05</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1340.120</td> <th>  Durbin-Watson:     </th> <td>   2.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3232.810</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.230</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 5.611</td>  <th>  Cond. No.          </th> <td>    3.28</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.339\n",
       "Model:                            OLS   Adj. R-squared:                  0.339\n",
       "Method:                 Least Squares   F-statistic:                     1548.\n",
       "Date:                Wed, 16 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        23:03:11   Log-Likelihood:                -86683.\n",
       "No. Observations:                6028   AIC:                         1.734e+05\n",
       "Df Residuals:                    6025   BIC:                         1.734e+05\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept   1.046e+06   7775.607    134.534      0.000    1.03e+06    1.06e+06\n",
       "lodge      -7.411e+05   1.44e+04    -51.396      0.000   -7.69e+05   -7.13e+05\n",
       "ranch       -4.71e+05   1.27e+04    -37.115      0.000   -4.96e+05   -4.46e+05\n",
       "==============================================================================\n",
       "Omnibus:                     1340.120   Durbin-Watson:                   2.004\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             3232.810\n",
       "Skew:                           1.230   Prob(JB):                         0.00\n",
       "Kurtosis:                       5.611   Cond. No.                         3.28\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['A', 'B', 'C']] = pd.get_dummies(df['neighborhood']) \n",
    "df[['lodge', 'ranch', 'victorian']] = pd.get_dummies(df['style'])\n",
    "# You always drop one level from each category. This is called the baseline (the dropped column)\n",
    "df['intercept'] = 1\n",
    "lm = sm.OLS(df['price'], df[['intercept', 'lodge', 'ranch']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The intercept means that if our home is a **victorian** home, we predict its price to be 1,046,000 dollars and **lodge** is predicted to be 741,000 less than a **victorian**. A **ranch** is predicted to be 471,000 less than a **victorian**.\n",
    "\n",
    "\n",
    "`1.` Use the [pd.get_dummies](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) documentation to assist you with obtaining dummy variables for the **neighborhood** column.  Then use [join](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.join.html) to add the dummy variables to your dataframe, **df**, and store the joined results in **df_new**.\n",
    "\n",
    "Fit a linear model using **all three levels** of **neighborhood** to predict the price. Don't forget an intercept.\n",
    "\n",
    "Use your results to answer quiz 1 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "columns overlap but no suffix specified: Index([u'A', u'B', u'C'], dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-0aa0c279160c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mneighborhood_dummies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'neighborhood'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneighborhood_dummies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/george/miniconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[1;32m   6334\u001b[0m         \u001b[0;31m# For SparseDataFrame's benefit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6335\u001b[0m         return self._join_compat(other, on=on, how=how, lsuffix=lsuffix,\n\u001b[0;32m-> 6336\u001b[0;31m                                  rsuffix=rsuffix, sort=sort)\n\u001b[0m\u001b[1;32m   6337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6338\u001b[0m     def _join_compat(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[0;32m/home/george/miniconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_join_compat\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[1;32m   6349\u001b[0m             return merge(self, other, left_on=on, how=how,\n\u001b[1;32m   6350\u001b[0m                          \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6351\u001b[0;31m                          suffixes=(lsuffix, rsuffix), sort=sort)\n\u001b[0m\u001b[1;32m   6352\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/george/miniconda2/lib/python2.7/site-packages/pandas/core/reshape/merge.pyc\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     60\u001b[0m                          \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                          validate=validate)\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/george/miniconda2/lib/python2.7/site-packages/pandas/core/reshape/merge.pyc\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         llabels, rlabels = items_overlap_with_suffix(ldata.items, lsuf,\n\u001b[0;32m--> 574\u001b[0;31m                                                      rdata.items, rsuf)\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0mlindexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mleft_indexer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/george/miniconda2/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mitems_overlap_with_suffix\u001b[0;34m(left, lsuffix, right, rsuffix)\u001b[0m\n\u001b[1;32m   5242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlsuffix\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5243\u001b[0m             raise ValueError('columns overlap but no suffix specified: '\n\u001b[0;32m-> 5244\u001b[0;31m                              '{rename}'.format(rename=to_rename))\n\u001b[0m\u001b[1;32m   5245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5246\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mlrenamer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: columns overlap but no suffix specified: Index([u'A', u'B', u'C'], dtype='object')"
     ]
    }
   ],
   "source": [
    "neighborhood_dummies = pd.get_dummies(df['neighborhood'])\n",
    "df_new = df.join(neighborhood_dummies)\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_new['intercept'] = 1\n",
    "lm = sm.OLS(df_new['price'], df_new[['intercept', 'A', 'B', 'C']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.`  Now, fit an appropriate linear model for using **neighborhood** to predict the price of a home. Use **neighborhood A** as your baseline.  Use your resulting model to answer the questions in Quiz 2 and Quiz 3 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-5137271516ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOLS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intercept'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'B'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_new' is not defined"
     ]
    }
   ],
   "source": [
    "lm = sm.OLS(df_new['price'], df_new[['intercept', 'C', 'B']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Observe que cada um dos coeficientes é uma comparação da categoria com a referência. Portanto, um coeficiente positivo sugere que o bairro é mais caro, em média, do que a referência. Por outro lado, um coeficiente negativo sugere que o bairro é menos caro, em média, do que a referência.\n",
    "\n",
    "* Você pode olhar para os valores-p para comparar com o bairro A. Para comparar o bairro B ao bairro C, você pode comparar os intervalos de confiança. Como os intervalos de confiança para B e C não se sobrepõem, temos provas de que eles diferem também.\n",
    "\n",
    "`3.` Run the two cells below to look at the home prices for the A and C neighborhoods. Add neighborhood B. This creates a glimpse into the differences that you found in the previous linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-29aa02c4c120>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"C == 1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'C'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"A == 1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'A'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"B == 1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'B'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_new' is not defined"
     ]
    }
   ],
   "source": [
    "plt.hist(df_new.query(\"C == 1\")['price'], alpha = 0.3, label = 'C');\n",
    "plt.hist(df_new.query(\"A == 1\")['price'], alpha = 0.3, label = 'A');\n",
    "plt.hist(df_new.query(\"B == 1\")['price'], alpha = 0.3, label = 'B')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Now, add dummy variables for the **style** of house. Create a new linear model using these new dummies, as well as the previous **neighborhood** dummies.  Use **ranch** as the baseline for the **style**.  Additionally, add **bathrooms** and **bedrooms** to your linear model.  Don't forget an intercept.  Use the results of your linear model to answer the last two questions below. **Home prices are measured in dollars, and this dataset is not real.**\n",
    "\n",
    "To minimize scrolling, it might be useful to open another browser window to this concept to answer the quiz questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-1fc9cea67b7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'style'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnew_new_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOLS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_new_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_new_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intercept'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'B'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lodge'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'victorian'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bedrooms'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bathrooms'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_new' is not defined"
     ]
    }
   ],
   "source": [
    "sty = pd.get_dummies(df_new['style'])\n",
    "new_new_df = df_new.join(sty)\n",
    "lm = sm.OLS(new_new_df['price'], new_new_df[['intercept', 'B', 'C', 'lodge', 'victorian', 'bedrooms', 'bathrooms']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 80.9%  (r-squared) da variabilidade no preço pode ser explicada pelo modelo linear construído usando o estilo de quartos, banheiros, vizinhança e da casa;\n",
    "* Para cada quarto adicional em uma casa, espera-se um aumento de preço de 173200, em que todas as outras variáveis são mantidas constantes;\n",
    "* Para cada banheiro adicional em uma casa, espera-se um aumento de preço de 99960, em que todas as outras variáveis são mantidas constantes;\n",
    "* Espera-se que uma casa vitoriana custará 70560 a mais do que uma fazenda, sendo todo o resto igual;\n",
    "* Espera-se que uma casa na vizinhança C custará 7168 menos que uma casa na vizinhança A, sendo todo o resto igual.\n",
    "\n",
    "---\n",
    "\n",
    "## Dummy variables recap\n",
    "\n",
    "The biggest reason for use encoding one, zero and negative one is that it changes the coefficients that we get back from the model as well as how interpret those coefficients.\n",
    "With one-zero encoding your interpreter coefficients as a comparion to a baseline category. However, if use one-zer-negative one encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>house_id</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>style</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1112</td>\n",
       "      <td>B</td>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>598291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491</td>\n",
       "      <td>B</td>\n",
       "      <td>3512</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1744259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5952</td>\n",
       "      <td>B</td>\n",
       "      <td>1134</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>571669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3525</td>\n",
       "      <td>A</td>\n",
       "      <td>1940</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>493675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5108</td>\n",
       "      <td>B</td>\n",
       "      <td>2208</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1101539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   house_id neighborhood  area  bedrooms  bathrooms      style    price\n",
       "0      1112            B  1188         3          2      ranch   598291\n",
       "1       491            B  3512         5          3  victorian  1744259\n",
       "2      5952            B  1134         3          2      ranch   571669\n",
       "3      3525            A  1940         4          2      ranch   493675\n",
       "4      5108            B  2208         6          4  victorian  1101539"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./house_prices.csv')\n",
    "df2 = df.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The below function creates 1, 0, -1 coded dummy variables.\n",
    "\n",
    "def dummy_cat(df, col):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df - the dataframe where col is stored\n",
    "    col - the categorical column you want to dummy (as a string)\n",
    "    OUTPUT:\n",
    "    df - the dataframe with the added columns\n",
    "         for dummy variables using 1, 0, -1 coding\n",
    "    '''\n",
    "    for idx, val_0 in enumerate(df[col].unique()):\n",
    "        if idx + 1 < df[col].nunique():            \n",
    "            df[val_0] = df[col].apply(lambda x: 1 if x == val_0 else 0)\n",
    "        else:    \n",
    "            df[val_0] = df[col].apply(lambda x: -1 if x == val_0 else 0)\n",
    "            for idx, val_1 in enumerate(df[col].unique()):\n",
    "                if idx + 1 < df[col].nunique():\n",
    "                    df[val_1] = df[val_0] + df[val_1]\n",
    "                else:\n",
    "                    del df[val_1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>house_id</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>style</th>\n",
       "      <th>price</th>\n",
       "      <th>ranch</th>\n",
       "      <th>victorian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1112</td>\n",
       "      <td>B</td>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>598291</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491</td>\n",
       "      <td>B</td>\n",
       "      <td>3512</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1744259</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5952</td>\n",
       "      <td>B</td>\n",
       "      <td>1134</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>571669</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3525</td>\n",
       "      <td>A</td>\n",
       "      <td>1940</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>493675</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5108</td>\n",
       "      <td>B</td>\n",
       "      <td>2208</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1101539</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7507</td>\n",
       "      <td>C</td>\n",
       "      <td>1785</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>lodge</td>\n",
       "      <td>455235</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4964</td>\n",
       "      <td>B</td>\n",
       "      <td>2996</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1489871</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7627</td>\n",
       "      <td>C</td>\n",
       "      <td>3263</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>victorian</td>\n",
       "      <td>821931</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6571</td>\n",
       "      <td>A</td>\n",
       "      <td>1159</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>299903</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5220</td>\n",
       "      <td>A</td>\n",
       "      <td>1248</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>victorian</td>\n",
       "      <td>321975</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   house_id neighborhood  area  bedrooms  bathrooms      style    price  \\\n",
       "0      1112            B  1188         3          2      ranch   598291   \n",
       "1       491            B  3512         5          3  victorian  1744259   \n",
       "2      5952            B  1134         3          2      ranch   571669   \n",
       "3      3525            A  1940         4          2      ranch   493675   \n",
       "4      5108            B  2208         6          4  victorian  1101539   \n",
       "5      7507            C  1785         4          2      lodge   455235   \n",
       "6      4964            B  2996         5          3  victorian  1489871   \n",
       "7      7627            C  3263         5          3  victorian   821931   \n",
       "8      6571            A  1159         3          2      ranch   299903   \n",
       "9      5220            A  1248         3          2  victorian   321975   \n",
       "\n",
       "   ranch  victorian  \n",
       "0      1          0  \n",
       "1      0          1  \n",
       "2      1          0  \n",
       "3      1          0  \n",
       "4      0          1  \n",
       "5     -1         -1  \n",
       "6      0          1  \n",
       "7      0          1  \n",
       "8      1          0  \n",
       "9      0          1  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = dummy_cat(df, 'style') # Use on style\n",
    "new_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.339</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.339</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1548.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 16 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:03:15</td>     <th>  Log-Likelihood:    </th> <td> -86683.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.734e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6025</td>      <th>  BIC:               </th> <td>1.734e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td> 6.421e+05</td> <td> 5854.251</td> <td>  109.677</td> <td> 0.000</td> <td> 6.31e+05</td> <td> 6.54e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ranch</th>     <td>-6.695e+04</td> <td> 8233.489</td> <td>   -8.131</td> <td> 0.000</td> <td>-8.31e+04</td> <td>-5.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>victorian</th> <td>  4.04e+05</td> <td> 7377.372</td> <td>   54.763</td> <td> 0.000</td> <td>  3.9e+05</td> <td> 4.18e+05</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1340.120</td> <th>  Durbin-Watson:     </th> <td>   2.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3232.810</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.230</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 5.611</td>  <th>  Cond. No.          </th> <td>    1.84</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.339\n",
       "Model:                            OLS   Adj. R-squared:                  0.339\n",
       "Method:                 Least Squares   F-statistic:                     1548.\n",
       "Date:                Wed, 16 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        23:03:15   Log-Likelihood:                -86683.\n",
       "No. Observations:                6028   AIC:                         1.734e+05\n",
       "Df Residuals:                    6025   BIC:                         1.734e+05\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept   6.421e+05   5854.251    109.677      0.000    6.31e+05    6.54e+05\n",
       "ranch      -6.695e+04   8233.489     -8.131      0.000   -8.31e+04   -5.08e+04\n",
       "victorian    4.04e+05   7377.372     54.763      0.000     3.9e+05    4.18e+05\n",
       "==============================================================================\n",
       "Omnibus:                     1340.120   Durbin-Watson:                   2.004\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             3232.810\n",
       "Skew:                           1.230   Prob(JB):                         0.00\n",
       "Kurtosis:                       5.611   Cond. No.                         1.84\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['intercept'] = 1\n",
    "\n",
    "lm = sm.OLS(new_df['price'], new_df[['intercept', 'ranch', 'victorian']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>house_id</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>style</th>\n",
       "      <th>price</th>\n",
       "      <th>lodge</th>\n",
       "      <th>ranch</th>\n",
       "      <th>victorian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1112</td>\n",
       "      <td>B</td>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>598291</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491</td>\n",
       "      <td>B</td>\n",
       "      <td>3512</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1744259</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5952</td>\n",
       "      <td>B</td>\n",
       "      <td>1134</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>571669</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3525</td>\n",
       "      <td>A</td>\n",
       "      <td>1940</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>493675</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5108</td>\n",
       "      <td>B</td>\n",
       "      <td>2208</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1101539</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7507</td>\n",
       "      <td>C</td>\n",
       "      <td>1785</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>lodge</td>\n",
       "      <td>455235</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4964</td>\n",
       "      <td>B</td>\n",
       "      <td>2996</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1489871</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7627</td>\n",
       "      <td>C</td>\n",
       "      <td>3263</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>victorian</td>\n",
       "      <td>821931</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6571</td>\n",
       "      <td>A</td>\n",
       "      <td>1159</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>299903</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5220</td>\n",
       "      <td>A</td>\n",
       "      <td>1248</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>victorian</td>\n",
       "      <td>321975</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   house_id neighborhood  area  bedrooms  bathrooms      style    price  \\\n",
       "0      1112            B  1188         3          2      ranch   598291   \n",
       "1       491            B  3512         5          3  victorian  1744259   \n",
       "2      5952            B  1134         3          2      ranch   571669   \n",
       "3      3525            A  1940         4          2      ranch   493675   \n",
       "4      5108            B  2208         6          4  victorian  1101539   \n",
       "5      7507            C  1785         4          2      lodge   455235   \n",
       "6      4964            B  2996         5          3  victorian  1489871   \n",
       "7      7627            C  3263         5          3  victorian   821931   \n",
       "8      6571            A  1159         3          2      ranch   299903   \n",
       "9      5220            A  1248         3          2  victorian   321975   \n",
       "\n",
       "   lodge  ranch  victorian  \n",
       "0      0      1          0  \n",
       "1      0      0          1  \n",
       "2      0      1          0  \n",
       "3      0      1          0  \n",
       "4      0      0          1  \n",
       "5      1      0          0  \n",
       "6      0      0          1  \n",
       "7      0      0          1  \n",
       "8      0      1          0  \n",
       "9      0      0          1  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "style_dummies = pd.get_dummies(df['style'])\n",
    "new_df2 = df2.join(style_dummies)\n",
    "new_df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.339</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.339</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1548.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 16 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:03:16</td>     <th>  Log-Likelihood:    </th> <td> -86683.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.734e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6025</td>      <th>  BIC:               </th> <td>1.734e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>  3.05e+05</td> <td> 1.21e+04</td> <td>   25.120</td> <td> 0.000</td> <td> 2.81e+05</td> <td> 3.29e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ranch</th>     <td> 2.701e+05</td> <td> 1.57e+04</td> <td>   17.153</td> <td> 0.000</td> <td> 2.39e+05</td> <td> 3.01e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>victorian</th> <td> 7.411e+05</td> <td> 1.44e+04</td> <td>   51.396</td> <td> 0.000</td> <td> 7.13e+05</td> <td> 7.69e+05</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1340.120</td> <th>  Durbin-Watson:     </th> <td>   2.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3232.810</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.230</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 5.611</td>  <th>  Cond. No.          </th> <td>    4.77</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.339\n",
       "Model:                            OLS   Adj. R-squared:                  0.339\n",
       "Method:                 Least Squares   F-statistic:                     1548.\n",
       "Date:                Wed, 16 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        23:03:16   Log-Likelihood:                -86683.\n",
       "No. Observations:                6028   AIC:                         1.734e+05\n",
       "Df Residuals:                    6025   BIC:                         1.734e+05\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept    3.05e+05   1.21e+04     25.120      0.000    2.81e+05    3.29e+05\n",
       "ranch       2.701e+05   1.57e+04     17.153      0.000    2.39e+05    3.01e+05\n",
       "victorian   7.411e+05   1.44e+04     51.396      0.000    7.13e+05    7.69e+05\n",
       "==============================================================================\n",
       "Omnibus:                     1340.120   Durbin-Watson:                   2.004\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             3232.810\n",
       "Skew:                           1.230   Prob(JB):                         0.00\n",
       "Kurtosis:                       5.611   Cond. No.                         4.77\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df2['intercept'] = 1\n",
    "\n",
    "lm2 = sm.OLS(new_df2['price'], new_df2[['intercept', 'ranch', 'victorian']])\n",
    "results2 = lm2.fit()\n",
    "results2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alguns pontos a serem observados: Em primeiro lugar, a programação 1, 0 significa uma comparação com a categoria de base. Depois, a programação 1, 0,-1 significa uma comparação com a média geral. Por fim, a linguagem de aumento de uma unidade está associada às variáveis quantitativas, e não às variáveis categóricas.\n",
    "\n",
    "* 33.9% da variabilidade no preço pode ser explicada pelo estilo de casa.\n",
    "* 642100 é o preço médio de moradia previsto, sem levar em conta o estilo.\n",
    "* Em comparação a uma hospedaria, prevemos que uma casa vitoriana tenha uma alta de preço de 741100, mantendo todo o resto constante.\n",
    "* Em comparação a uma casa mediana, prevemos que o preço de uma casa vitoriana seja 404000 maior, mantendo todas as outras variáveis constantes.\n",
    "\n",
    "Para prever a categoria de referência na codificação 1, 0, você tem que utilizar o intercepto. Na codificação 1, 0, -1, você precisa multiplicar cada coeficiente categórico por -1 para chegar na categoria que falta. Com isto em mente, qual é o preço médio previsto para hospedarias utilizando o modelo de codificação 1, 0, -1?\n",
    "\n",
    "* Multiplicando -1 pela fazenda e pela casa vitoriana, obtemos o seguinte resultado: 642100 + 66950 - 404000 = 305050. Observe também que isso coincide com a mesma previsão (erro de 50 devido aos arredondamentos), que você vê no modelo de codificação 0,1\n",
    "\n",
    "---\n",
    "\n",
    "## Potential problems introduction\n",
    "\n",
    "There's a number of problems that may arise. First, what is your model for?\n",
    "* To understand if your X and Y variables are related?\n",
    "* To best predict the response variable?\n",
    "* Find which variables are really useful in predicting your response?\n",
    "\n",
    "Depending on which aspects you're most interesed in, this can help determine which problems you actually care about addressing.\n",
    "\n",
    "* A linear relationship may not exist between your response and predictor variables;\n",
    "* You might have correlated errors;\n",
    "* You might not have constant variance of your errors;\n",
    "* You might have outliers or leverage points that hurt your model;\n",
    "* You might have multicolliearity.\n",
    "\n",
    "Chapter 3 of \"[An introduction to statistical learning](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf)\" dives into each of the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Não-linearidade das relações entre preditor e resposta\n",
    "2. Correlação dos termos de erro\n",
    "3. Variância não-constante e erros normalmente distribuídos\n",
    "4. Outliers/pontos de alta alavancagem\n",
    "5. Colinearidade\n",
    "\n",
    "### Linearidade\n",
    "A suposição de linearidade é que o modelo linear representa a verdadeira relação existente entre a variável de resposta e a preditora. Se isso não for verdade, então as suas previsões não serão muito precisas. Além disso, as relações lineares associadas aos seus coeficientes também não são muito úteis.\n",
    "\n",
    "Para avaliar se uma relação linear é razoável, um gráfico dos resíduos $(y-\\hat{y})$ pelos valores preditos (\\hat{y}) geralmente é útil. Se existem padrões de curvatura neste gráfico, isso sugere que um modelo linear pode não se ajustar adequadamente aos dados, e alguma outra relação existe entre as variáveis preditoras e de resposta. Existem muitas maneiras de criar modelos não-lineares (até mesmo usando o formato do modelo linear), e você será apresentado a algumas delas.\n",
    "\n",
    "Na imagem na parte inferior desta página, os modelos são considerados viesados. O ideal seria que tivéssemos uma dispersão aleatória de pontos como na figura do gráfico de resíduos do canto superior esquerdo.\n",
    "\n",
    "#### Erros correlacionados\n",
    "Os erros correlacionados frequentemente ocorrem quando nossos dados são coletados ao longo do tempo (como na projeção de preços de ações ou taxas de juros futuras) ou quando os dados são relacionados espacialmente (como a previsão de regiões de inundações ou secas). Muitas vezes podemos melhorar nossas previsões usando informações dos últimos pontos de dados (para o tempo) ou os pontos nas proximidades (para o espaço).\n",
    "\n",
    "O principal problema em não levar em conta os erros correlacionados é que você poderia utilizar essa correlação para sua vantagem, prevendo de uma maneira melhor os eventos futuros ou eventos espacialmente próximos uns dos outros.\n",
    "\n",
    "Um dos jeitos mais comuns de identificar se os erros são correlacionados é verificar o domínio de onde os dados são coletados. Se você não tiver certeza, há um teste conhecido como teste [Durbin-Watson](https://en.wikipedia.org/wiki/Durbin%E2%80%93Watson_statistic) que é comumente usado para avaliar se a correlação dos erros é um problema. Depois disso, os modelos [ARIMA ou ARMA](http://www.statsref.com/HTML/index.html?arima.html) são comumente implementados para usar esta correlação em previsões melhores.\n",
    "\n",
    "#### Variância não-constante e erros normalmente distribuídos\n",
    "Variância não-constante ocorre quando a propagação dos valores previstos difere dependendo de qual é o valor que se está tentando prever. Isto não é um problema enorme em termos de uma boa previsão. No entanto, isso leva a intervalos de confiança e p-valores que são imprecisos. Os intervalos de confiança para os coeficientes serão amplos demais para áreas onde os valores reais estão mais perto dos valores previstos, mas serão muito estreitos para áreas onde os valores reais estão mais separados dos valores previstos.\n",
    "\n",
    "Comumente, um logaritmo (ou alguma outra transformação da variável de resposta) é feita para “se livrar” da variância não-constante. A fim de escolher a transformação, um [Box-Cox](http://www.statisticshowto.com/box-cox-transformation/) é geralmente usado.\n",
    "\n",
    "A variância não constante pode ser avaliada novamente, usando um gráfico dos resíduos pelos valores previstos. Na imagem na parte inferior da página, a variância não-constante é rotulada como **heteroscedástica**. Idealmente, queremos um modelo não viesado e com resíduos homocedásticos (consistente em toda o intervalo de valores).\n",
    "\n",
    "Embora o texto não aborde a normalidade dos resíduos, esta é uma suposição importante da regressão se você está interessado em criar intervalos de confiança adequados. Mais sobre este tema é fornecido [aqui](http://www.itl.nist.gov/div898/handbook/pri/section2/pri24.htm).\n",
    "\n",
    "#### Outliers/pontos de alavancagem\n",
    "Outliers e pontos de alavancagem são pontos que se encontram longe das tendências regulares de seus dados. Estes pontos podem ter uma grande influência na sua solução. Na prática, estes pontos podem até ser erros de digitação. Se você estiver agregando dados de várias fontes, é possível que alguns dos valores dos dados sejam transferidos ou agregados incorretamente.\n",
    "\n",
    "Em outros momentos os outliers são pontos de dados precisos e verdadeiros, não necessariamente um erro de medição ou de entrada de dados. Nesses casos, o ‘ajuste’ é mais subjetivo. Muitas vezes a estratégia para trabalhar com estes pontos depende do objetivo de sua análise. Modelos lineares usando o método de mínimos quadrados ordinários, em particular, não são muito robustos. Ou seja, outliers grandes podem alterar fortemente nossos resultados. Existem técnicas para combater isso - amplamente conhecidas como técnicas de **regularização**. Elas estão além do escopo desta aula, mas são discutidas rapidamente na versão gratuita do [Nanodegree de Machine Learning](https://classroom.udacity.com/courses/ud120).\n",
    "\n",
    "Um curso inteiro sobre regressão é fornecido pela Penn State University e eles tomam um tempo particularmente grande para discutir o tema dos pontos de alavancagem [aqui](https://newonlinecourses.science.psu.edu/stat501/node/336/).\n",
    "\n",
    "#### Colinearidade (multicolinearidade)\n",
    "ulticolinearidade ocorre quando temos variáveis preditoras que estão correlacionadas entre si. Uma das principais preocupações da multicolinearidade é que ela pode levar a coeficientes a serem invertidos da direção que esperamos na regressão linear simples.\n",
    "\n",
    "Uma das maneiras mais comuns para identificar multicolinearidade é com gráficos bivariados ou com **fatores de inflação de variância (ou VIFs)**.\n",
    "\n",
    "![ibagem](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/June/5b3254fc_estatistica-regressao-linear-multipla-pt/estatistica-regressao-linear-multipla-pt.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Multicollinearity and VIFs\n",
    "\n",
    "One of the main assumptions of multiple linear regression models, is that our predictor variables are uncorrelated with one another (our x-var should be correlated with the response, but not each other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>house_id</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>style</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1112</td>\n",
       "      <td>B</td>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>598291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491</td>\n",
       "      <td>B</td>\n",
       "      <td>3512</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1744259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5952</td>\n",
       "      <td>B</td>\n",
       "      <td>1134</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>571669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3525</td>\n",
       "      <td>A</td>\n",
       "      <td>1940</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>493675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5108</td>\n",
       "      <td>B</td>\n",
       "      <td>2208</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1101539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   house_id neighborhood  area  bedrooms  bathrooms      style    price\n",
       "0      1112            B  1188         3          2      ranch   598291\n",
       "1       491            B  3512         5          3  victorian  1744259\n",
       "2      5952            B  1134         3          2      ranch   571669\n",
       "3      3525            A  1940         4          2      ranch   493675\n",
       "4      5108            B  2208         6          4  victorian  1101539"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./house_prices.csv')\n",
    "df2 = df.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 540x540 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.pairplot(df[['area', 'bedrooms', 'bathrooms']]);\n",
    "# each of them has pretty strong positive relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4230.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 16 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:09:39</td>     <th>  Log-Likelihood:    </th> <td> -84517.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.690e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6024</td>      <th>  BIC:               </th> <td>1.691e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td> 1.007e+04</td> <td> 1.04e+04</td> <td>    0.972</td> <td> 0.331</td> <td>-1.02e+04</td> <td> 3.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>area</th>      <td>  345.9110</td> <td>    7.227</td> <td>   47.863</td> <td> 0.000</td> <td>  331.743</td> <td>  360.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedrooms</th>  <td>-2925.8063</td> <td> 1.03e+04</td> <td>   -0.285</td> <td> 0.775</td> <td> -2.3e+04</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms</th> <td> 7345.3917</td> <td> 1.43e+04</td> <td>    0.515</td> <td> 0.607</td> <td>-2.06e+04</td> <td> 3.53e+04</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>367.658</td> <th>  Durbin-Watson:     </th> <td>   2.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 350.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.536</td>  <th>  Prob(JB):          </th> <td>9.40e-77</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.503</td>  <th>  Cond. No.          </th> <td>1.16e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.16e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.678\n",
       "Model:                            OLS   Adj. R-squared:                  0.678\n",
       "Method:                 Least Squares   F-statistic:                     4230.\n",
       "Date:                Wed, 16 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        23:09:39   Log-Likelihood:                -84517.\n",
       "No. Observations:                6028   AIC:                         1.690e+05\n",
       "Df Residuals:                    6024   BIC:                         1.691e+05\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept   1.007e+04   1.04e+04      0.972      0.331   -1.02e+04    3.04e+04\n",
       "area         345.9110      7.227     47.863      0.000     331.743     360.079\n",
       "bedrooms   -2925.8063   1.03e+04     -0.285      0.775    -2.3e+04    1.72e+04\n",
       "bathrooms   7345.3917   1.43e+04      0.515      0.607   -2.06e+04    3.53e+04\n",
       "==============================================================================\n",
       "Omnibus:                      367.658   Durbin-Watson:                   2.007\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              350.116\n",
       "Skew:                           0.536   Prob(JB):                     9.40e-77\n",
       "Kurtosis:                       2.503   Cond. No.                     1.16e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.16e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['intercept'] = 1\n",
    "lm = sm.OLS(df['price'], df[['intercept', 'area', 'bedrooms', 'bathrooms']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The bedrooms has a negative coefficient associated with it.\n",
    "* Even though price and bedrooms have a positive relationship between one another, in the multiple linear regression model it showed up negative\n",
    "* The interpretation of this coefficient is now counter-intuitive to the relationship expected and what is actually true in the bivariate case.\n",
    "* This is one potential side effect of having multicollinearity in the model, is these flipped coefficients from what you expect to be true.\n",
    "* Another way to identify our predictors is correlated with one another, is **variance inflation factors** (VIFs)\n",
    "* It can be calculated for each x variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VIF Factor</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.327102</td>\n",
       "      <td>Intercept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.458190</td>\n",
       "      <td>area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.854484</td>\n",
       "      <td>bedrooms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.006851</td>\n",
       "      <td>bathrooms</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VIF Factor   features\n",
       "0    7.327102  Intercept\n",
       "1    5.458190       area\n",
       "2   20.854484   bedrooms\n",
       "3   19.006851  bathrooms"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y,  X = dmatrices('price ~ area + bedrooms + bathrooms', df, return_type='dataframe')\n",
    "vif = pd.DataFrame()\n",
    "vif['VIF Factor'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif['features'] = X.columns\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remove at least one of these last two variables as both of their variants inflation factors are larger than 10.\n",
    "\n",
    "Vimos que quando variáveis ‘x’ estão relacionadas entre si, podemos inverter as relações em nossos modelos de regressão linear múltipla frente ao que esperaríamos quando olhamos para as relações bivariadas da regressão linear.\n",
    "\n",
    "Para saber mais sobre VIFs e multicolinearidade, aqui está a publicação referenciada do vídeo sobre [VIFs](https://etav.github.io/python/vif_factor_python.html).\n",
    "\n",
    "* The case that X variables were correlated with one another can lead to flipped regression coefficients from the expected relationships and inaccurate hypothesis testing results.\n",
    "* When X variables are related to one another these results can be very misleading.\n",
    "* We saw two ways to identify multicollinearity: scatterplot matrix or variance inflation factors (VIFs).\n",
    "* If you have larger than ten for a VIF then you have multicollinearity in your model. \n",
    "* VIF for a particular variable is computed as one over one minus R squared, where the R squared is computed as the R squared for that X variable being predicted by all the other X variables.\n",
    "\n",
    "$VIF_i=\\frac{1}{1-R_i^2}$\n",
    "\n",
    "$x_i=b_0+b_1x_1+b_2x_2+...+b_n+x_n$\n",
    "\n",
    ">R2 = <1-R2\n",
    "\n",
    "It's unusual to find only one large VIF in a model, because if two or more X variables are related to one anoter you would expect each of these variables to have a high VIF.\n",
    "The most common way to work with variables that are corre\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
