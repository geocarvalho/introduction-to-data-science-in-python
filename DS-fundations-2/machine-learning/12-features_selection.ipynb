{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "* Select the best features available;\n",
    "* Don't select unecessary features;\n",
    "* Create new features\n",
    "\n",
    "## Adding a new feature\n",
    "\n",
    "1. human intuition;\n",
    "2. code up the new feature;\n",
    "3. visualize;\n",
    "4. repeat.\n",
    "\n",
    "## Getting rid of features\n",
    "\n",
    "* It's noisy;\n",
    "* It causes overfitting;\n",
    "* It is strongly related (highly correlated) with a feature that's already present;\n",
    "* Addtional features slow down training/testing process\n",
    "\n",
    "## Features != Information\n",
    "\n",
    "* We want information to draw conclusions and have insights;\n",
    "* Features is the actual number or caracteristics of a particular data point that's attempting to access information;\n",
    "* Is a little bit like the difference between the quantity of something and the quality;\n",
    "* Example of prepross is in `tools/email_preprocess.py`\n",
    "\n",
    "## Sklearn options\n",
    "\n",
    "* `SelectPercentile`: Selects the % of strong features;\n",
    "* `SelectKBest`: Select k strong features.\n",
    "* In `TfidfVectorizer` the argument `max_df` can be used for feature reduction (dimensionality reduction)\n",
    "\n",
    "## Bias-variance dilemma and number of features\n",
    "**High bias algorithm**\n",
    "* Pays little attetion to the training data and is kind of oversimplified;\n",
    "* High error on training set (in regression it means low r-squared or a large sum of the squared residual errors);\n",
    "* Few features used.\n",
    "\n",
    "**High variance**\n",
    "* Pays too much attention to the data (doesn't generalize well overfits). It doesn't generalize well to new situations that it hasn't quite seen before (basically memorize the training examples, overfitting);\n",
    "* Good fit to the training but has a higher error on test set because it's not generalizing very well;\n",
    "* Many features, carefully optimized performance on training data.\n",
    "\n",
    "Using few features can cause a classic high bias type regime, it's an oversimplified situation.\n",
    "\n",
    "With a model where it was very carefully tuned to minimize the sum of squared errors on a regression (SSE) and using lots of features to try  to get every little bit of information out of the data that it could. **That's can cause a high variance situation**, can overfit to the data.\n",
    "\n",
    "So there's this tradeoff between sort of the goodness of the fit and the simplicity of the fit.\n",
    "\n",
    "Fit the algorithm with few features, but using the case of regression as a large r-squared (rÂ²) or conversely a low sum of the squared residual errors (SSE).\n",
    "\n",
    "## Balacing errors with the number of features\n",
    "\n",
    "* How we can mathematically define what this arc might be so that the maximum point can be found;\n",
    "* This process is called **regularization**;\n",
    "* It's an automatic form of feature selection that some algorithms can do completely on their own, they can trade off between the precision, the goodness of it, the very low error and te complexity of fitting on lots of different features.\n",
    "\n",
    "## Regularization in regression\n",
    "\n",
    "Method for automatically penalizing extra features that is used in a model.\n",
    "\n",
    "**Lasso regression**\n",
    "\n",
    "minimize $SSE+\\lambda|\\beta|$\n",
    "\n",
    "* Minimize the sum of the squared erros in my fit. Minimize the distance between my fit and any given data point or the square of that distance;\n",
    "* In addition to minimizing the sum of the squared errors and the number of features to be used\n",
    "* $\\lambda$: penalty parameter;\n",
    "* $\\beta$: coefficients of my regression (number of features to use);\n",
    "\n",
    "Comparing two different fits (with different number of features):\n",
    "* The one that has more features will almost have a smaller sum of the squared error (it can fit more precisely to the points). But it pay penalty for using that extra feature;\n",
    "* The gain that it get in precision (the goodness of fit of my regression) has to be a bigger gain thant the loss take as a result of having additional feature in the regression;\n",
    "* Having small errors and having a simpler fit that's using fewer features;\n",
    "* Automatically takes into account this penalty parameter, it actually figure out which features are the ones that have the most important effect on the regression;\n",
    "* Once it's found those features, it can actually eliminate or set to zero the coefficients for the features that don't help.\n",
    "\n",
    "## Lasso regression\n",
    "\n",
    "For features that don't help the regression results enough, it can set the coefficient of those features to a very small value (zero).\n",
    "\n",
    "* [sklearn Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.85 0.  ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "clf = linear_model.Lasso(alpha=0.1)\n",
    "clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n",
    "print(clf.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Because we have two features for each training point, the coefficient of 0 means that it's effectively not being used in the regression. The second feature can disregard, at least in this particular regeression, all the discriminating power is coming from the first feature.\n",
    "\n",
    "If a decision tree is overfit, we expect the accuracy on a test set to be pretty low and a high accuracy on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
