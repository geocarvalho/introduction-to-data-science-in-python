{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating line\n",
    "Support vector machine (SVM) find a separating line (hyperplane) between data of two classes.\n",
    "* The best line maximizes the distance (margin) to the nearest point (relative to both classes);\n",
    "* Inside the SVM is to maximize robustness of the result;\n",
    "* SVM puts first and foremost the correct classification of the label, and then maximize the margin (classify correctly and then maximize the margin);\n",
    "* Outlier is a point inside another class, impossible to be classified correctly by the algorithm, but can be tolerate;\n",
    "* [sklearn.svm.SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "\n",
    "\n",
    "## Nonlinear data\n",
    "There isn't good linear separator between two classes in circle like scatter graph. But inputing $x^2+y^2$ ($z$) we'll have a three-dimensional input space and it can it can separe two classes by a nonlinear decision surface in form of a circel.\n",
    "\n",
    "## Creating a new feature\n",
    "Use the $|x|$ to create a new scatter plot with a nonlinear feature.\n",
    "\n",
    "## Kernel trick\n",
    "There are functions that take a low dimentional input space or feature space and map it to a very high dimensional space\n",
    "$x,y$(not linear separable) ->(kernel functions) $x_1x_2x_3x_4x_5$(separable problem) -> solution -> go back to $x,y$(not linear separable)\n",
    "* Using [sklearn.svm.SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) we can specify the algorithm of kernel we want use.\n",
    "\n",
    "## Kernel and gamma\n",
    "Parameters are arguments we pass when we create a classifier (before fitting) and it make a huge difference in the decision boundary that the algorithm arrives at.\n",
    "* kernel (linear vs rbf)\n",
    "* C - controls tradeoff between smooth decision boundary and classifying training points correctly\n",
    "> Does a large C mean a smooth boundary or more training points correct? More training points correct (using rbf kernel)\n",
    "* gamma (1000 vs 1) - defines how far the influence of a single training example reaches\n",
    "> If $\\gamma$ has a low value it means that every point has a far reach (the points far away the decision boundary are get in considaration - linear/smoother decision boundary), while high values means close reach (the decision boundary are just dependent of the very closest points, ignoring the further away ones - wiggly decision boundary).\n",
    "\n",
    "## Overfitting\n",
    "Correctly classifies the data sometimes but looks erratic in some places, that happens when we take the data too literal.\n",
    "The algorith produce complex distributions to classify something simple.\n",
    "It can be controlled by parameters\n",
    "\n",
    "## Strenghts and weaknesses\n",
    "\n",
    "Strenghts\n",
    "* They work really well in complicated domains where there is a clear margin of separation;\n",
    "\n",
    "Weaknesses\n",
    "* Don't perform so well in very large data sets, because the training time happens to be cubic in the size of the data set;\n",
    "* Don't work well with lots of noise (causing overfitting), when the class are very overlapping we have to count independent evidence and Naive Bayes would be better;\n",
    "* Can be slow for large data sets.\n",
    "\n",
    "## Running script \n",
    "> no. of Chris training emails: 7936\n",
    "\n",
    "> no. of Sara training emails: 7884\n",
    "\n",
    "> ('tempo de treinamento:', 202.866, 's')\n",
    "\n",
    "> ('tempo de predicao:', 22.289, 's')\n",
    "\n",
    "> 0.9840728100113766\n",
    "\n",
    "* Reducing the train's sample number:\n",
    "\n",
    "> ('Features to train: ', 158)\n",
    "\n",
    "> ('Labels to train: ', 158)\n",
    "\n",
    "> ('tempo de treinamento:', 0.194, 's')\n",
    "\n",
    "> ('tempo de predicao:', 1.987, 's')\n",
    "\n",
    "> 0.8845278725824801\n",
    "\n",
    "* With the reduced algorithm, changed kernel parameter to `rbf`:\n",
    "\n",
    "> ('tempo de treinamento:', 0.272, 's')\n",
    "\n",
    "> ('tempo de predicao:', 3.117, 's')\n",
    "\n",
    "> 0.6160409556313993\n",
    "\n",
    "* Using the last algorithm, let's change the C paramenter. The best paramater value (10, 100, 1000, 10000) and his results:\n",
    "\n",
    "> 10000 (more complex boundary)\n",
    "\n",
    "> ('tempo de treinamento:', 0.144, 's')\n",
    "\n",
    "> ('tempo de predicao:', 1.193, 's')\n",
    "\n",
    "> 0.8924914675767918\n",
    "\n",
    "* Come back to the first algorithm, but change the C and kernel parameters to the last result obtained:\n",
    "\n",
    "> no. of Chris training emails: 7936\n",
    "\n",
    "> no. of Sara training emails: 7884\n",
    "\n",
    "> ('tempo de treinamento:', 147.211, 's')\n",
    "\n",
    "> ('tempo de predicao:', 15.049, 's')\n",
    "\n",
    "> 0.9908987485779295\n",
    "\n",
    "* Print the elements 10, 26 and 50 from the pred result:\n",
    "\n",
    "> (1, 0, 1)\n",
    "\n",
    "* Calculate how many of the pred result were Chris (1), using all the data?\n",
    "\n",
    "> 877\n",
    "\n",
    "## Considarations\n",
    "\n",
    "> Esperamos que esteja ficando mais claro o tópico levantado pelo Sebastian quando ele disse que o Naive Bayes é uma ótima opção para trabalhar com textos -- é bastante rápido e geralmente dá resultados melhores para este problema em particular. É claro, existem uma grande diversidade de problemas em que o SVM pode trabalhar melhor. Saber qual classificador testar em cada tipo de problema faz parte da arte e ciência por trás da aprendizagem de máquina. Além de escolher um algoritmo, dependendo de qual você testar, existem otimizações de parâmetros para você fazer, e finalmente, há a possibilidade de sobreajustar seu modelo (especialmente se você não possui muitos dados para treinamento).\n",
    "\n",
    "> Uma sugestão é tentar avaliar diversos classificadores diferentes para cada problema. Otimizar os parâmetros pode gerar uma quantidade gigante de trabalho, mas até o final da aula nós vamos trabalhar com o GridCV, uma ótima ferramenta do sklearn que encontra parâmetros ótimos automaticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
